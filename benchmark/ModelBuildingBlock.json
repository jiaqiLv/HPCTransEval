[
    {
        "op_name": "inception",
        "c_code": "void default_function_kernel(float* T_concat, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6) {\n  float pad_temp[3244800];\n  float pool_max[3145728];\n  float conv2d_nhwc[524288];\n  float conv2d_nhwc_1[2097152];\n  float conv2d_nhwc_2[1048576];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 130; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 130; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 12; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          pad_temp[((((ax0_ax1_fused * 24960) + (ax2 * 192)) + (ax3_outer * 16)) + ax3_inner)] = (((((1 <= ax0_ax1_fused) && (ax0_ax1_fused < 129)) && (1 <= ax2)) && (ax2 < 129)) ? ph[(((((ax0_ax1_fused * 24576) + (ax2 * 192)) + (ax3_outer * 16)) + ax3_inner) - 24768)] : -3.402823e+38f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 128; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 128; ++ax2_1) {\n      for (int32_t ax3_outer_1 = 0; ax3_outer_1 < 12; ++ax3_outer_1) {\n        for (int32_t ax3_inner_1 = 0; ax3_inner_1 < 16; ++ax3_inner_1) {\n          pool_max[((((ax0_ax1_fused_1 * 24576) + (ax2_1 * 192)) + (ax3_outer_1 * 16)) + ax3_inner_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0_ax1_fused_1 * 24576) + (ax2_1 * 192)) + (ax3_outer_1 * 16)) + ax3_inner_1)] = max(pool_max[((((ax0_ax1_fused_1 * 24576) + (ax2_1 * 192)) + (ax3_outer_1 * 16)) + ax3_inner_1)], pad_temp[((((((rv0 * 24960) + (ax0_ax1_fused_1 * 24960)) + (rv1 * 192)) + (ax2_1 * 192)) + (ax3_outer_1 * 16)) + ax3_inner_1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused = 0; nn_yy_fused < 128; ++nn_yy_fused) {\n    for (int32_t xx = 0; xx < 128; ++xx) {\n      for (int32_t ff_outer = 0; ff_outer < 2; ++ff_outer) {\n        for (int32_t ff_inner = 0; ff_inner < 16; ++ff_inner) {\n          pad_temp[((((nn_yy_fused * 4096) + (xx * 32)) + (ff_outer * 16)) + ff_inner)] = 0.000000e+00f;\n          for (int32_t rc = 0; rc < 192; ++rc) {\n            pad_temp[((((nn_yy_fused * 4096) + (xx * 32)) + (ff_outer * 16)) + ff_inner)] = (pad_temp[((((nn_yy_fused * 4096) + (xx * 32)) + (ff_outer * 16)) + ff_inner)] + (pool_max[(((nn_yy_fused * 24576) + (xx * 192)) + rc)] * ph_1[(((rc * 32) + (ff_outer * 16)) + ff_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 2; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          pool_max[((((i0_i1_fused * 4096) + (i2 * 32)) + (i3_outer * 16)) + i3_inner)] = max(pad_temp[((((i0_i1_fused * 4096) + (i2 * 32)) + (i3_outer * 16)) + i3_inner)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 128; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 128; ++i2_1) {\n      for (int32_t i3_outer_1 = 0; i3_outer_1 < 12; ++i3_outer_1) {\n        for (int32_t i3_inner_1 = 0; i3_inner_1 < 16; ++i3_inner_1) {\n          pad_temp[((((i0_i1_fused_1 * 24576) + (i2_1 * 192)) + (i3_outer_1 * 16)) + i3_inner_1)] = ph[((((i0_i1_fused_1 * 24576) + (i2_1 * 192)) + (i3_outer_1 * 16)) + i3_inner_1)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused_1 = 0; nn_yy_fused_1 < 128; ++nn_yy_fused_1) {\n    for (int32_t xx_1 = 0; xx_1 < 128; ++xx_1) {\n      for (int32_t ff_inner_1 = 0; ff_inner_1 < 16; ++ff_inner_1) {\n        conv2d_nhwc[(((nn_yy_fused_1 * 2048) + (xx_1 * 16)) + ff_inner_1)] = 0.000000e+00f;\n        for (int32_t rc_1 = 0; rc_1 < 192; ++rc_1) {\n          conv2d_nhwc[(((nn_yy_fused_1 * 2048) + (xx_1 * 16)) + ff_inner_1)] = (conv2d_nhwc[(((nn_yy_fused_1 * 2048) + (xx_1 * 16)) + ff_inner_1)] + (pad_temp[(((nn_yy_fused_1 * 24576) + (xx_1 * 192)) + rc_1)] * ph_2[((rc_1 * 16) + ff_inner_1)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_2 = 0; i0_i1_fused_2 < 128; ++i0_i1_fused_2) {\n    for (int32_t i2_2 = 0; i2_2 < 128; ++i2_2) {\n      for (int32_t i3_inner_2 = 0; i3_inner_2 < 16; ++i3_inner_2) {\n        conv2d_nhwc[(((i0_i1_fused_2 * 2048) + (i2_2 * 16)) + i3_inner_2)] = max(conv2d_nhwc[(((i0_i1_fused_2 * 2048) + (i2_2 * 16)) + i3_inner_2)], 0.000000e+00f);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_3 = 0; i0_i1_fused_3 < 132; ++i0_i1_fused_3) {\n    for (int32_t i2_3 = 0; i2_3 < 132; ++i2_3) {\n      for (int32_t i3_inner_3 = 0; i3_inner_3 < 16; ++i3_inner_3) {\n        pad_temp[(((i0_i1_fused_3 * 2112) + (i2_3 * 16)) + i3_inner_3)] = (((((2 <= i0_i1_fused_3) && (i0_i1_fused_3 < 130)) && (2 <= i2_3)) && (i2_3 < 130)) ? conv2d_nhwc[((((i0_i1_fused_3 * 2048) + (i2_3 * 16)) + i3_inner_3) - 4128)] : 0.000000e+00f);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused_2 = 0; nn_yy_fused_2 < 128; ++nn_yy_fused_2) {\n    for (int32_t xx_2 = 0; xx_2 < 128; ++xx_2) {\n      for (int32_t ff_outer_1 = 0; ff_outer_1 < 2; ++ff_outer_1) {\n        for (int32_t ff_inner_2 = 0; ff_inner_2 < 16; ++ff_inner_2) {\n          conv2d_nhwc[((((nn_yy_fused_2 * 4096) + (xx_2 * 32)) + (ff_outer_1 * 16)) + ff_inner_2)] = 0.000000e+00f;\n          for (int32_t ry = 0; ry < 5; ++ry) {\n            for (int32_t rx = 0; rx < 5; ++rx) {\n              for (int32_t rc_2 = 0; rc_2 < 16; ++rc_2) {\n                conv2d_nhwc[((((nn_yy_fused_2 * 4096) + (xx_2 * 32)) + (ff_outer_1 * 16)) + ff_inner_2)] = (conv2d_nhwc[((((nn_yy_fused_2 * 4096) + (xx_2 * 32)) + (ff_outer_1 * 16)) + ff_inner_2)] + (pad_temp[(((((ry * 2112) + (nn_yy_fused_2 * 2112)) + (xx_2 * 16)) + (rx * 16)) + rc_2)] * ph_3[(((((ry * 2560) + (rx * 512)) + (rc_2 * 32)) + (ff_outer_1 * 16)) + ff_inner_2)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_4 = 0; i0_i1_fused_4 < 128; ++i0_i1_fused_4) {\n    for (int32_t i2_4 = 0; i2_4 < 128; ++i2_4) {\n      for (int32_t i3_outer_2 = 0; i3_outer_2 < 2; ++i3_outer_2) {\n        for (int32_t i3_inner_4 = 0; i3_inner_4 < 16; ++i3_inner_4) {\n          conv2d_nhwc[((((i0_i1_fused_4 * 4096) + (i2_4 * 32)) + (i3_outer_2 * 16)) + i3_inner_4)] = max(conv2d_nhwc[((((i0_i1_fused_4 * 4096) + (i2_4 * 32)) + (i3_outer_2 * 16)) + i3_inner_4)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_5 = 0; i0_i1_fused_5 < 128; ++i0_i1_fused_5) {\n    for (int32_t i2_5 = 0; i2_5 < 128; ++i2_5) {\n      for (int32_t i3_outer_3 = 0; i3_outer_3 < 12; ++i3_outer_3) {\n        for (int32_t i3_inner_5 = 0; i3_inner_5 < 16; ++i3_inner_5) {\n          pad_temp[((((i0_i1_fused_5 * 24576) + (i2_5 * 192)) + (i3_outer_3 * 16)) + i3_inner_5)] = ph[((((i0_i1_fused_5 * 24576) + (i2_5 * 192)) + (i3_outer_3 * 16)) + i3_inner_5)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused_3 = 0; nn_yy_fused_3 < 128; ++nn_yy_fused_3) {\n    for (int32_t xx_3 = 0; xx_3 < 128; ++xx_3) {\n      for (int32_t ff_outer_2 = 0; ff_outer_2 < 6; ++ff_outer_2) {\n        for (int32_t ff_inner_3 = 0; ff_inner_3 < 16; ++ff_inner_3) {\n          conv2d_nhwc_1[((((nn_yy_fused_3 * 12288) + (xx_3 * 96)) + (ff_outer_2 * 16)) + ff_inner_3)] = 0.000000e+00f;\n          for (int32_t rc_3 = 0; rc_3 < 192; ++rc_3) {\n            conv2d_nhwc_1[((((nn_yy_fused_3 * 12288) + (xx_3 * 96)) + (ff_outer_2 * 16)) + ff_inner_3)] = (conv2d_nhwc_1[((((nn_yy_fused_3 * 12288) + (xx_3 * 96)) + (ff_outer_2 * 16)) + ff_inner_3)] + (pad_temp[(((nn_yy_fused_3 * 24576) + (xx_3 * 192)) + rc_3)] * ph_4[(((rc_3 * 96) + (ff_outer_2 * 16)) + ff_inner_3)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_6 = 0; i0_i1_fused_6 < 128; ++i0_i1_fused_6) {\n    for (int32_t i2_6 = 0; i2_6 < 128; ++i2_6) {\n      for (int32_t i3_outer_4 = 0; i3_outer_4 < 6; ++i3_outer_4) {\n        for (int32_t i3_inner_6 = 0; i3_inner_6 < 16; ++i3_inner_6) {\n          conv2d_nhwc_1[((((i0_i1_fused_6 * 12288) + (i2_6 * 96)) + (i3_outer_4 * 16)) + i3_inner_6)] = max(conv2d_nhwc_1[((((i0_i1_fused_6 * 12288) + (i2_6 * 96)) + (i3_outer_4 * 16)) + i3_inner_6)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_7 = 0; i0_i1_fused_7 < 130; ++i0_i1_fused_7) {\n    for (int32_t i2_7 = 0; i2_7 < 130; ++i2_7) {\n      for (int32_t i3_outer_5 = 0; i3_outer_5 < 6; ++i3_outer_5) {\n        for (int32_t i3_inner_7 = 0; i3_inner_7 < 16; ++i3_inner_7) {\n          pad_temp[((((i0_i1_fused_7 * 12480) + (i2_7 * 96)) + (i3_outer_5 * 16)) + i3_inner_7)] = (((((1 <= i0_i1_fused_7) && (i0_i1_fused_7 < 129)) && (1 <= i2_7)) && (i2_7 < 129)) ? conv2d_nhwc_1[(((((i0_i1_fused_7 * 12288) + (i2_7 * 96)) + (i3_outer_5 * 16)) + i3_inner_7) - 12384)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused_4 = 0; nn_yy_fused_4 < 128; ++nn_yy_fused_4) {\n    for (int32_t xx_4 = 0; xx_4 < 128; ++xx_4) {\n      for (int32_t ff_outer_3 = 0; ff_outer_3 < 8; ++ff_outer_3) {\n        for (int32_t ff_inner_4 = 0; ff_inner_4 < 16; ++ff_inner_4) {\n          conv2d_nhwc_1[((((nn_yy_fused_4 * 16384) + (xx_4 * 128)) + (ff_outer_3 * 16)) + ff_inner_4)] = 0.000000e+00f;\n          for (int32_t ry_1 = 0; ry_1 < 3; ++ry_1) {\n            for (int32_t rx_1 = 0; rx_1 < 3; ++rx_1) {\n              for (int32_t rc_4 = 0; rc_4 < 96; ++rc_4) {\n                conv2d_nhwc_1[((((nn_yy_fused_4 * 16384) + (xx_4 * 128)) + (ff_outer_3 * 16)) + ff_inner_4)] = (conv2d_nhwc_1[((((nn_yy_fused_4 * 16384) + (xx_4 * 128)) + (ff_outer_3 * 16)) + ff_inner_4)] + (pad_temp[(((((ry_1 * 12480) + (nn_yy_fused_4 * 12480)) + (xx_4 * 96)) + (rx_1 * 96)) + rc_4)] * ph_5[(((((ry_1 * 36864) + (rx_1 * 12288)) + (rc_4 * 128)) + (ff_outer_3 * 16)) + ff_inner_4)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_8 = 0; i0_i1_fused_8 < 128; ++i0_i1_fused_8) {\n    for (int32_t i2_8 = 0; i2_8 < 128; ++i2_8) {\n      for (int32_t i3_outer_6 = 0; i3_outer_6 < 8; ++i3_outer_6) {\n        for (int32_t i3_inner_8 = 0; i3_inner_8 < 16; ++i3_inner_8) {\n          conv2d_nhwc_1[((((i0_i1_fused_8 * 16384) + (i2_8 * 128)) + (i3_outer_6 * 16)) + i3_inner_8)] = max(conv2d_nhwc_1[((((i0_i1_fused_8 * 16384) + (i2_8 * 128)) + (i3_outer_6 * 16)) + i3_inner_8)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_9 = 0; i0_i1_fused_9 < 128; ++i0_i1_fused_9) {\n    for (int32_t i2_9 = 0; i2_9 < 128; ++i2_9) {\n      for (int32_t i3_outer_7 = 0; i3_outer_7 < 12; ++i3_outer_7) {\n        for (int32_t i3_inner_9 = 0; i3_inner_9 < 16; ++i3_inner_9) {\n          pad_temp[((((i0_i1_fused_9 * 24576) + (i2_9 * 192)) + (i3_outer_7 * 16)) + i3_inner_9)] = ph[((((i0_i1_fused_9 * 24576) + (i2_9 * 192)) + (i3_outer_7 * 16)) + i3_inner_9)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_yy_fused_5 = 0; nn_yy_fused_5 < 128; ++nn_yy_fused_5) {\n    for (int32_t xx_5 = 0; xx_5 < 128; ++xx_5) {\n      for (int32_t ff_outer_4 = 0; ff_outer_4 < 4; ++ff_outer_4) {\n        for (int32_t ff_inner_5 = 0; ff_inner_5 < 16; ++ff_inner_5) {\n          conv2d_nhwc_2[((((nn_yy_fused_5 * 8192) + (xx_5 * 64)) + (ff_outer_4 * 16)) + ff_inner_5)] = 0.000000e+00f;\n          for (int32_t rc_5 = 0; rc_5 < 192; ++rc_5) {\n            conv2d_nhwc_2[((((nn_yy_fused_5 * 8192) + (xx_5 * 64)) + (ff_outer_4 * 16)) + ff_inner_5)] = (conv2d_nhwc_2[((((nn_yy_fused_5 * 8192) + (xx_5 * 64)) + (ff_outer_4 * 16)) + ff_inner_5)] + (pad_temp[(((nn_yy_fused_5 * 24576) + (xx_5 * 192)) + rc_5)] * ph_6[(((rc_5 * 64) + (ff_outer_4 * 16)) + ff_inner_5)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_10 = 0; i0_i1_fused_10 < 128; ++i0_i1_fused_10) {\n    for (int32_t i2_10 = 0; i2_10 < 128; ++i2_10) {\n      for (int32_t i3_outer_8 = 0; i3_outer_8 < 4; ++i3_outer_8) {\n        for (int32_t i3_inner_10 = 0; i3_inner_10 < 16; ++i3_inner_10) {\n          conv2d_nhwc_2[((((i0_i1_fused_10 * 8192) + (i2_10 * 64)) + (i3_outer_8 * 16)) + i3_inner_10)] = max(conv2d_nhwc_2[((((i0_i1_fused_10 * 8192) + (i2_10 * 64)) + (i3_outer_8 * 16)) + i3_inner_10)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 128; ++ax0_ax1_fused_2) {\n    for (int32_t ax2_2 = 0; ax2_2 < 128; ++ax2_2) {\n      for (int32_t ax3_outer_2 = 0; ax3_outer_2 < 16; ++ax3_outer_2) {\n        for (int32_t ax3_inner_2 = 0; ax3_inner_2 < 16; ++ax3_inner_2) {\n          T_concat[((((ax0_ax1_fused_2 * 32768) + (ax2_2 * 256)) + (ax3_outer_2 * 16)) + ax3_inner_2)] = ((14 <= ax3_outer_2) ? pool_max[(((((ax0_ax1_fused_2 * 4096) + (ax2_2 * 32)) + (ax3_outer_2 * 16)) + ax3_inner_2) - 224)] : ((12 <= ax3_outer_2) ? conv2d_nhwc[(((((ax0_ax1_fused_2 * 4096) + (ax2_2 * 32)) + (ax3_outer_2 * 16)) + ax3_inner_2) - 192)] : ((4 <= ax3_outer_2) ? conv2d_nhwc_1[(((((ax0_ax1_fused_2 * 16384) + (ax2_2 * 128)) + (ax3_outer_2 * 16)) + ax3_inner_2) - 64)] : conv2d_nhwc_2[((((ax0_ax1_fused_2 * 8192) + (ax2_2 * 64)) + (ax3_outer_2 * 16)) + ax3_inner_2)])));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ pad_temp, float* __restrict__ pool_max) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 12; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    pool_max[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = -3.402823e+38f;\n    for (int rv0 = 0; rv0 < 3; ++rv0) {\n      for (int rv1 = 0; rv1 < 3; ++rv1) {\n        pool_max[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(pool_max[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], pad_temp[(((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 32) + (((int)blockIdx.x) >> 3)) / 3) * 24960) + (rv0 * 24960)) + (rv1 * 192)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 24576))]);\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ conv2d_nhwc) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 6; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 7; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    if ((((i0_i1_fused_i2_fused_i3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7)) < 12675) {\n      pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (((((195 <= (((i0_i1_fused_i2_fused_i3_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6))) && ((((i0_i1_fused_i2_fused_i3_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) < 25155)) && (3 <= ((((i0_i1_fused_i2_fused_i3_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) % 390))) && (((((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 12480) < 12384)) ? conv2d_nhwc[(((((((i0_i1_fused_i2_fused_i3_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) / 195) * 12288) + ((((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 12480)) - 12384)] : 0.000000e+00f);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_4(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 12; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = ph[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ pad_temp, float* __restrict__ pool_max) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 2; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    pool_max[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_10(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 12; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = ph[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 2; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    if ((((i0_i1_fused_i2_fused_i3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 1089) {\n      pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (((((33 <= (((i0_i1_fused_i2_fused_i3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7))) && ((((i0_i1_fused_i2_fused_i3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7)) < 2145)) && (1 <= ((((i0_i1_fused_i2_fused_i3_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) % 66))) && (((((i0_i1_fused_i2_fused_i3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) % 132) < 130)) ? conv2d_nhwc[((((((((i0_i1_fused_i2_fused_i3_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) / 33) * 2048) + (((((i0_i1_fused_i2_fused_i3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) % 132) * 16)) + (((int)threadIdx.x) & 15)) - 4128)] : 0.000000e+00f);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_14(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int nn_yy_fused_xx_fused_ff_fused_outer = 0; nn_yy_fused_xx_fused_ff_fused_outer < 8; ++nn_yy_fused_xx_fused_ff_fused_outer) {\n    conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    for (int ry = 0; ry < 3; ++ry) {\n      for (int rx = 0; rx < 3; ++rx) {\n        for (int rc = 0; rc < 96; ++rc) {\n          conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pad_temp[(((((((nn_yy_fused_xx_fused_ff_fused_outer * 199680) + ((((int)blockIdx.x) >> 4) * 12480)) + (ry * 12480)) + ((((int)blockIdx.x) & 15) * 768)) + ((((int)threadIdx.x) >> 7) * 96)) + (rx * 96)) + rc)] * ph[((((ry * 36864) + (rx * 12288)) + (rc * 128)) + (((int)threadIdx.x) & 127))]));\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ T_concat, float* __restrict__ conv2d_nhwc, float* __restrict__ conv2d_nhwc_1, float* __restrict__ conv2d_nhwc_2, float* __restrict__ pool_max) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 16; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    T_concat[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = ((224 <= (((int)threadIdx.x) & 255)) ? pool_max[(((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 32768) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) >> 8) * 32)) + (((int)threadIdx.x) & 255)) - 224)] : ((192 <= (((int)threadIdx.x) & 255)) ? conv2d_nhwc[(((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 32768) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) >> 8) * 32)) + (((int)threadIdx.x) & 255)) - 192)] : ((64 <= (((int)threadIdx.x) & 255)) ? conv2d_nhwc_1[(((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 131072) + (((int)blockIdx.x) * 512)) + ((((int)threadIdx.x) >> 8) * 128)) + (((int)threadIdx.x) & 255)) - 64)] : conv2d_nhwc_2[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)threadIdx.x) & 255))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ conv2d_nhwc) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 2; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_5(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  conv2d_nhwc[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rc = 0; rc < 192; ++rc) {\n    conv2d_nhwc[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nhwc[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((((int)blockIdx.x) * 12288) + ((((int)threadIdx.x) >> 4) * 192)) + rc)] * ph[((rc * 16) + (((int)threadIdx.x) & 15))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_6(float* __restrict__ conv2d_nhwc) {\n  conv2d_nhwc[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nhwc[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_8(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int nn_yy_fused_xx_fused_ff_fused_outer = 0; nn_yy_fused_xx_fused_ff_fused_outer < 2; ++nn_yy_fused_xx_fused_ff_fused_outer) {\n    conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    for (int ry = 0; ry < 5; ++ry) {\n      for (int rx = 0; rx < 5; ++rx) {\n        for (int rc = 0; rc < 16; ++rc) {\n          conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pad_temp[(((((((nn_yy_fused_xx_fused_ff_fused_outer * 135168) + ((((int)blockIdx.x) >> 2) * 2112)) + (ry * 2112)) + ((((int)blockIdx.x) & 3) * 512)) + ((((int)threadIdx.x) >> 5) * 16)) + (rx * 16)) + rc)] * ph[((((ry * 2560) + (rx * 512)) + (rc * 32)) + (((int)threadIdx.x) & 31))]));\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_17(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int nn_yy_fused_xx_fused_ff_fused_outer = 0; nn_yy_fused_xx_fused_ff_fused_outer < 4; ++nn_yy_fused_xx_fused_ff_fused_outer) {\n    conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    for (int rc = 0; rc < 192; ++rc) {\n      conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pad_temp[((((nn_yy_fused_xx_fused_ff_fused_outer * 786432) + (((int)blockIdx.x) * 3072)) + ((((int)threadIdx.x) >> 6) * 192)) + rc)] * ph[((rc * 64) + (((int)threadIdx.x) & 63))]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_16(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 12; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    pad_temp[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = ph[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_18(float* __restrict__ conv2d_nhwc) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 4; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ conv2d_nhwc, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int nn_yy_fused_xx_fused_ff_fused_outer = 0; nn_yy_fused_xx_fused_ff_fused_outer < 6; ++nn_yy_fused_xx_fused_ff_fused_outer) {\n    conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    for (int rc = 0; rc < 192; ++rc) {\n      conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nhwc[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pad_temp[((((((nn_yy_fused_xx_fused_ff_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) / 3) * 192) + rc)] * ph[((rc * 96) + ((((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 96))]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ conv2d_nhwc) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 8; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nhwc[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 13; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 12675) {\n      pad_temp[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (((((195 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7))) && ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7)) < 25155)) && (3 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) % 390))) && (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 24960) < 24768)) ? ph[(((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7)) / 195) * 24576) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 24960)) - 24768)] : -3.402823e+38f);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_2(float* __restrict__ pad_temp, float* __restrict__ ph, float* __restrict__ pool_max) {\n  for (int nn_yy_fused_xx_fused_ff_fused_outer = 0; nn_yy_fused_xx_fused_ff_fused_outer < 2; ++nn_yy_fused_xx_fused_ff_fused_outer) {\n    pad_temp[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    for (int rc = 0; rc < 192; ++rc) {\n      pad_temp[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (pad_temp[(((nn_yy_fused_xx_fused_ff_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pool_max[((((nn_yy_fused_xx_fused_ff_fused_outer * 1572864) + (((int)blockIdx.x) * 6144)) + ((((int)threadIdx.x) >> 5) * 192)) + rc)] * ph[((rc * 32) + (((int)threadIdx.x) & 31))]));\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 128, 128, 192), \"float32\"), ph_1: T.Buffer((1, 1, 192, 64), \"float32\"), ph_2: T.Buffer((1, 1, 192, 96), \"float32\"), ph_3: T.Buffer((3, 3, 96, 128), \"float32\"), ph_4: T.Buffer((1, 1, 192, 16), \"float32\"), ph_5: T.Buffer((5, 5, 16, 32), \"float32\"), ph_6: T.Buffer((1, 1, 192, 32), \"float32\"), T_concat: T.Buffer((1, 128, 128, 256), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([3244800], \"float32\", \"global\")\n        pool_max = T.allocate([3145728], \"float32\", \"global\")\n        conv2d_nhwc = T.allocate([524288], \"float32\", \"global\")\n        conv2d_nhwc_1 = T.allocate([2097152], \"float32\", \"global\")\n        conv2d_nhwc_2 = T.allocate([1048576], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((3244800,), data=pad_temp)\n        ph_7 = T.Buffer((3145728,), data=ph.data)\n        for ax0_ax1_fused in T.parallel(130):\n            for ax2, ax3_outer, ax3_inner in T.grid(130, 12, 16):\n                cse_var_2: T.int32 = ax2 * 192\n                cse_var_1: T.int32 = ax3_outer * 16\n                pad_temp_1[ax0_ax1_fused * 24960 + cse_var_2 + cse_var_1 + ax3_inner] = T.if_then_else(1 <= ax0_ax1_fused and ax0_ax1_fused < 129 and 1 <= ax2 and ax2 < 129, ph_7[ax0_ax1_fused * 24576 + cse_var_2 + cse_var_1 + ax3_inner - 24768], T.float32(-3.4028234663852886e+38))\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 12, 16):\n                pool_max_1 = T.Buffer((3145728,), data=pool_max)\n                pool_max_1[ax0_ax1_fused * 24576 + ax2 * 192 + ax3_outer * 16 + ax3_inner] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_5: T.int32 = ax2 * 192\n                    cse_var_4: T.int32 = ax3_outer * 16\n                    cse_var_3: T.int32 = ax0_ax1_fused * 24576 + cse_var_5 + cse_var_4 + ax3_inner\n                    pool_max_1[cse_var_3] = T.max(pool_max_1[cse_var_3], pad_temp_1[rv0 * 24960 + ax0_ax1_fused * 24960 + cse_var_5 + rv1 * 192 + cse_var_4 + ax3_inner])\n        pad_temp_2 = T.Buffer((524288,), data=pad_temp)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_outer, ff_inner in T.grid(128, 2, 16):\n                pad_temp_2[nn_yy_fused * 4096 + xx * 32 + ff_outer * 16 + ff_inner] = T.float32(0)\n                for rc in range(192):\n                    cse_var_7: T.int32 = ff_outer * 16\n                    cse_var_6: T.int32 = nn_yy_fused * 4096 + xx * 32 + cse_var_7 + ff_inner\n                    pool_max_1 = T.Buffer((3145728,), data=pool_max)\n                    ph_8 = T.Buffer((6144,), data=ph_6.data)\n                    pad_temp_2[cse_var_6] = pad_temp_2[cse_var_6] + pool_max_1[nn_yy_fused * 24576 + xx * 192 + rc] * ph_8[rc * 32 + cse_var_7 + ff_inner]\n        pool_max_1 = T.Buffer((524288,), data=pool_max)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 2, 16):\n                cse_var_8: T.int32 = i0_i1_fused * 4096 + i2 * 32 + i3_outer * 16 + i3_inner\n                pool_max_1[cse_var_8] = T.max(pad_temp_2[cse_var_8], T.float32(0))\n        pad_temp_3 = T.Buffer((3145728,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 12, 16):\n                cse_var_9: T.int32 = i0_i1_fused * 24576 + i2 * 192 + i3_outer * 16 + i3_inner\n                pad_temp_3[cse_var_9] = ph_7[cse_var_9]\n        conv2d_nhwc_3 = T.Buffer((262144,), data=conv2d_nhwc)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_inner in T.grid(128, 16):\n                conv2d_nhwc_3[nn_yy_fused * 2048 + xx * 16 + ff_inner] = T.float32(0)\n                for rc in range(192):\n                    cse_var_10: T.int32 = nn_yy_fused * 2048 + xx * 16 + ff_inner\n                    ph_8 = T.Buffer((3072,), data=ph_4.data)\n                    conv2d_nhwc_3[cse_var_10] = conv2d_nhwc_3[cse_var_10] + pad_temp_3[nn_yy_fused * 24576 + xx * 192 + rc] * ph_8[rc * 16 + ff_inner]\n        conv2d_nhwc_4 = T.Buffer((262144,), data=conv2d_nhwc)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_inner in T.grid(128, 16):\n                cse_var_11: T.int32 = i0_i1_fused * 2048 + i2 * 16 + i3_inner\n                conv2d_nhwc_4[cse_var_11] = T.max(conv2d_nhwc_3[cse_var_11], T.float32(0))\n        pad_temp_4 = T.Buffer((278784,), data=pad_temp)\n        for i0_i1_fused in T.parallel(132):\n            for i2, i3_inner in T.grid(132, 16):\n                cse_var_12: T.int32 = i2 * 16\n                pad_temp_4[i0_i1_fused * 2112 + cse_var_12 + i3_inner] = T.if_then_else(2 <= i0_i1_fused and i0_i1_fused < 130 and 2 <= i2 and i2 < 130, conv2d_nhwc_4[i0_i1_fused * 2048 + cse_var_12 + i3_inner - 4128], T.float32(0))\n        conv2d_nhwc_5 = T.Buffer((524288,), data=conv2d_nhwc)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_outer, ff_inner in T.grid(128, 2, 16):\n                conv2d_nhwc_5[nn_yy_fused * 4096 + xx * 32 + ff_outer * 16 + ff_inner] = T.float32(0)\n                for ry, rx, rc in T.grid(5, 5, 16):\n                    cse_var_14: T.int32 = ff_outer * 16\n                    cse_var_13: T.int32 = nn_yy_fused * 4096 + xx * 32 + cse_var_14 + ff_inner\n                    ph_8 = T.Buffer((12800,), data=ph_5.data)\n                    conv2d_nhwc_5[cse_var_13] = conv2d_nhwc_5[cse_var_13] + pad_temp_4[ry * 2112 + nn_yy_fused * 2112 + xx * 16 + rx * 16 + rc] * ph_8[ry * 2560 + rx * 512 + rc * 32 + cse_var_14 + ff_inner]\n        conv2d_nhwc_6 = T.Buffer((524288,), data=conv2d_nhwc)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 2, 16):\n                cse_var_15: T.int32 = i0_i1_fused * 4096 + i2 * 32 + i3_outer * 16 + i3_inner\n                conv2d_nhwc_6[cse_var_15] = T.max(conv2d_nhwc_5[cse_var_15], T.float32(0))\n        pad_temp_5 = T.Buffer((3145728,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 12, 16):\n                cse_var_16: T.int32 = i0_i1_fused * 24576 + i2 * 192 + i3_outer * 16 + i3_inner\n                pad_temp_5[cse_var_16] = ph_7[cse_var_16]\n        conv2d_nhwc_7 = T.Buffer((1572864,), data=conv2d_nhwc_1)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_outer, ff_inner in T.grid(128, 6, 16):\n                conv2d_nhwc_7[nn_yy_fused * 12288 + xx * 96 + ff_outer * 16 + ff_inner] = T.float32(0)\n                for rc in range(192):\n                    cse_var_18: T.int32 = ff_outer * 16\n                    cse_var_17: T.int32 = nn_yy_fused * 12288 + xx * 96 + cse_var_18 + ff_inner\n                    ph_8 = T.Buffer((18432,), data=ph_2.data)\n                    conv2d_nhwc_7[cse_var_17] = conv2d_nhwc_7[cse_var_17] + pad_temp_5[nn_yy_fused * 24576 + xx * 192 + rc] * ph_8[rc * 96 + cse_var_18 + ff_inner]\n        conv2d_nhwc_8 = T.Buffer((1572864,), data=conv2d_nhwc_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 6, 16):\n                cse_var_19: T.int32 = i0_i1_fused * 12288 + i2 * 96 + i3_outer * 16 + i3_inner\n                conv2d_nhwc_8[cse_var_19] = T.max(conv2d_nhwc_7[cse_var_19], T.float32(0))\n        pad_temp_6 = T.Buffer((1622400,), data=pad_temp)\n        for i0_i1_fused in T.parallel(130):\n            for i2, i3_outer, i3_inner in T.grid(130, 6, 16):\n                cse_var_21: T.int32 = i2 * 96\n                cse_var_20: T.int32 = i3_outer * 16\n                pad_temp_6[i0_i1_fused * 12480 + cse_var_21 + cse_var_20 + i3_inner] = T.if_then_else(1 <= i0_i1_fused and i0_i1_fused < 129 and 1 <= i2 and i2 < 129, conv2d_nhwc_8[i0_i1_fused * 12288 + cse_var_21 + cse_var_20 + i3_inner - 12384], T.float32(0))\n        conv2d_nhwc_9 = T.Buffer((2097152,), data=conv2d_nhwc_1)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_outer, ff_inner in T.grid(128, 8, 16):\n                conv2d_nhwc_9[nn_yy_fused * 16384 + xx * 128 + ff_outer * 16 + ff_inner] = T.float32(0)\n                for ry, rx, rc in T.grid(3, 3, 96):\n                    cse_var_23: T.int32 = ff_outer * 16\n                    cse_var_22: T.int32 = nn_yy_fused * 16384 + xx * 128 + cse_var_23 + ff_inner\n                    ph_8 = T.Buffer((110592,), data=ph_3.data)\n                    conv2d_nhwc_9[cse_var_22] = conv2d_nhwc_9[cse_var_22] + pad_temp_6[ry * 12480 + nn_yy_fused * 12480 + xx * 96 + rx * 96 + rc] * ph_8[ry * 36864 + rx * 12288 + rc * 128 + cse_var_23 + ff_inner]\n        conv2d_nhwc_10 = T.Buffer((2097152,), data=conv2d_nhwc_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_24: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                conv2d_nhwc_10[cse_var_24] = T.max(conv2d_nhwc_9[cse_var_24], T.float32(0))\n        pad_temp_7 = T.Buffer((3145728,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 12, 16):\n                cse_var_25: T.int32 = i0_i1_fused * 24576 + i2 * 192 + i3_outer * 16 + i3_inner\n                pad_temp_7[cse_var_25] = ph_7[cse_var_25]\n        conv2d_nhwc_11 = T.Buffer((1048576,), data=conv2d_nhwc_2)\n        for nn_yy_fused in T.parallel(128):\n            for xx, ff_outer, ff_inner in T.grid(128, 4, 16):\n                conv2d_nhwc_11[nn_yy_fused * 8192 + xx * 64 + ff_outer * 16 + ff_inner] = T.float32(0)\n                for rc in range(192):\n                    cse_var_27: T.int32 = ff_outer * 16\n                    cse_var_26: T.int32 = nn_yy_fused * 8192 + xx * 64 + cse_var_27 + ff_inner\n                    ph_8 = T.Buffer((12288,), data=ph_1.data)\n                    conv2d_nhwc_11[cse_var_26] = conv2d_nhwc_11[cse_var_26] + pad_temp_7[nn_yy_fused * 24576 + xx * 192 + rc] * ph_8[rc * 64 + cse_var_27 + ff_inner]\n        conv2d_nhwc_12 = T.Buffer((1048576,), data=conv2d_nhwc_2)\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3_outer, i3_inner in T.grid(128, 4, 16):\n                cse_var_28: T.int32 = i0_i1_fused * 8192 + i2 * 64 + i3_outer * 16 + i3_inner\n                conv2d_nhwc_12[cse_var_28] = T.max(conv2d_nhwc_11[cse_var_28], T.float32(0))\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 16, 16):\n                cse_var_30: T.int32 = ax3_outer * 16\n                cse_var_29: T.int32 = ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_30 + ax3_inner\n                T_concat_1 = T.Buffer((4194304,), data=T_concat.data)\n                T_concat_1[ax0_ax1_fused * 32768 + ax2 * 256 + cse_var_30 + ax3_inner] = T.if_then_else(14 <= ax3_outer, pool_max_1[cse_var_29 - 224], T.if_then_else(12 <= ax3_outer, conv2d_nhwc_6[cse_var_29 - 192], T.if_then_else(4 <= ax3_outer, conv2d_nhwc_10[ax0_ax1_fused * 16384 + ax2 * 128 + cse_var_30 + ax3_inner - 64], conv2d_nhwc_12[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_30 + ax3_inner])))",
        "op_args": "None",
        "input_shape": [
            [
                1,
                128,
                128,
                192
            ],
            [
                1,
                1,
                192,
                64
            ],
            [
                1,
                1,
                192,
                96
            ],
            [
                3,
                3,
                96,
                128
            ],
            [
                1,
                1,
                192,
                16
            ],
            [
                5,
                5,
                16,
                32
            ],
            [
                1,
                1,
                192,
                32
            ]
        ],
        "output_shape": [
            [
                1,
                128,
                128,
                256
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "T_concat"
        ]
    },
    {
        "op_name": "gru",
        "c_code": "void default_function_kernel(float* T_add, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10, float* ph_11, float* ph_12) {\n  float T_matmul_NN[512];\n  float T_matmul_NN_1[512];\n  float T_matmul_NN_2[512];\n  float T_matmul_NN_3[512];\n  float T_matmul_NN_4[512];\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1_outer = 0; i1_outer < 16; ++i1_outer) {\n      for (int32_t i1_inner = 0; i1_inner < 16; ++i1_inner) {\n        T_matmul_NN[(((i0 * 256) + (i1_outer * 16)) + i1_inner)] = 0.000000e+00f;\n        for (int32_t k = 0; k < 1000; ++k) {\n          T_matmul_NN[(((i0 * 256) + (i1_outer * 16)) + i1_inner)] = (T_matmul_NN[(((i0 * 256) + (i1_outer * 16)) + i1_inner)] + (ph[((i0 * 1000) + k)] * ph_1[(((k * 256) + (i1_outer * 16)) + i1_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int32_t i1_outer_1 = 0; i1_outer_1 < 16; ++i1_outer_1) {\n      for (int32_t i1_inner_1 = 0; i1_inner_1 < 16; ++i1_inner_1) {\n        T_matmul_NN_1[(((i0_1 * 256) + (i1_outer_1 * 16)) + i1_inner_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 256; ++k_1) {\n          T_matmul_NN_1[(((i0_1 * 256) + (i1_outer_1 * 16)) + i1_inner_1)] = (T_matmul_NN_1[(((i0_1 * 256) + (i1_outer_1 * 16)) + i1_inner_1)] + (ph_2[((i0_1 * 256) + k_1)] * ph_3[(((k_1 * 256) + (i1_outer_1 * 16)) + i1_inner_1)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1_outer = 0; ax1_outer < 16; ++ax1_outer) {\n      for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {\n        T_matmul_NN[(((ax0 * 256) + (ax1_outer * 16)) + ax1_inner)] = (T_matmul_NN[(((ax0 * 256) + (ax1_outer * 16)) + ax1_inner)] + T_matmul_NN_1[(((ax0 * 256) + (ax1_outer * 16)) + ax1_inner)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 2; ++ax0_1) {\n    for (int32_t ax1_outer_1 = 0; ax1_outer_1 < 16; ++ax1_outer_1) {\n      for (int32_t ax1_inner_1 = 0; ax1_inner_1 < 16; ++ax1_inner_1) {\n        T_matmul_NN[(((ax0_1 * 256) + (ax1_outer_1 * 16)) + ax1_inner_1)] = (T_matmul_NN[(((ax0_1 * 256) + (ax1_outer_1 * 16)) + ax1_inner_1)] + ph_4[((ax1_outer_1 * 16) + ax1_inner_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 2; ++i0_2) {\n    for (int32_t i1_outer_2 = 0; i1_outer_2 < 16; ++i1_outer_2) {\n      for (int32_t i1_inner_2 = 0; i1_inner_2 < 16; ++i1_inner_2) {\n        T_matmul_NN[(((i0_2 * 256) + (i1_outer_2 * 16)) + i1_inner_2)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - T_matmul_NN[(((i0_2 * 256) + (i1_outer_2 * 16)) + i1_inner_2)]))));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_2 = 0; ax0_2 < 2; ++ax0_2) {\n    for (int32_t ax1_outer_2 = 0; ax1_outer_2 < 16; ++ax1_outer_2) {\n      for (int32_t ax1_inner_2 = 0; ax1_inner_2 < 16; ++ax1_inner_2) {\n        T_matmul_NN_1[(((ax0_2 * 256) + (ax1_outer_2 * 16)) + ax1_inner_2)] = (T_matmul_NN[(((ax0_2 * 256) + (ax1_outer_2 * 16)) + ax1_inner_2)] * ph_2[(((ax0_2 * 256) + (ax1_outer_2 * 16)) + ax1_inner_2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_3 = 0; ax0_3 < 2; ++ax0_3) {\n    for (int32_t ax1_outer_3 = 0; ax1_outer_3 < 16; ++ax1_outer_3) {\n      for (int32_t ax1_inner_3 = 0; ax1_inner_3 < 16; ++ax1_inner_3) {\n        T_matmul_NN[(((ax0_3 * 256) + (ax1_outer_3 * 16)) + ax1_inner_3)] = (1.000000e+00f - T_matmul_NN[(((ax0_3 * 256) + (ax1_outer_3 * 16)) + ax1_inner_3)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_3 = 0; i0_3 < 2; ++i0_3) {\n    for (int32_t i1_outer_3 = 0; i1_outer_3 < 16; ++i1_outer_3) {\n      for (int32_t i1_inner_3 = 0; i1_inner_3 < 16; ++i1_inner_3) {\n        T_matmul_NN_2[(((i0_3 * 256) + (i1_outer_3 * 16)) + i1_inner_3)] = 0.000000e+00f;\n        for (int32_t k_2 = 0; k_2 < 1000; ++k_2) {\n          T_matmul_NN_2[(((i0_3 * 256) + (i1_outer_3 * 16)) + i1_inner_3)] = (T_matmul_NN_2[(((i0_3 * 256) + (i1_outer_3 * 16)) + i1_inner_3)] + (ph[((i0_3 * 1000) + k_2)] * ph_5[(((k_2 * 256) + (i1_outer_3 * 16)) + i1_inner_3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_4 = 0; i0_4 < 2; ++i0_4) {\n    for (int32_t i1_outer_4 = 0; i1_outer_4 < 16; ++i1_outer_4) {\n      for (int32_t i1_inner_4 = 0; i1_inner_4 < 16; ++i1_inner_4) {\n        T_matmul_NN_3[(((i0_4 * 256) + (i1_outer_4 * 16)) + i1_inner_4)] = 0.000000e+00f;\n        for (int32_t k_3 = 0; k_3 < 1000; ++k_3) {\n          T_matmul_NN_3[(((i0_4 * 256) + (i1_outer_4 * 16)) + i1_inner_4)] = (T_matmul_NN_3[(((i0_4 * 256) + (i1_outer_4 * 16)) + i1_inner_4)] + (ph[((i0_4 * 1000) + k_3)] * ph_6[(((k_3 * 256) + (i1_outer_4 * 16)) + i1_inner_4)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_5 = 0; i0_5 < 2; ++i0_5) {\n    for (int32_t i1_outer_5 = 0; i1_outer_5 < 16; ++i1_outer_5) {\n      for (int32_t i1_inner_5 = 0; i1_inner_5 < 16; ++i1_inner_5) {\n        T_matmul_NN_4[(((i0_5 * 256) + (i1_outer_5 * 16)) + i1_inner_5)] = 0.000000e+00f;\n        for (int32_t k_4 = 0; k_4 < 256; ++k_4) {\n          T_matmul_NN_4[(((i0_5 * 256) + (i1_outer_5 * 16)) + i1_inner_5)] = (T_matmul_NN_4[(((i0_5 * 256) + (i1_outer_5 * 16)) + i1_inner_5)] + (ph_2[((i0_5 * 256) + k_4)] * ph_7[(((k_4 * 256) + (i1_outer_5 * 16)) + i1_inner_5)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_4 = 0; ax0_4 < 2; ++ax0_4) {\n    for (int32_t ax1_outer_4 = 0; ax1_outer_4 < 16; ++ax1_outer_4) {\n      for (int32_t ax1_inner_4 = 0; ax1_inner_4 < 16; ++ax1_inner_4) {\n        T_matmul_NN_3[(((ax0_4 * 256) + (ax1_outer_4 * 16)) + ax1_inner_4)] = (T_matmul_NN_3[(((ax0_4 * 256) + (ax1_outer_4 * 16)) + ax1_inner_4)] + T_matmul_NN_4[(((ax0_4 * 256) + (ax1_outer_4 * 16)) + ax1_inner_4)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_5 = 0; ax0_5 < 2; ++ax0_5) {\n    for (int32_t ax1_outer_5 = 0; ax1_outer_5 < 16; ++ax1_outer_5) {\n      for (int32_t ax1_inner_5 = 0; ax1_inner_5 < 16; ++ax1_inner_5) {\n        T_matmul_NN_3[(((ax0_5 * 256) + (ax1_outer_5 * 16)) + ax1_inner_5)] = (T_matmul_NN_3[(((ax0_5 * 256) + (ax1_outer_5 * 16)) + ax1_inner_5)] + ph_8[((ax1_outer_5 * 16) + ax1_inner_5)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_6 = 0; i0_6 < 2; ++i0_6) {\n    for (int32_t i1_outer_6 = 0; i1_outer_6 < 16; ++i1_outer_6) {\n      for (int32_t i1_inner_6 = 0; i1_inner_6 < 16; ++i1_inner_6) {\n        T_matmul_NN_3[(((i0_6 * 256) + (i1_outer_6 * 16)) + i1_inner_6)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - T_matmul_NN_3[(((i0_6 * 256) + (i1_outer_6 * 16)) + i1_inner_6)]))));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_6 = 0; ax0_6 < 2; ++ax0_6) {\n    for (int32_t ax1_outer_6 = 0; ax1_outer_6 < 16; ++ax1_outer_6) {\n      for (int32_t ax1_inner_6 = 0; ax1_inner_6 < 16; ++ax1_inner_6) {\n        T_matmul_NN_3[(((ax0_6 * 256) + (ax1_outer_6 * 16)) + ax1_inner_6)] = (T_matmul_NN_3[(((ax0_6 * 256) + (ax1_outer_6 * 16)) + ax1_inner_6)] * ph_2[(((ax0_6 * 256) + (ax1_outer_6 * 16)) + ax1_inner_6)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_7 = 0; i0_7 < 2; ++i0_7) {\n    for (int32_t i1_outer_7 = 0; i1_outer_7 < 16; ++i1_outer_7) {\n      for (int32_t i1_inner_7 = 0; i1_inner_7 < 16; ++i1_inner_7) {\n        T_matmul_NN_4[(((i0_7 * 256) + (i1_outer_7 * 16)) + i1_inner_7)] = 0.000000e+00f;\n        for (int32_t k_5 = 0; k_5 < 256; ++k_5) {\n          T_matmul_NN_4[(((i0_7 * 256) + (i1_outer_7 * 16)) + i1_inner_7)] = (T_matmul_NN_4[(((i0_7 * 256) + (i1_outer_7 * 16)) + i1_inner_7)] + (T_matmul_NN_3[((i0_7 * 256) + k_5)] * ph_9[(((k_5 * 256) + (i1_outer_7 * 16)) + i1_inner_7)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_7 = 0; ax0_7 < 2; ++ax0_7) {\n    for (int32_t ax1_outer_7 = 0; ax1_outer_7 < 16; ++ax1_outer_7) {\n      for (int32_t ax1_inner_7 = 0; ax1_inner_7 < 16; ++ax1_inner_7) {\n        T_matmul_NN_2[(((ax0_7 * 256) + (ax1_outer_7 * 16)) + ax1_inner_7)] = (T_matmul_NN_2[(((ax0_7 * 256) + (ax1_outer_7 * 16)) + ax1_inner_7)] + T_matmul_NN_4[(((ax0_7 * 256) + (ax1_outer_7 * 16)) + ax1_inner_7)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_8 = 0; ax0_8 < 2; ++ax0_8) {\n    for (int32_t ax1_outer_8 = 0; ax1_outer_8 < 16; ++ax1_outer_8) {\n      for (int32_t ax1_inner_8 = 0; ax1_inner_8 < 16; ++ax1_inner_8) {\n        T_matmul_NN_2[(((ax0_8 * 256) + (ax1_outer_8 * 16)) + ax1_inner_8)] = (T_matmul_NN_2[(((ax0_8 * 256) + (ax1_outer_8 * 16)) + ax1_inner_8)] + ph_10[((ax1_outer_8 * 16) + ax1_inner_8)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_8 = 0; i0_8 < 2; ++i0_8) {\n    for (int32_t i1_outer_8 = 0; i1_outer_8 < 16; ++i1_outer_8) {\n      for (int32_t i1_inner_8 = 0; i1_inner_8 < 16; ++i1_inner_8) {\n        T_matmul_NN_2[(((i0_8 * 256) + (i1_outer_8 * 16)) + i1_inner_8)] = tanhf(T_matmul_NN_2[(((i0_8 * 256) + (i1_outer_8 * 16)) + i1_inner_8)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_9 = 0; ax0_9 < 2; ++ax0_9) {\n    for (int32_t ax1_outer_9 = 0; ax1_outer_9 < 16; ++ax1_outer_9) {\n      for (int32_t ax1_inner_9 = 0; ax1_inner_9 < 16; ++ax1_inner_9) {\n        T_matmul_NN[(((ax0_9 * 256) + (ax1_outer_9 * 16)) + ax1_inner_9)] = (T_matmul_NN[(((ax0_9 * 256) + (ax1_outer_9 * 16)) + ax1_inner_9)] * T_matmul_NN_2[(((ax0_9 * 256) + (ax1_outer_9 * 16)) + ax1_inner_9)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_10 = 0; ax0_10 < 2; ++ax0_10) {\n    for (int32_t ax1_outer_10 = 0; ax1_outer_10 < 16; ++ax1_outer_10) {\n      for (int32_t ax1_inner_10 = 0; ax1_inner_10 < 16; ++ax1_inner_10) {\n        T_matmul_NN_1[(((ax0_10 * 256) + (ax1_outer_10 * 16)) + ax1_inner_10)] = (T_matmul_NN_1[(((ax0_10 * 256) + (ax1_outer_10 * 16)) + ax1_inner_10)] + T_matmul_NN[(((ax0_10 * 256) + (ax1_outer_10 * 16)) + ax1_inner_10)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_9 = 0; i0_9 < 2; ++i0_9) {\n    for (int32_t i1_outer_9 = 0; i1_outer_9 < 8; ++i1_outer_9) {\n      for (int32_t i1_inner_9 = 0; i1_inner_9 < 16; ++i1_inner_9) {\n        T_matmul_NN_3[(((i0_9 * 128) + (i1_outer_9 * 16)) + i1_inner_9)] = 0.000000e+00f;\n        for (int32_t k_6 = 0; k_6 < 256; ++k_6) {\n          T_matmul_NN_3[(((i0_9 * 128) + (i1_outer_9 * 16)) + i1_inner_9)] = (T_matmul_NN_3[(((i0_9 * 128) + (i1_outer_9 * 16)) + i1_inner_9)] + (T_matmul_NN_1[((i0_9 * 256) + k_6)] * ph_11[(((k_6 * 128) + (i1_outer_9 * 16)) + i1_inner_9)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_11 = 0; ax0_11 < 2; ++ax0_11) {\n    for (int32_t ax1_outer_11 = 0; ax1_outer_11 < 8; ++ax1_outer_11) {\n      for (int32_t ax1_inner_11 = 0; ax1_inner_11 < 16; ++ax1_inner_11) {\n        T_add[(((ax0_11 * 128) + (ax1_outer_11 * 16)) + ax1_inner_11)] = (T_matmul_NN_3[(((ax0_11 * 128) + (ax1_outer_11 * 16)) + ax1_inner_11)] + ph_12[((ax1_outer_11 * 16) + ax1_inner_11)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(512) default_function_kernel(float* __restrict__ T_matmul_NN, float* __restrict__ ph, float* __restrict__ ph_1) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 1000; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (ph[(((((int)threadIdx.x) >> 8) * 1000) + k)] * ph_1[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(256) default_function_kernel_21(float* __restrict__ T_add, float* __restrict__ T_matmul_NN, float* __restrict__ ph) {\n  T_add[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + ph[(((int)threadIdx.x) & 127)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_2(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + T_matmul_NN_1[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_19(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + T_matmul_NN_1[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_5(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN_1[((int)threadIdx.x)] * ph[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_17(float* __restrict__ T_matmul_NN) {\n  T_matmul_NN[((int)threadIdx.x)] = tanhf(T_matmul_NN[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_14(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 256; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (T_matmul_NN_1[(((((int)threadIdx.x) >> 8) * 256) + k)] * ph[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_16(float* __restrict__ T_matmul_NN, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + ph[(((int)threadIdx.x) & 255)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_4(float* __restrict__ T_matmul_NN) {\n  T_matmul_NN[((int)threadIdx.x)] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - T_matmul_NN[((int)threadIdx.x)]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_8(float* __restrict__ T_matmul_NN, float* __restrict__ ph, float* __restrict__ ph_1) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 1000; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (ph[(((((int)threadIdx.x) >> 8) * 1000) + k)] * ph_1[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_13(float* __restrict__ T_matmul_NN, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] * ph[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_3(float* __restrict__ T_matmul_NN, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + ph[(((int)threadIdx.x) & 255)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_12(float* __restrict__ T_matmul_NN) {\n  T_matmul_NN[((int)threadIdx.x)] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - T_matmul_NN[((int)threadIdx.x)]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_11(float* __restrict__ T_matmul_NN, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + ph[(((int)threadIdx.x) & 255)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_9(float* __restrict__ T_matmul_NN, float* __restrict__ ph, float* __restrict__ ph_1) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 256; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (ph[(((((int)threadIdx.x) >> 8) * 256) + k)] * ph_1[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_10(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + T_matmul_NN_1[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_7(float* __restrict__ T_matmul_NN, float* __restrict__ ph, float* __restrict__ ph_1) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 1000; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (ph[(((((int)threadIdx.x) >> 8) * 1000) + k)] * ph_1[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_15(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + T_matmul_NN_1[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_18(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1) {\n  T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] * T_matmul_NN_1[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(256) default_function_kernel_20(float* __restrict__ T_matmul_NN, float* __restrict__ T_matmul_NN_1, float* __restrict__ ph) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 256; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (T_matmul_NN_1[(((((int)threadIdx.x) >> 7) * 256) + k)] * ph[((k * 128) + (((int)threadIdx.x) & 127))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_6(float* __restrict__ T_matmul_NN) {\n  T_matmul_NN[((int)threadIdx.x)] = (1.000000e+00f - T_matmul_NN[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_1(float* __restrict__ T_matmul_NN, float* __restrict__ ph, float* __restrict__ ph_1) {\n  T_matmul_NN[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 256; ++k) {\n    T_matmul_NN[((int)threadIdx.x)] = (T_matmul_NN[((int)threadIdx.x)] + (ph[(((((int)threadIdx.x) >> 8) * 256) + k)] * ph_1[((k * 256) + (((int)threadIdx.x) & 255))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((2, 1000), \"float32\"), ph_1: T.Buffer((2, 256), \"float32\"), ph_2: T.Buffer((1000, 256), \"float32\"), ph_3: T.Buffer((256, 256), \"float32\"), ph_4: T.Buffer((256,), \"float32\"), ph_5: T.Buffer((1000, 256), \"float32\"), ph_6: T.Buffer((256, 256), \"float32\"), ph_7: T.Buffer((256,), \"float32\"), ph_8: T.Buffer((1000, 256), \"float32\"), ph_9: T.Buffer((256, 256), \"float32\"), ph_10: T.Buffer((256,), \"float32\"), ph_11: T.Buffer((256, 128), \"float32\"), ph_12: T.Buffer((128,), \"float32\"), T_add: T.Buffer((2, 128), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_matmul_NN = T.allocate([512], \"float32\", \"global\")\n        T_matmul_NN_1 = T.allocate([512], \"float32\", \"global\")\n        T_matmul_NN_2 = T.allocate([512], \"float32\", \"global\")\n        T_matmul_NN_3 = T.allocate([512], \"float32\", \"global\")\n        T_matmul_NN_4 = T.allocate([512], \"float32\", \"global\")\n        T_matmul_NN_5 = T.Buffer((512,), data=T_matmul_NN)\n        ph_13 = T.Buffer((2000,), data=ph.data)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_5[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(1000):\n                    cse_var_2: T.int32 = i1_outer * 16\n                    cse_var_1: T.int32 = i0 * 256 + cse_var_2 + i1_inner\n                    ph_14 = T.Buffer((256000,), data=ph_2.data)\n                    T_matmul_NN_5[cse_var_1] = T_matmul_NN_5[cse_var_1] + ph_13[i0 * 1000 + k] * ph_14[k * 256 + cse_var_2 + i1_inner]\n        T_matmul_NN_6 = T.Buffer((512,), data=T_matmul_NN_1)\n        ph_14 = T.Buffer((512,), data=ph_1.data)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_6[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(256):\n                    cse_var_5: T.int32 = i1_outer * 16\n                    cse_var_4: T.int32 = i0 * 256\n                    cse_var_3: T.int32 = cse_var_4 + cse_var_5 + i1_inner\n                    ph_15 = T.Buffer((65536,), data=ph_3.data)\n                    T_matmul_NN_6[cse_var_3] = T_matmul_NN_6[cse_var_3] + ph_14[cse_var_4 + k] * ph_15[k * 256 + cse_var_5 + i1_inner]\n        T_matmul_NN_7 = T.Buffer((512,), data=T_matmul_NN)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_6: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_7[cse_var_6] = T_matmul_NN_5[cse_var_6] + T_matmul_NN_6[cse_var_6]\n        T_matmul_NN_8 = T.Buffer((512,), data=T_matmul_NN)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_8: T.int32 = ax1_outer * 16\n                cse_var_7: T.int32 = ax0 * 256 + cse_var_8 + ax1_inner\n                T_matmul_NN_8[cse_var_7] = T_matmul_NN_7[cse_var_7] + ph_4[cse_var_8 + ax1_inner]\n        T_matmul_NN_9 = T.Buffer((512,), data=T_matmul_NN)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                cse_var_9: T.int32 = i0 * 256 + i1_outer * 16 + i1_inner\n                T_matmul_NN_9[cse_var_9] = T.sigmoid(T_matmul_NN_8[cse_var_9])\n        T_matmul_NN_10 = T.Buffer((512,), data=T_matmul_NN_1)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_10: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_10[cse_var_10] = T_matmul_NN_9[cse_var_10] * ph_14[cse_var_10]\n        T_matmul_NN_11 = T.Buffer((512,), data=T_matmul_NN)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_11: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_11[cse_var_11] = T.float32(1) - T_matmul_NN_9[cse_var_11]\n        T_matmul_NN_12 = T.Buffer((512,), data=T_matmul_NN_2)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_12[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(1000):\n                    cse_var_13: T.int32 = i1_outer * 16\n                    cse_var_12: T.int32 = i0 * 256 + cse_var_13 + i1_inner\n                    ph_15 = T.Buffer((256000,), data=ph_8.data)\n                    T_matmul_NN_12[cse_var_12] = T_matmul_NN_12[cse_var_12] + ph_13[i0 * 1000 + k] * ph_15[k * 256 + cse_var_13 + i1_inner]\n        T_matmul_NN_13 = T.Buffer((512,), data=T_matmul_NN_3)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_13[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(1000):\n                    cse_var_15: T.int32 = i1_outer * 16\n                    cse_var_14: T.int32 = i0 * 256 + cse_var_15 + i1_inner\n                    ph_15 = T.Buffer((256000,), data=ph_5.data)\n                    T_matmul_NN_13[cse_var_14] = T_matmul_NN_13[cse_var_14] + ph_13[i0 * 1000 + k] * ph_15[k * 256 + cse_var_15 + i1_inner]\n        T_matmul_NN_14 = T.Buffer((512,), data=T_matmul_NN_4)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_14[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(256):\n                    cse_var_18: T.int32 = i1_outer * 16\n                    cse_var_17: T.int32 = i0 * 256\n                    cse_var_16: T.int32 = cse_var_17 + cse_var_18 + i1_inner\n                    ph_15 = T.Buffer((65536,), data=ph_6.data)\n                    T_matmul_NN_14[cse_var_16] = T_matmul_NN_14[cse_var_16] + ph_14[cse_var_17 + k] * ph_15[k * 256 + cse_var_18 + i1_inner]\n        T_matmul_NN_15 = T.Buffer((512,), data=T_matmul_NN_3)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_19: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_15[cse_var_19] = T_matmul_NN_13[cse_var_19] + T_matmul_NN_14[cse_var_19]\n        T_matmul_NN_16 = T.Buffer((512,), data=T_matmul_NN_3)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_21: T.int32 = ax1_outer * 16\n                cse_var_20: T.int32 = ax0 * 256 + cse_var_21 + ax1_inner\n                T_matmul_NN_16[cse_var_20] = T_matmul_NN_15[cse_var_20] + ph_7[cse_var_21 + ax1_inner]\n        T_matmul_NN_17 = T.Buffer((512,), data=T_matmul_NN_3)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                cse_var_22: T.int32 = i0 * 256 + i1_outer * 16 + i1_inner\n                T_matmul_NN_17[cse_var_22] = T.sigmoid(T_matmul_NN_16[cse_var_22])\n        T_matmul_NN_18 = T.Buffer((512,), data=T_matmul_NN_3)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_23: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_18[cse_var_23] = T_matmul_NN_17[cse_var_23] * ph_14[cse_var_23]\n        T_matmul_NN_19 = T.Buffer((512,), data=T_matmul_NN_4)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                T_matmul_NN_19[i0 * 256 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(256):\n                    cse_var_26: T.int32 = i1_outer * 16\n                    cse_var_25: T.int32 = i0 * 256\n                    cse_var_24: T.int32 = cse_var_25 + cse_var_26 + i1_inner\n                    ph_15 = T.Buffer((65536,), data=ph_9.data)\n                    T_matmul_NN_19[cse_var_24] = T_matmul_NN_19[cse_var_24] + T_matmul_NN_18[cse_var_25 + k] * ph_15[k * 256 + cse_var_26 + i1_inner]\n        T_matmul_NN_20 = T.Buffer((512,), data=T_matmul_NN_2)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_27: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_20[cse_var_27] = T_matmul_NN_12[cse_var_27] + T_matmul_NN_19[cse_var_27]\n        T_matmul_NN_21 = T.Buffer((512,), data=T_matmul_NN_2)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_29: T.int32 = ax1_outer * 16\n                cse_var_28: T.int32 = ax0 * 256 + cse_var_29 + ax1_inner\n                T_matmul_NN_21[cse_var_28] = T_matmul_NN_20[cse_var_28] + ph_10[cse_var_29 + ax1_inner]\n        T_matmul_NN_22 = T.Buffer((512,), data=T_matmul_NN_2)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(16, 16):\n                cse_var_30: T.int32 = i0 * 256 + i1_outer * 16 + i1_inner\n                T_matmul_NN_22[cse_var_30] = T.tanh(T_matmul_NN_21[cse_var_30])\n        T_matmul_NN_23 = T.Buffer((512,), data=T_matmul_NN)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_31: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_23[cse_var_31] = T_matmul_NN_11[cse_var_31] * T_matmul_NN_22[cse_var_31]\n        T_matmul_NN_24 = T.Buffer((512,), data=T_matmul_NN_1)\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(16, 16):\n                cse_var_32: T.int32 = ax0 * 256 + ax1_outer * 16 + ax1_inner\n                T_matmul_NN_24[cse_var_32] = T_matmul_NN_10[cse_var_32] + T_matmul_NN_23[cse_var_32]\n        T_matmul_NN_25 = T.Buffer((256,), data=T_matmul_NN_3)\n        for i0 in T.parallel(2):\n            for i1_outer, i1_inner in T.grid(8, 16):\n                T_matmul_NN_25[i0 * 128 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(256):\n                    cse_var_34: T.int32 = i1_outer * 16\n                    cse_var_33: T.int32 = i0 * 128 + cse_var_34 + i1_inner\n                    ph_15 = T.Buffer((32768,), data=ph_11.data)\n                    T_matmul_NN_25[cse_var_33] = T_matmul_NN_25[cse_var_33] + T_matmul_NN_24[i0 * 256 + k] * ph_15[k * 128 + cse_var_34 + i1_inner]\n        for ax0 in T.parallel(2):\n            for ax1_outer, ax1_inner in T.grid(8, 16):\n                cse_var_36: T.int32 = ax1_outer * 16\n                cse_var_35: T.int32 = ax0 * 128 + cse_var_36 + ax1_inner\n                T_add_1 = T.Buffer((256,), data=T_add.data)\n                T_add_1[cse_var_35] = T_matmul_NN_25[cse_var_35] + ph_12[cse_var_36 + ax1_inner]",
        "op_args": "None",
        "input_shape": [
            [
                2,
                1000
            ],
            [
                2,
                256
            ],
            [
                1000,
                256
            ],
            [
                256,
                256
            ],
            [
                256
            ],
            [
                1000,
                256
            ],
            [
                256,
                256
            ],
            [
                256
            ],
            [
                1000,
                256
            ],
            [
                256,
                256
            ],
            [
                256
            ],
            [
                256,
                128
            ],
            [
                128
            ]
        ],
        "output_shape": [
            [
                2,
                128
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "T_add"
        ]
    },
    {
        "op_name": "transformer",
        "c_code": "void default_function_kernel(float* T_add, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10, float* ph_11) {\n  float T_cast[65536];\n  float T_cast_1[1024];\n  float T_multiply[66048];\n  float T_multiply_red[128];\n  float T_matmul_NN[65536];\n  float T_transpose[65536];\n  float T_strided_slice[32768];\n  float T_strided_slice_1[131072];\n  float T_strided_slice_2[32768];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2_outer = 0; ax2_outer < 32; ++ax2_outer) {\n      for (int32_t ax2_inner = 0; ax2_inner < 16; ++ax2_inner) {\n        T_cast[(((ax0_ax1_fused * 512) + (ax2_outer * 16)) + ax2_inner)] = ph[(((ax0_ax1_fused * 512) + (ax2_outer * 16)) + ax2_inner)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_outer = 0; ax0_outer < 32; ++ax0_outer) {\n    for (int32_t ax0_inner = 0; ax0_inner < 16; ++ax0_inner) {\n      T_cast_1[((ax0_outer * 16) + ax0_inner)] = ph_1[((ax0_outer * 16) + ax0_inner)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 128; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_outer_1 = 0; ax2_outer_1 < 32; ++ax2_outer_1) {\n      for (int32_t ax2_inner_1 = 0; ax2_inner_1 < 16; ++ax2_inner_1) {\n        T_multiply[(((ax0_ax1_fused_1 * 512) + (ax2_outer_1 * 16)) + ax2_inner_1)] = (T_cast[(((ax0_ax1_fused_1 * 512) + (ax2_outer_1 * 16)) + ax2_inner_1)] * T_cast[(((ax0_ax1_fused_1 * 512) + (ax2_outer_1 * 16)) + ax2_inner_1)]);\n      }\n    }\n  }\n  for (int32_t ax1_outer = 0; ax1_outer < 8; ++ax1_outer) {\n    for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {\n      T_multiply_red[((ax1_outer * 16) + ax1_inner)] = 0.000000e+00f;\n      for (int32_t k2 = 0; k2 < 512; ++k2) {\n        T_multiply_red[((ax1_outer * 16) + ax1_inner)] = (T_multiply_red[((ax1_outer * 16) + ax1_inner)] + T_multiply[(((ax1_outer * 8192) + (ax1_inner * 512)) + k2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 128; ++ax0_ax1_fused_2) {\n    for (int32_t ax2_outer_2 = 0; ax2_outer_2 < 32; ++ax2_outer_2) {\n      for (int32_t ax2_inner_2 = 0; ax2_inner_2 < 16; ++ax2_inner_2) {\n        T_cast[(((ax0_ax1_fused_2 * 512) + (ax2_outer_2 * 16)) + ax2_inner_2)] = ((T_cast[(((ax0_ax1_fused_2 * 512) + (ax2_outer_2 * 16)) + ax2_inner_2)] * T_cast_1[((ax2_outer_2 * 16) + ax2_inner_2)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax0_ax1_fused_2] * 1.953125e-03f) + 1.000000e-05f))));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 128; ++ax0) {\n    for (int32_t ax1_outer_1 = 0; ax1_outer_1 < 32; ++ax1_outer_1) {\n      for (int32_t ax1_inner_1 = 0; ax1_inner_1 < 16; ++ax1_inner_1) {\n        T_multiply[(((ax0 * 512) + (ax1_outer_1 * 16)) + ax1_inner_1)] = T_cast[(((ax0 * 512) + (ax1_outer_1 * 16)) + ax1_inner_1)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 128; ++i0) {\n    for (int32_t i1_outer = 0; i1_outer < 32; ++i1_outer) {\n      for (int32_t i1_inner = 0; i1_inner < 16; ++i1_inner) {\n        T_matmul_NN[(((i0 * 512) + (i1_outer * 16)) + i1_inner)] = 0.000000e+00f;\n        for (int32_t k = 0; k < 512; ++k) {\n          T_matmul_NN[(((i0 * 512) + (i1_outer * 16)) + i1_inner)] = (T_matmul_NN[(((i0 * 512) + (i1_outer * 16)) + i1_inner)] + (T_multiply[((i0 * 512) + k)] * ph_2[(((k * 512) + (i1_outer * 16)) + i1_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_3 = 0; ax0_ax1_fused_3 < 8; ++ax0_ax1_fused_3) {\n    for (int32_t ax2 = 0; ax2 < 128; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 4; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          T_transpose[((((ax0_ax1_fused_3 * 8192) + (ax2 * 64)) + (ax3_outer * 16)) + ax3_inner)] = T_matmul_NN[((((ax2 * 512) + (ax0_ax1_fused_3 * 64)) + (ax3_outer * 16)) + ax3_inner)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_4 = 0; ax0_ax1_fused_4 < 8; ++ax0_ax1_fused_4) {\n    for (int32_t ax2_1 = 0; ax2_1 < 128; ++ax2_1) {\n      for (int32_t ax3_outer_1 = 0; ax3_outer_1 < 4; ++ax3_outer_1) {\n        for (int32_t ax3_inner_1 = 0; ax3_inner_1 < 16; ++ax3_inner_1) {\n          T_matmul_NN[((((ax0_ax1_fused_4 * 8192) + (ax2_1 * 64)) + (ax3_outer_1 * 16)) + ax3_inner_1)] = (T_transpose[((((ax0_ax1_fused_4 * 8192) + (ax2_1 * 64)) + (ax3_outer_1 * 16)) + ax3_inner_1)] * ph_3[(((ax2_1 * 64) + (ax3_outer_1 * 16)) + ax3_inner_1)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_5 = 0; ax0_ax1_fused_5 < 8; ++ax0_ax1_fused_5) {\n    for (int32_t ax2_2 = 0; ax2_2 < 128; ++ax2_2) {\n      for (int32_t ax3_outer_2 = 0; ax3_outer_2 < 2; ++ax3_outer_2) {\n        for (int32_t ax3_inner_2 = 0; ax3_inner_2 < 16; ++ax3_inner_2) {\n          T_strided_slice[((((ax0_ax1_fused_5 * 4096) + (ax2_2 * 32)) + (ax3_outer_2 * 16)) + ax3_inner_2)] = T_transpose[(((((ax0_ax1_fused_5 * 8192) + (ax2_2 * 64)) + (ax3_outer_2 * 16)) + ax3_inner_2) + 32)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_6 = 0; ax0_ax1_fused_6 < 8; ++ax0_ax1_fused_6) {\n    for (int32_t ax2_3 = 0; ax2_3 < 128; ++ax2_3) {\n      for (int32_t ax3_outer_3 = 0; ax3_outer_3 < 2; ++ax3_outer_3) {\n        for (int32_t ax3_inner_3 = 0; ax3_inner_3 < 16; ++ax3_inner_3) {\n          T_strided_slice_1[((((ax0_ax1_fused_6 * 4096) + (ax2_3 * 32)) + (ax3_outer_3 * 16)) + ax3_inner_3)] = T_transpose[((((ax0_ax1_fused_6 * 8192) + (ax2_3 * 64)) + (ax3_outer_3 * 16)) + ax3_inner_3)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_7 = 0; ax0_ax1_fused_7 < 8; ++ax0_ax1_fused_7) {\n    for (int32_t ax2_4 = 0; ax2_4 < 128; ++ax2_4) {\n      for (int32_t ax3_outer_4 = 0; ax3_outer_4 < 4; ++ax3_outer_4) {\n        for (int32_t ax3_inner_4 = 0; ax3_inner_4 < 16; ++ax3_inner_4) {\n          T_transpose[((((ax0_ax1_fused_7 * 8192) + (ax2_4 * 64)) + (ax3_outer_4 * 16)) + ax3_inner_4)] = ((2 <= ax3_outer_4) ? T_strided_slice[(((((ax0_ax1_fused_7 * 4096) + (ax2_4 * 32)) + (ax3_outer_4 * 16)) + ax3_inner_4) - 32)] : T_strided_slice_1[((((ax0_ax1_fused_7 * 4096) + (ax2_4 * 32)) + (ax3_outer_4 * 16)) + ax3_inner_4)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_8 = 0; ax0_ax1_fused_8 < 8; ++ax0_ax1_fused_8) {\n    for (int32_t ax2_5 = 0; ax2_5 < 128; ++ax2_5) {\n      for (int32_t ax3_outer_5 = 0; ax3_outer_5 < 4; ++ax3_outer_5) {\n        for (int32_t ax3_inner_5 = 0; ax3_inner_5 < 16; ++ax3_inner_5) {\n          T_transpose[((((ax0_ax1_fused_8 * 8192) + (ax2_5 * 64)) + (ax3_outer_5 * 16)) + ax3_inner_5)] = (T_transpose[((((ax0_ax1_fused_8 * 8192) + (ax2_5 * 64)) + (ax3_outer_5 * 16)) + ax3_inner_5)] * ph_4[(((ax2_5 * 64) + (ax3_outer_5 * 16)) + ax3_inner_5)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_9 = 0; ax0_ax1_fused_9 < 8; ++ax0_ax1_fused_9) {\n    for (int32_t ax2_6 = 0; ax2_6 < 128; ++ax2_6) {\n      for (int32_t ax3_outer_6 = 0; ax3_outer_6 < 4; ++ax3_outer_6) {\n        for (int32_t ax3_inner_6 = 0; ax3_inner_6 < 16; ++ax3_inner_6) {\n          T_matmul_NN[((((ax0_ax1_fused_9 * 8192) + (ax2_6 * 64)) + (ax3_outer_6 * 16)) + ax3_inner_6)] = (T_matmul_NN[((((ax0_ax1_fused_9 * 8192) + (ax2_6 * 64)) + (ax3_outer_6 * 16)) + ax3_inner_6)] + T_transpose[((((ax0_ax1_fused_9 * 8192) + (ax2_6 * 64)) + (ax3_outer_6 * 16)) + ax3_inner_6)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 128; ++i0_1) {\n    for (int32_t i1_outer_1 = 0; i1_outer_1 < 32; ++i1_outer_1) {\n      for (int32_t i1_inner_1 = 0; i1_inner_1 < 16; ++i1_inner_1) {\n        T_transpose[(((i0_1 * 512) + (i1_outer_1 * 16)) + i1_inner_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 512; ++k_1) {\n          T_transpose[(((i0_1 * 512) + (i1_outer_1 * 16)) + i1_inner_1)] = (T_transpose[(((i0_1 * 512) + (i1_outer_1 * 16)) + i1_inner_1)] + (T_multiply[((i0_1 * 512) + k_1)] * ph_5[(((k_1 * 512) + (i1_outer_1 * 16)) + i1_inner_1)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_10 = 0; ax0_ax1_fused_10 < 8; ++ax0_ax1_fused_10) {\n    for (int32_t ax2_7 = 0; ax2_7 < 128; ++ax2_7) {\n      for (int32_t ax3_outer_7 = 0; ax3_outer_7 < 4; ++ax3_outer_7) {\n        for (int32_t ax3_inner_7 = 0; ax3_inner_7 < 16; ++ax3_inner_7) {\n          T_strided_slice_1[((((ax0_ax1_fused_10 * 8192) + (ax2_7 * 64)) + (ax3_outer_7 * 16)) + ax3_inner_7)] = T_transpose[((((ax2_7 * 512) + (ax0_ax1_fused_10 * 64)) + (ax3_outer_7 * 16)) + ax3_inner_7)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_11 = 0; ax0_ax1_fused_11 < 8; ++ax0_ax1_fused_11) {\n    for (int32_t ax2_8 = 0; ax2_8 < 128; ++ax2_8) {\n      for (int32_t ax3_outer_8 = 0; ax3_outer_8 < 4; ++ax3_outer_8) {\n        for (int32_t ax3_inner_8 = 0; ax3_inner_8 < 16; ++ax3_inner_8) {\n          T_transpose[((((ax0_ax1_fused_11 * 8192) + (ax2_8 * 64)) + (ax3_outer_8 * 16)) + ax3_inner_8)] = (T_strided_slice_1[((((ax0_ax1_fused_11 * 8192) + (ax2_8 * 64)) + (ax3_outer_8 * 16)) + ax3_inner_8)] * ph_3[(((ax2_8 * 64) + (ax3_outer_8 * 16)) + ax3_inner_8)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_12 = 0; ax0_ax1_fused_12 < 8; ++ax0_ax1_fused_12) {\n    for (int32_t ax2_9 = 0; ax2_9 < 128; ++ax2_9) {\n      for (int32_t ax3_outer_9 = 0; ax3_outer_9 < 2; ++ax3_outer_9) {\n        for (int32_t ax3_inner_9 = 0; ax3_inner_9 < 16; ++ax3_inner_9) {\n          T_strided_slice[((((ax0_ax1_fused_12 * 4096) + (ax2_9 * 32)) + (ax3_outer_9 * 16)) + ax3_inner_9)] = T_strided_slice_1[(((((ax0_ax1_fused_12 * 8192) + (ax2_9 * 64)) + (ax3_outer_9 * 16)) + ax3_inner_9) + 32)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_13 = 0; ax0_ax1_fused_13 < 8; ++ax0_ax1_fused_13) {\n    for (int32_t ax2_10 = 0; ax2_10 < 128; ++ax2_10) {\n      for (int32_t ax3_outer_10 = 0; ax3_outer_10 < 2; ++ax3_outer_10) {\n        for (int32_t ax3_inner_10 = 0; ax3_inner_10 < 16; ++ax3_inner_10) {\n          T_strided_slice_2[((((ax0_ax1_fused_13 * 4096) + (ax2_10 * 32)) + (ax3_outer_10 * 16)) + ax3_inner_10)] = T_strided_slice_1[((((ax0_ax1_fused_13 * 8192) + (ax2_10 * 64)) + (ax3_outer_10 * 16)) + ax3_inner_10)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_14 = 0; ax0_ax1_fused_14 < 8; ++ax0_ax1_fused_14) {\n    for (int32_t ax2_11 = 0; ax2_11 < 128; ++ax2_11) {\n      for (int32_t ax3_outer_11 = 0; ax3_outer_11 < 4; ++ax3_outer_11) {\n        for (int32_t ax3_inner_11 = 0; ax3_inner_11 < 16; ++ax3_inner_11) {\n          T_strided_slice_1[((((ax0_ax1_fused_14 * 8192) + (ax2_11 * 64)) + (ax3_outer_11 * 16)) + ax3_inner_11)] = ((2 <= ax3_outer_11) ? T_strided_slice[(((((ax0_ax1_fused_14 * 4096) + (ax2_11 * 32)) + (ax3_outer_11 * 16)) + ax3_inner_11) - 32)] : T_strided_slice_2[((((ax0_ax1_fused_14 * 4096) + (ax2_11 * 32)) + (ax3_outer_11 * 16)) + ax3_inner_11)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_15 = 0; ax0_ax1_fused_15 < 8; ++ax0_ax1_fused_15) {\n    for (int32_t ax2_12 = 0; ax2_12 < 128; ++ax2_12) {\n      for (int32_t ax3_outer_12 = 0; ax3_outer_12 < 4; ++ax3_outer_12) {\n        for (int32_t ax3_inner_12 = 0; ax3_inner_12 < 16; ++ax3_inner_12) {\n          T_strided_slice_1[((((ax0_ax1_fused_15 * 8192) + (ax2_12 * 64)) + (ax3_outer_12 * 16)) + ax3_inner_12)] = (T_strided_slice_1[((((ax0_ax1_fused_15 * 8192) + (ax2_12 * 64)) + (ax3_outer_12 * 16)) + ax3_inner_12)] * ph_4[(((ax2_12 * 64) + (ax3_outer_12 * 16)) + ax3_inner_12)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_16 = 0; ax0_ax1_fused_16 < 8; ++ax0_ax1_fused_16) {\n    for (int32_t ax2_13 = 0; ax2_13 < 128; ++ax2_13) {\n      for (int32_t ax3_outer_13 = 0; ax3_outer_13 < 4; ++ax3_outer_13) {\n        for (int32_t ax3_inner_13 = 0; ax3_inner_13 < 16; ++ax3_inner_13) {\n          T_transpose[((((ax0_ax1_fused_16 * 8192) + (ax2_13 * 64)) + (ax3_outer_13 * 16)) + ax3_inner_13)] = (T_transpose[((((ax0_ax1_fused_16 * 8192) + (ax2_13 * 64)) + (ax3_outer_13 * 16)) + ax3_inner_13)] + T_strided_slice_1[((((ax0_ax1_fused_16 * 8192) + (ax2_13 * 64)) + (ax3_outer_13 * 16)) + ax3_inner_13)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_17 = 0; ax0_ax1_fused_17 < 8; ++ax0_ax1_fused_17) {\n    for (int32_t ax2_14 = 0; ax2_14 < 128; ++ax2_14) {\n      for (int32_t ax3_outer_14 = 0; ax3_outer_14 < 8; ++ax3_outer_14) {\n        for (int32_t ax3_inner_14 = 0; ax3_inner_14 < 16; ++ax3_inner_14) {\n          T_strided_slice_1[((((ax0_ax1_fused_17 * 16384) + (ax2_14 * 128)) + (ax3_outer_14 * 16)) + ax3_inner_14)] = 0.000000e+00f;\n          for (int32_t h = 0; h < 64; ++h) {\n            T_strided_slice_1[((((ax0_ax1_fused_17 * 16384) + (ax2_14 * 128)) + (ax3_outer_14 * 16)) + ax3_inner_14)] = (T_strided_slice_1[((((ax0_ax1_fused_17 * 16384) + (ax2_14 * 128)) + (ax3_outer_14 * 16)) + ax3_inner_14)] + (T_matmul_NN[(((ax0_ax1_fused_17 * 8192) + (ax2_14 * 64)) + h)] * T_transpose[((((ax0_ax1_fused_17 * 8192) + (ax3_outer_14 * 1024)) + (ax3_inner_14 * 64)) + h)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2_outer = 0; i2_outer < 8; ++i2_outer) {\n      for (int32_t i2_inner = 0; i2_inner < 16; ++i2_inner) {\n        T_cast_1[(((i0_i1_fused * 128) + (i2_outer * 16)) + i2_inner)] = -3.402823e+38f;\n        for (int32_t k_2 = 0; k_2 < 128; ++k_2) {\n          T_cast_1[(((i0_i1_fused * 128) + (i2_outer * 16)) + i2_inner)] = max(T_cast_1[(((i0_i1_fused * 128) + (i2_outer * 16)) + i2_inner)], T_strided_slice_1[((((i0_i1_fused * 16384) + (i2_outer * 2048)) + (i2_inner * 128)) + k_2)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 8; ++i0_i1_fused_1) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 8; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          T_strided_slice_1[((((i0_i1_fused_1 * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)] = expf((T_strided_slice_1[((((i0_i1_fused_1 * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)] - T_cast_1[((i0_i1_fused_1 * 128) + i2)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_2 = 0; i0_i1_fused_2 < 8; ++i0_i1_fused_2) {\n    for (int32_t i2_outer_1 = 0; i2_outer_1 < 8; ++i2_outer_1) {\n      for (int32_t i2_inner_1 = 0; i2_inner_1 < 16; ++i2_inner_1) {\n        T_cast_1[(((i0_i1_fused_2 * 128) + (i2_outer_1 * 16)) + i2_inner_1)] = 0.000000e+00f;\n        for (int32_t k_3 = 0; k_3 < 128; ++k_3) {\n          T_cast_1[(((i0_i1_fused_2 * 128) + (i2_outer_1 * 16)) + i2_inner_1)] = (T_cast_1[(((i0_i1_fused_2 * 128) + (i2_outer_1 * 16)) + i2_inner_1)] + T_strided_slice_1[((((i0_i1_fused_2 * 16384) + (i2_outer_1 * 2048)) + (i2_inner_1 * 128)) + k_3)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_3 = 0; i0_i1_fused_3 < 8; ++i0_i1_fused_3) {\n    for (int32_t i2_1 = 0; i2_1 < 128; ++i2_1) {\n      for (int32_t i3_outer_1 = 0; i3_outer_1 < 8; ++i3_outer_1) {\n        for (int32_t i3_inner_1 = 0; i3_inner_1 < 16; ++i3_inner_1) {\n          T_strided_slice_1[((((i0_i1_fused_3 * 16384) + (i2_1 * 128)) + (i3_outer_1 * 16)) + i3_inner_1)] = (T_strided_slice_1[((((i0_i1_fused_3 * 16384) + (i2_1 * 128)) + (i3_outer_1 * 16)) + i3_inner_1)] / T_cast_1[((i0_i1_fused_3 * 128) + i2_1)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 128; ++i0_2) {\n    for (int32_t i1_outer_2 = 0; i1_outer_2 < 32; ++i1_outer_2) {\n      for (int32_t i1_inner_2 = 0; i1_inner_2 < 16; ++i1_inner_2) {\n        T_matmul_NN[(((i0_2 * 512) + (i1_outer_2 * 16)) + i1_inner_2)] = 0.000000e+00f;\n        for (int32_t k_4 = 0; k_4 < 512; ++k_4) {\n          T_matmul_NN[(((i0_2 * 512) + (i1_outer_2 * 16)) + i1_inner_2)] = (T_matmul_NN[(((i0_2 * 512) + (i1_outer_2 * 16)) + i1_inner_2)] + (T_multiply[((i0_2 * 512) + k_4)] * ph_6[(((k_4 * 512) + (i1_outer_2 * 16)) + i1_inner_2)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_18 = 0; ax0_ax1_fused_18 < 8; ++ax0_ax1_fused_18) {\n    for (int32_t ax2_15 = 0; ax2_15 < 128; ++ax2_15) {\n      for (int32_t ax3_outer_15 = 0; ax3_outer_15 < 4; ++ax3_outer_15) {\n        for (int32_t ax3_inner_15 = 0; ax3_inner_15 < 16; ++ax3_inner_15) {\n          T_transpose[((((ax0_ax1_fused_18 * 8192) + (ax2_15 * 64)) + (ax3_outer_15 * 16)) + ax3_inner_15)] = T_matmul_NN[((((ax2_15 * 512) + (ax0_ax1_fused_18 * 64)) + (ax3_outer_15 * 16)) + ax3_inner_15)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_19 = 0; ax0_ax1_fused_19 < 8; ++ax0_ax1_fused_19) {\n    for (int32_t ax2_16 = 0; ax2_16 < 128; ++ax2_16) {\n      for (int32_t ax3_outer_16 = 0; ax3_outer_16 < 4; ++ax3_outer_16) {\n        for (int32_t ax3_inner_16 = 0; ax3_inner_16 < 16; ++ax3_inner_16) {\n          T_multiply[((((ax0_ax1_fused_19 * 8192) + (ax2_16 * 64)) + (ax3_outer_16 * 16)) + ax3_inner_16)] = 0.000000e+00f;\n          for (int32_t k_5 = 0; k_5 < 128; ++k_5) {\n            T_multiply[((((ax0_ax1_fused_19 * 8192) + (ax2_16 * 64)) + (ax3_outer_16 * 16)) + ax3_inner_16)] = (T_multiply[((((ax0_ax1_fused_19 * 8192) + (ax2_16 * 64)) + (ax3_outer_16 * 16)) + ax3_inner_16)] + (T_strided_slice_1[(((ax0_ax1_fused_19 * 16384) + (ax2_16 * 128)) + k_5)] * T_transpose[((((ax0_ax1_fused_19 * 8192) + (k_5 * 64)) + (ax3_outer_16 * 16)) + ax3_inner_16)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_20 = 0; ax0_ax1_fused_20 < 128; ++ax0_ax1_fused_20) {\n    for (int32_t ax2_17 = 0; ax2_17 < 8; ++ax2_17) {\n      for (int32_t ax3_outer_17 = 0; ax3_outer_17 < 4; ++ax3_outer_17) {\n        for (int32_t ax3_inner_17 = 0; ax3_inner_17 < 16; ++ax3_inner_17) {\n          T_matmul_NN[((((ax0_ax1_fused_20 * 512) + (ax2_17 * 64)) + (ax3_outer_17 * 16)) + ax3_inner_17)] = T_multiply[((((ax2_17 * 8192) + (ax0_ax1_fused_20 * 64)) + (ax3_outer_17 * 16)) + ax3_inner_17)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_3 = 0; i0_3 < 128; ++i0_3) {\n    for (int32_t i1_outer_3 = 0; i1_outer_3 < 32; ++i1_outer_3) {\n      for (int32_t i1_inner_3 = 0; i1_inner_3 < 16; ++i1_inner_3) {\n        T_transpose[(((i0_3 * 512) + (i1_outer_3 * 16)) + i1_inner_3)] = 0.000000e+00f;\n        for (int32_t k_6 = 0; k_6 < 512; ++k_6) {\n          T_transpose[(((i0_3 * 512) + (i1_outer_3 * 16)) + i1_inner_3)] = (T_transpose[(((i0_3 * 512) + (i1_outer_3 * 16)) + i1_inner_3)] + (T_matmul_NN[((i0_3 * 512) + k_6)] * ph_7[(((k_6 * 512) + (i1_outer_3 * 16)) + i1_inner_3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_21 = 0; ax0_ax1_fused_21 < 128; ++ax0_ax1_fused_21) {\n    for (int32_t ax2_outer_3 = 0; ax2_outer_3 < 32; ++ax2_outer_3) {\n      for (int32_t ax2_inner_3 = 0; ax2_inner_3 < 16; ++ax2_inner_3) {\n        T_transpose[(((ax0_ax1_fused_21 * 512) + (ax2_outer_3 * 16)) + ax2_inner_3)] = (T_transpose[(((ax0_ax1_fused_21 * 512) + (ax2_outer_3 * 16)) + ax2_inner_3)] + T_cast[(((ax0_ax1_fused_21 * 512) + (ax2_outer_3 * 16)) + ax2_inner_3)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_outer_1 = 0; ax0_outer_1 < 32; ++ax0_outer_1) {\n    for (int32_t ax0_inner_1 = 0; ax0_inner_1 < 16; ++ax0_inner_1) {\n      T_cast_1[((ax0_outer_1 * 16) + ax0_inner_1)] = ph_8[((ax0_outer_1 * 16) + ax0_inner_1)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_22 = 0; ax0_ax1_fused_22 < 128; ++ax0_ax1_fused_22) {\n    for (int32_t ax2_outer_4 = 0; ax2_outer_4 < 32; ++ax2_outer_4) {\n      for (int32_t ax2_inner_4 = 0; ax2_inner_4 < 16; ++ax2_inner_4) {\n        T_multiply[(((ax0_ax1_fused_22 * 512) + (ax2_outer_4 * 16)) + ax2_inner_4)] = (T_transpose[(((ax0_ax1_fused_22 * 512) + (ax2_outer_4 * 16)) + ax2_inner_4)] * T_transpose[(((ax0_ax1_fused_22 * 512) + (ax2_outer_4 * 16)) + ax2_inner_4)]);\n      }\n    }\n  }\n  for (int32_t ax1_outer_2 = 0; ax1_outer_2 < 8; ++ax1_outer_2) {\n    for (int32_t ax1_inner_2 = 0; ax1_inner_2 < 16; ++ax1_inner_2) {\n      T_multiply_red[((ax1_outer_2 * 16) + ax1_inner_2)] = 0.000000e+00f;\n      for (int32_t k2_1 = 0; k2_1 < 512; ++k2_1) {\n        T_multiply_red[((ax1_outer_2 * 16) + ax1_inner_2)] = (T_multiply_red[((ax1_outer_2 * 16) + ax1_inner_2)] + T_multiply[(((ax1_outer_2 * 8192) + (ax1_inner_2 * 512)) + k2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_23 = 0; ax0_ax1_fused_23 < 128; ++ax0_ax1_fused_23) {\n    for (int32_t ax2_outer_5 = 0; ax2_outer_5 < 32; ++ax2_outer_5) {\n      for (int32_t ax2_inner_5 = 0; ax2_inner_5 < 16; ++ax2_inner_5) {\n        T_transpose[(((ax0_ax1_fused_23 * 512) + (ax2_outer_5 * 16)) + ax2_inner_5)] = ((T_transpose[(((ax0_ax1_fused_23 * 512) + (ax2_outer_5 * 16)) + ax2_inner_5)] * T_cast_1[((ax2_outer_5 * 16) + ax2_inner_5)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax0_ax1_fused_23] * 1.953125e-03f) + 1.000000e-05f))));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 48; ++ax0_1) {\n    for (int32_t ax1_outer_3 = 0; ax1_outer_3 < 32; ++ax1_outer_3) {\n      for (int32_t ax1_inner_3 = 0; ax1_inner_3 < 16; ++ax1_inner_3) {\n        T_strided_slice[(((ax0_1 * 512) + (ax1_outer_3 * 16)) + ax1_inner_3)] = T_transpose[(((ax0_1 * 512) + (ax1_outer_3 * 16)) + ax1_inner_3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_4 = 0; i0_4 < 48; ++i0_4) {\n    for (int32_t i1_outer_4 = 0; i1_outer_4 < 86; ++i1_outer_4) {\n      for (int32_t i1_inner_4 = 0; i1_inner_4 < 16; ++i1_inner_4) {\n        T_strided_slice_1[(((i0_4 * 1376) + (i1_outer_4 * 16)) + i1_inner_4)] = 0.000000e+00f;\n        for (int32_t k_7 = 0; k_7 < 512; ++k_7) {\n          T_strided_slice_1[(((i0_4 * 1376) + (i1_outer_4 * 16)) + i1_inner_4)] = (T_strided_slice_1[(((i0_4 * 1376) + (i1_outer_4 * 16)) + i1_inner_4)] + (T_strided_slice[((i0_4 * 512) + k_7)] * ph_9[(((k_7 * 1376) + (i1_outer_4 * 16)) + i1_inner_4)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_5 = 0; i0_5 < 48; ++i0_5) {\n    for (int32_t i1_outer_5 = 0; i1_outer_5 < 86; ++i1_outer_5) {\n      for (int32_t i1_inner_5 = 0; i1_inner_5 < 16; ++i1_inner_5) {\n        T_multiply[(((i0_5 * 1376) + (i1_outer_5 * 16)) + i1_inner_5)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - T_strided_slice_1[(((i0_5 * 1376) + (i1_outer_5 * 16)) + i1_inner_5)]))));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_2 = 0; ax0_2 < 48; ++ax0_2) {\n    for (int32_t ax1_outer_4 = 0; ax1_outer_4 < 86; ++ax1_outer_4) {\n      for (int32_t ax1_inner_4 = 0; ax1_inner_4 < 16; ++ax1_inner_4) {\n        T_multiply[(((ax0_2 * 1376) + (ax1_outer_4 * 16)) + ax1_inner_4)] = (T_strided_slice_1[(((ax0_2 * 1376) + (ax1_outer_4 * 16)) + ax1_inner_4)] * T_multiply[(((ax0_2 * 1376) + (ax1_outer_4 * 16)) + ax1_inner_4)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_6 = 0; i0_6 < 48; ++i0_6) {\n    for (int32_t i1_outer_6 = 0; i1_outer_6 < 86; ++i1_outer_6) {\n      for (int32_t i1_inner_6 = 0; i1_inner_6 < 16; ++i1_inner_6) {\n        T_strided_slice_1[(((i0_6 * 1376) + (i1_outer_6 * 16)) + i1_inner_6)] = 0.000000e+00f;\n        for (int32_t k_8 = 0; k_8 < 512; ++k_8) {\n          T_strided_slice_1[(((i0_6 * 1376) + (i1_outer_6 * 16)) + i1_inner_6)] = (T_strided_slice_1[(((i0_6 * 1376) + (i1_outer_6 * 16)) + i1_inner_6)] + (T_strided_slice[((i0_6 * 512) + k_8)] * ph_10[(((k_8 * 1376) + (i1_outer_6 * 16)) + i1_inner_6)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_3 = 0; ax0_3 < 48; ++ax0_3) {\n    for (int32_t ax1_outer_5 = 0; ax1_outer_5 < 86; ++ax1_outer_5) {\n      for (int32_t ax1_inner_5 = 0; ax1_inner_5 < 16; ++ax1_inner_5) {\n        T_multiply[(((ax0_3 * 1376) + (ax1_outer_5 * 16)) + ax1_inner_5)] = (T_multiply[(((ax0_3 * 1376) + (ax1_outer_5 * 16)) + ax1_inner_5)] * T_strided_slice_1[(((ax0_3 * 1376) + (ax1_outer_5 * 16)) + ax1_inner_5)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_7 = 0; i0_7 < 48; ++i0_7) {\n    for (int32_t i1_outer_7 = 0; i1_outer_7 < 86; ++i1_outer_7) {\n      for (int32_t i1_inner_7 = 0; i1_inner_7 < 16; ++i1_inner_7) {\n        T_strided_slice_1[(((i0_7 * 1376) + (i1_outer_7 * 16)) + i1_inner_7)] = 0.000000e+00f;\n        for (int32_t k_9 = 0; k_9 < 1376; ++k_9) {\n          T_strided_slice_1[(((i0_7 * 1376) + (i1_outer_7 * 16)) + i1_inner_7)] = (T_strided_slice_1[(((i0_7 * 1376) + (i1_outer_7 * 16)) + i1_inner_7)] + (T_multiply[((i0_7 * 1376) + k_9)] * ph_11[(((k_9 * 1376) + (i1_outer_7 * 16)) + i1_inner_7)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_24 = 0; ax0_ax1_fused_24 < 128; ++ax0_ax1_fused_24) {\n    for (int32_t ax2_outer_6 = 0; ax2_outer_6 < 32; ++ax2_outer_6) {\n      for (int32_t ax2_inner_6 = 0; ax2_inner_6 < 16; ++ax2_inner_6) {\n        T_matmul_NN[(((ax0_ax1_fused_24 * 512) + (ax2_outer_6 * 16)) + ax2_inner_6)] = T_strided_slice_1[(((ax0_ax1_fused_24 * 512) + (ax2_outer_6 * 16)) + ax2_inner_6)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_25 = 0; ax0_ax1_fused_25 < 128; ++ax0_ax1_fused_25) {\n    for (int32_t ax2_outer_7 = 0; ax2_outer_7 < 32; ++ax2_outer_7) {\n      for (int32_t ax2_inner_7 = 0; ax2_inner_7 < 16; ++ax2_inner_7) {\n        T_add[(((ax0_ax1_fused_25 * 512) + (ax2_outer_7 * 16)) + ax2_inner_7)] = (T_transpose[(((ax0_ax1_fused_25 * 512) + (ax2_outer_7 * 16)) + ax2_inner_7)] + T_matmul_NN[(((ax0_ax1_fused_25 * 512) + (ax2_outer_7 * 16)) + ax2_inner_7)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_37(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_22(float* __restrict__ T_matmul_NN, float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int h = 0; h < 64; ++h) {\n    T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_matmul_NN[(((((int)blockIdx.x) * 512) + ((((int)threadIdx.x) >> 7) * 64)) + h)] * T_transpose[((((((int)blockIdx.x) >> 4) * 8192) + ((((int)threadIdx.x) & 127) * 64)) + h)]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_39(float* __restrict__ T_multiply, float* __restrict__ T_strided_slice) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_28(float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_matmul_NN[(((((((int)blockIdx.x) & 7) * 8192) + ((((int)threadIdx.x) >> 6) * 512)) + ((((int)blockIdx.x) >> 3) * 64)) + (((int)threadIdx.x) & 63))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_8(float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose, float* __restrict__ ph) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[(((((int)blockIdx.x) & 7) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_cast, float* __restrict__ ph) {\n  T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ph[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_transpose[(((((((int)blockIdx.x) & 7) * 8192) + ((((int)threadIdx.x) >> 6) * 512)) + ((((int)blockIdx.x) >> 3) * 64)) + (((int)threadIdx.x) & 63))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_40(float* __restrict__ T_multiply, float* __restrict__ T_strided_slice) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_2(float* __restrict__ T_cast, float* __restrict__ T_multiply) {\n  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((32 <= (((int)threadIdx.x) & 63)) ? T_strided_slice[((((((int)blockIdx.x) * 512) + ((((int)threadIdx.x) >> 6) * 32)) + (((int)threadIdx.x) & 63)) - 32)] : T_strided_slice_1[(((((int)blockIdx.x) * 512) + ((((int)threadIdx.x) >> 6) * 32)) + (((int)threadIdx.x) & 63))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1, float* __restrict__ T_strided_slice_2) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((32 <= (((int)threadIdx.x) & 63)) ? T_strided_slice_1[((((((int)blockIdx.x) * 512) + ((((int)threadIdx.x) >> 6) * 32)) + (((int)threadIdx.x) & 63)) - 32)] : T_strided_slice_2[(((((int)blockIdx.x) * 512) + ((((int)threadIdx.x) >> 6) * 32)) + (((int)threadIdx.x) & 63))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_23(float* __restrict__ T_cast, float* __restrict__ T_strided_slice) {\n  T_cast[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int k = 0; k < 128; ++k) {\n    T_cast[((int)threadIdx.x)] = max(T_cast[((int)threadIdx.x)], T_strided_slice[((((int)threadIdx.x) * 128) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_transpose[((((((int)blockIdx.x) * 2048) + ((((int)threadIdx.x) >> 5) * 64)) + (((int)threadIdx.x) & 31)) + 32)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_16(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose, float* __restrict__ ph) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[(((((int)blockIdx.x) & 7) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_4(float* __restrict__ T_cast, float* __restrict__ T_cast_1, float* __restrict__ T_multiply_red) {\n  T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_cast_1[(((int)threadIdx.x) & 511)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))] * 1.953125e-03f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_25(float* __restrict__ T_cast, float* __restrict__ T_strided_slice) {\n  T_cast[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 128; ++k) {\n    T_cast[((int)threadIdx.x)] = (T_cast[((int)threadIdx.x)] + T_strided_slice[((((int)threadIdx.x) * 128) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_45(float* __restrict__ T_add, float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose) {\n  T_add[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_matmul_NN[(((((((int)blockIdx.x) & 7) * 8192) + ((((int)threadIdx.x) >> 6) * 512)) + ((((int)blockIdx.x) >> 3) * 64)) + (((int)threadIdx.x) & 63))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_6(float* __restrict__ T_matmul_NN, float* __restrict__ T_multiply, float* __restrict__ ph) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 512; ++k) {\n    T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_multiply[(((((int)blockIdx.x) * 1024) + ((((int)threadIdx.x) >> 9) * 512)) + k)] * ph[((k * 512) + (((int)threadIdx.x) & 511))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_32(float* __restrict__ T_cast, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_33(float* __restrict__ T_cast, float* __restrict__ ph) {\n  T_cast[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_20(float* __restrict__ T_strided_slice, float* __restrict__ ph) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[(((((int)blockIdx.x) & 7) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ T_transpose, float* __restrict__ ph) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[(((((int)blockIdx.x) & 7) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ T_multiply_red) {\n  T_multiply_red[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k2 = 0; k2 < 512; ++k2) {\n    T_multiply_red[((int)threadIdx.x)] = (T_multiply_red[((int)threadIdx.x)] + T_multiply[((((int)threadIdx.x) * 512) + k2)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_10(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_transpose[(((((int)blockIdx.x) * 2048) + ((((int)threadIdx.x) >> 5) * 64)) + (((int)threadIdx.x) & 31))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_5(float* __restrict__ T_cast, float* __restrict__ T_multiply) {\n  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_cast[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_24(float* __restrict__ T_cast, float* __restrict__ T_strided_slice) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = __expf((T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] - T_cast[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_30(float* __restrict__ T_matmul_NN, float* __restrict__ T_multiply) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_multiply[((((((((int)threadIdx.x) & 511) >> 6) * 8192) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) >> 9) * 64)) + (((int)threadIdx.x) & 63))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_43(float* __restrict__ T_multiply, float* __restrict__ T_strided_slice, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 1376; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n      T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_multiply[(((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 43) * 1376) + k)] * ph[((k * 1376) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 1376))]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_17(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_strided_slice_1[((((((int)blockIdx.x) * 2048) + ((((int)threadIdx.x) >> 5) * 64)) + (((int)threadIdx.x) & 31)) + 32)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_29(float* __restrict__ T_multiply, float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 128; ++k) {\n    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_strided_slice[(((((int)blockIdx.x) * 2048) + ((((int)threadIdx.x) >> 6) * 128)) + k)] * T_transpose[((((((int)blockIdx.x) >> 3) * 8192) + (k * 64)) + (((int)threadIdx.x) & 63))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_18(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_strided_slice_1[(((((int)blockIdx.x) * 2048) + ((((int)threadIdx.x) >> 5) * 64)) + (((int)threadIdx.x) & 31))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_31(float* __restrict__ T_matmul_NN, float* __restrict__ T_transpose, float* __restrict__ ph) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 512; ++k) {\n    T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_matmul_NN[(((((int)blockIdx.x) * 1024) + ((((int)threadIdx.x) >> 9) * 512)) + k)] * ph[((k * 512) + (((int)threadIdx.x) & 511))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(512) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ ph) {\n  T_cast[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_14(float* __restrict__ T_multiply, float* __restrict__ T_transpose, float* __restrict__ ph) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 512; ++k) {\n    T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_multiply[(((((int)blockIdx.x) * 1024) + ((((int)threadIdx.x) >> 9) * 512)) + k)] * ph[((k * 512) + (((int)threadIdx.x) & 511))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_21(float* __restrict__ T_strided_slice, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel_35(float* __restrict__ T_multiply, float* __restrict__ T_multiply_red) {\n  T_multiply_red[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k2 = 0; k2 < 512; ++k2) {\n    T_multiply_red[((int)threadIdx.x)] = (T_multiply_red[((int)threadIdx.x)] + T_multiply[((((int)threadIdx.x) * 512) + k2)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_27(float* __restrict__ T_matmul_NN, float* __restrict__ T_multiply, float* __restrict__ ph) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 512; ++k) {\n    T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_multiply[(((((int)blockIdx.x) * 1024) + ((((int)threadIdx.x) >> 9) * 512)) + k)] * ph[((k * 512) + (((int)threadIdx.x) & 511))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_34(float* __restrict__ T_multiply, float* __restrict__ T_transpose) {\n  T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_26(float* __restrict__ T_cast, float* __restrict__ T_strided_slice) {\n  T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] / T_cast[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_44(float* __restrict__ T_matmul_NN, float* __restrict__ T_strided_slice) {\n  T_matmul_NN[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_41(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 512; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n      T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_strided_slice_1[(((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 43) * 512) + k)] * ph[((k * 1376) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 1376))]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_38(float* __restrict__ T_strided_slice, float* __restrict__ T_strided_slice_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 512; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n      T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (T_strided_slice_1[(((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 43) * 512) + k)] * ph[((k * 1376) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 1376))]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_42(float* __restrict__ T_multiply, float* __restrict__ T_strided_slice) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 129) {\n    T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_multiply[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_strided_slice[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_36(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_cast[(((int)threadIdx.x) & 511)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))] * 1.953125e-03f) + 1.000000e-05f))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 128, 512), \"float32\"), ph_1: T.Buffer((512, 512), \"float32\"), ph_2: T.Buffer((512, 512), \"float32\"), ph_3: T.Buffer((512, 512), \"float32\"), ph_4: T.Buffer((512, 512), \"float32\"), ph_5: T.Buffer((512, 1376), \"float32\"), ph_6: T.Buffer((512, 1376), \"float32\"), ph_7: T.Buffer((1376, 1376), \"float32\"), ph_8: T.Buffer((512,), \"float32\"), ph_9: T.Buffer((512,), \"float32\"), ph_10: T.Buffer((1, 1, 128, 64), \"float32\"), ph_11: T.Buffer((1, 1, 128, 64), \"float32\"), T_add: T.Buffer((1, 128, 512), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_cast = T.allocate([65536], \"float32\", \"global\")\n        T_cast_1 = T.allocate([1024], \"float32\", \"global\")\n        T_multiply = T.allocate([66048], \"float32\", \"global\")\n        T_multiply_red = T.allocate([128], \"float32\", \"global\")\n        T_matmul_NN = T.allocate([65536], \"float32\", \"global\")\n        T_transpose = T.allocate([65536], \"float32\", \"global\")\n        T_strided_slice = T.allocate([32768], \"float32\", \"global\")\n        T_strided_slice_1 = T.allocate([131072], \"float32\", \"global\")\n        T_strided_slice_2 = T.allocate([32768], \"float32\", \"global\")\n        T_cast_2 = T.Buffer((65536,), data=T_cast)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                ph_12 = T.Buffer((65536,), data=ph.data)\n                T_cast_2[cse_var_1] = ph_12[cse_var_1]\n        T_cast_3 = T.Buffer((512,), data=T_cast_1)\n        for ax0_outer in T.parallel(32):\n            for ax0_inner in range(16):\n                cse_var_2: T.int32 = ax0_outer * 16 + ax0_inner\n                T_cast_3[cse_var_2] = ph_8[cse_var_2]\n        T_multiply_1 = T.Buffer((65536,), data=T_multiply)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_3: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                T_multiply_1[cse_var_3] = T_cast_2[cse_var_3] * T_cast_2[cse_var_3]\n        T_multiply_red_1 = T.Buffer((128,), data=T_multiply_red)\n        for ax1_outer, ax1_inner in T.grid(8, 16):\n            T_multiply_red_1[ax1_outer * 16 + ax1_inner] = T.float32(0)\n            for k2 in range(512):\n                cse_var_4: T.int32 = ax1_outer * 16 + ax1_inner\n                T_multiply_red_1[cse_var_4] = T_multiply_red_1[cse_var_4] + T_multiply_1[ax1_outer * 8192 + ax1_inner * 512 + k2]\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_6: T.int32 = ax2_outer * 16\n                cse_var_5: T.int32 = ax0_ax1_fused * 512 + cse_var_6 + ax2_inner\n                T_cast_4 = T.Buffer((65536,), data=T_cast)\n                T_cast_4[cse_var_5] = T_cast_2[cse_var_5] * T_cast_3[cse_var_6 + ax2_inner] * T.rsqrt(T_multiply_red_1[ax0_ax1_fused] * T.float32(0.001953125) + T.float32(1.0000000000000001e-05))\n        T_multiply_2 = T.Buffer((65536,), data=T_multiply)\n        T_cast_4 = T.Buffer((65536,), data=T_cast)\n        for ax0 in T.parallel(128):\n            for ax1_outer, ax1_inner in T.grid(32, 16):\n                cse_var_7: T.int32 = ax0 * 512 + ax1_outer * 16 + ax1_inner\n                T_multiply_2[cse_var_7] = T_cast_4[cse_var_7]\n        for i0 in T.parallel(128):\n            for i1_outer, i1_inner in T.grid(32, 16):\n                T_matmul_NN_1 = T.Buffer((65536,), data=T_matmul_NN)\n                T_matmul_NN_1[i0 * 512 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_10: T.int32 = i1_outer * 16\n                    cse_var_9: T.int32 = i0 * 512\n                    cse_var_8: T.int32 = cse_var_9 + cse_var_10 + i1_inner\n                    ph_12 = T.Buffer((262144,), data=ph_1.data)\n                    T_matmul_NN_1[cse_var_8] = T_matmul_NN_1[cse_var_8] + T_multiply_2[cse_var_9 + k] * ph_12[k * 512 + cse_var_10 + i1_inner]\n        T_transpose_1 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_11: T.int32 = ax3_outer * 16\n                T_matmul_NN_1 = T.Buffer((65536,), data=T_matmul_NN)\n                T_transpose_1[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_11 + ax3_inner] = T_matmul_NN_1[ax2 * 512 + ax0_ax1_fused * 64 + cse_var_11 + ax3_inner]\n        T_matmul_NN_1 = T.Buffer((65536,), data=T_matmul_NN)\n        ph_12 = T.Buffer((8192,), data=ph_11.data)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_14: T.int32 = ax2 * 64\n                cse_var_13: T.int32 = ax3_outer * 16\n                cse_var_12: T.int32 = ax0_ax1_fused * 8192 + cse_var_14 + cse_var_13 + ax3_inner\n                T_matmul_NN_1[cse_var_12] = T_transpose_1[cse_var_12] * ph_12[cse_var_14 + cse_var_13 + ax3_inner]\n        T_strided_slice_3 = T.Buffer((32768,), data=T_strided_slice)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 2, 16):\n                cse_var_15: T.int32 = ax3_outer * 16\n                T_strided_slice_3[ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_15 + ax3_inner] = T_transpose_1[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_15 + ax3_inner + 32]\n        T_strided_slice_4 = T.Buffer((32768,), data=T_strided_slice_1)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 2, 16):\n                cse_var_16: T.int32 = ax3_outer * 16\n                T_strided_slice_4[ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_16 + ax3_inner] = T_transpose_1[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_16 + ax3_inner]\n        T_transpose_2 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_18: T.int32 = ax3_outer * 16\n                cse_var_17: T.int32 = ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_18 + ax3_inner\n                T_transpose_2[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_18 + ax3_inner] = T.if_then_else(2 <= ax3_outer, T_strided_slice_3[cse_var_17 - 32], T_strided_slice_4[cse_var_17])\n        T_transpose_3 = T.Buffer((65536,), data=T_transpose)\n        ph_13 = T.Buffer((8192,), data=ph_10.data)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_21: T.int32 = ax2 * 64\n                cse_var_20: T.int32 = ax3_outer * 16\n                cse_var_19: T.int32 = ax0_ax1_fused * 8192 + cse_var_21 + cse_var_20 + ax3_inner\n                T_transpose_3[cse_var_19] = T_transpose_2[cse_var_19] * ph_13[cse_var_21 + cse_var_20 + ax3_inner]\n        T_matmul_NN_2 = T.Buffer((65536,), data=T_matmul_NN)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_22: T.int32 = ax0_ax1_fused * 8192 + ax2 * 64 + ax3_outer * 16 + ax3_inner\n                T_matmul_NN_2[cse_var_22] = T_matmul_NN_1[cse_var_22] + T_transpose_3[cse_var_22]\n        for i0 in T.parallel(128):\n            for i1_outer, i1_inner in T.grid(32, 16):\n                T_transpose_4 = T.Buffer((65536,), data=T_transpose)\n                T_transpose_4[i0 * 512 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_25: T.int32 = i1_outer * 16\n                    cse_var_24: T.int32 = i0 * 512\n                    cse_var_23: T.int32 = cse_var_24 + cse_var_25 + i1_inner\n                    ph_14 = T.Buffer((262144,), data=ph_2.data)\n                    T_transpose_4[cse_var_23] = T_transpose_4[cse_var_23] + T_multiply_2[cse_var_24 + k] * ph_14[k * 512 + cse_var_25 + i1_inner]\n        T_strided_slice_5 = T.Buffer((65536,), data=T_strided_slice_1)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_26: T.int32 = ax3_outer * 16\n                T_transpose_4 = T.Buffer((65536,), data=T_transpose)\n                T_strided_slice_5[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_26 + ax3_inner] = T_transpose_4[ax2 * 512 + ax0_ax1_fused * 64 + cse_var_26 + ax3_inner]\n        T_transpose_4 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_29: T.int32 = ax2 * 64\n                cse_var_28: T.int32 = ax3_outer * 16\n                cse_var_27: T.int32 = ax0_ax1_fused * 8192 + cse_var_29 + cse_var_28 + ax3_inner\n                T_transpose_4[cse_var_27] = T_strided_slice_5[cse_var_27] * ph_12[cse_var_29 + cse_var_28 + ax3_inner]\n        T_strided_slice_6 = T.Buffer((32768,), data=T_strided_slice)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 2, 16):\n                cse_var_30: T.int32 = ax3_outer * 16\n                T_strided_slice_6[ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_30 + ax3_inner] = T_strided_slice_5[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_30 + ax3_inner + 32]\n        T_strided_slice_7 = T.Buffer((32768,), data=T_strided_slice_2)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 2, 16):\n                cse_var_31: T.int32 = ax3_outer * 16\n                T_strided_slice_7[ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_31 + ax3_inner] = T_strided_slice_5[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_31 + ax3_inner]\n        T_strided_slice_8 = T.Buffer((65536,), data=T_strided_slice_1)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_33: T.int32 = ax3_outer * 16\n                cse_var_32: T.int32 = ax0_ax1_fused * 4096 + ax2 * 32 + cse_var_33 + ax3_inner\n                T_strided_slice_8[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_33 + ax3_inner] = T.if_then_else(2 <= ax3_outer, T_strided_slice_6[cse_var_32 - 32], T_strided_slice_7[cse_var_32])\n        T_strided_slice_9 = T.Buffer((65536,), data=T_strided_slice_1)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_36: T.int32 = ax2 * 64\n                cse_var_35: T.int32 = ax3_outer * 16\n                cse_var_34: T.int32 = ax0_ax1_fused * 8192 + cse_var_36 + cse_var_35 + ax3_inner\n                T_strided_slice_9[cse_var_34] = T_strided_slice_8[cse_var_34] * ph_13[cse_var_36 + cse_var_35 + ax3_inner]\n        T_transpose_5 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_37: T.int32 = ax0_ax1_fused * 8192 + ax2 * 64 + ax3_outer * 16 + ax3_inner\n                T_transpose_5[cse_var_37] = T_transpose_4[cse_var_37] + T_strided_slice_9[cse_var_37]\n        T_strided_slice_10 = T.Buffer((131072,), data=T_strided_slice_1)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 8, 16):\n                T_strided_slice_10[ax0_ax1_fused * 16384 + ax2 * 128 + ax3_outer * 16 + ax3_inner] = T.float32(0)\n                for h in range(64):\n                    cse_var_39: T.int32 = ax0_ax1_fused * 8192\n                    cse_var_38: T.int32 = ax0_ax1_fused * 16384 + ax2 * 128 + ax3_outer * 16 + ax3_inner\n                    T_strided_slice_10[cse_var_38] = T_strided_slice_10[cse_var_38] + T_matmul_NN_2[cse_var_39 + ax2 * 64 + h] * T_transpose_5[cse_var_39 + ax3_outer * 1024 + ax3_inner * 64 + h]\n        T_cast_5 = T.Buffer((1024,), data=T_cast_1)\n        for i0_i1_fused in T.parallel(8):\n            for i2_outer, i2_inner in T.grid(8, 16):\n                T_cast_5[i0_i1_fused * 128 + i2_outer * 16 + i2_inner] = T.float32(-3.4028234663852886e+38)\n                for k in range(128):\n                    cse_var_40: T.int32 = i0_i1_fused * 128 + i2_outer * 16 + i2_inner\n                    T_cast_5[cse_var_40] = T.max(T_cast_5[cse_var_40], T_strided_slice_10[i0_i1_fused * 16384 + i2_outer * 2048 + i2_inner * 128 + k])\n        T_strided_slice_11 = T.Buffer((131072,), data=T_strided_slice_1)\n        for i0_i1_fused in T.parallel(8):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_41: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                T_strided_slice_11[cse_var_41] = T.exp(T_strided_slice_10[cse_var_41] - T_cast_5[i0_i1_fused * 128 + i2])\n        T_cast_6 = T.Buffer((1024,), data=T_cast_1)\n        for i0_i1_fused in T.parallel(8):\n            for i2_outer, i2_inner in T.grid(8, 16):\n                T_cast_6[i0_i1_fused * 128 + i2_outer * 16 + i2_inner] = T.float32(0)\n                for k in range(128):\n                    cse_var_42: T.int32 = i0_i1_fused * 128 + i2_outer * 16 + i2_inner\n                    T_cast_6[cse_var_42] = T_cast_6[cse_var_42] + T_strided_slice_11[i0_i1_fused * 16384 + i2_outer * 2048 + i2_inner * 128 + k]\n        T_strided_slice_12 = T.Buffer((131072,), data=T_strided_slice_1)\n        for i0_i1_fused in T.parallel(8):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_43: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                T_strided_slice_12[cse_var_43] = T_strided_slice_11[cse_var_43] / T_cast_6[i0_i1_fused * 128 + i2]\n        for i0 in T.parallel(128):\n            for i1_outer, i1_inner in T.grid(32, 16):\n                T_matmul_NN_3 = T.Buffer((65536,), data=T_matmul_NN)\n                T_matmul_NN_3[i0 * 512 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_46: T.int32 = i1_outer * 16\n                    cse_var_45: T.int32 = i0 * 512\n                    cse_var_44: T.int32 = cse_var_45 + cse_var_46 + i1_inner\n                    ph_14 = T.Buffer((262144,), data=ph_3.data)\n                    T_matmul_NN_3[cse_var_44] = T_matmul_NN_3[cse_var_44] + T_multiply_2[cse_var_45 + k] * ph_14[k * 512 + cse_var_46 + i1_inner]\n        T_transpose_6 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                cse_var_47: T.int32 = ax3_outer * 16\n                T_matmul_NN_3 = T.Buffer((65536,), data=T_matmul_NN)\n                T_transpose_6[ax0_ax1_fused * 8192 + ax2 * 64 + cse_var_47 + ax3_inner] = T_matmul_NN_3[ax2 * 512 + ax0_ax1_fused * 64 + cse_var_47 + ax3_inner]\n        T_multiply_3 = T.Buffer((65536,), data=T_multiply)\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3_outer, ax3_inner in T.grid(128, 4, 16):\n                T_multiply_3[ax0_ax1_fused * 8192 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = T.float32(0)\n                for k in range(128):\n                    cse_var_50: T.int32 = ax0_ax1_fused * 8192\n                    cse_var_49: T.int32 = ax3_outer * 16\n                    cse_var_48: T.int32 = cse_var_50 + ax2 * 64 + cse_var_49 + ax3_inner\n                    T_multiply_3[cse_var_48] = T_multiply_3[cse_var_48] + T_strided_slice_12[ax0_ax1_fused * 16384 + ax2 * 128 + k] * T_transpose_6[cse_var_50 + k * 64 + cse_var_49 + ax3_inner]\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2, ax3_outer, ax3_inner in T.grid(8, 4, 16):\n                cse_var_51: T.int32 = ax3_outer * 16\n                T_matmul_NN_3 = T.Buffer((65536,), data=T_matmul_NN)\n                T_matmul_NN_3[ax0_ax1_fused * 512 + ax2 * 64 + cse_var_51 + ax3_inner] = T_multiply_3[ax2 * 8192 + ax0_ax1_fused * 64 + cse_var_51 + ax3_inner]\n        T_transpose_7 = T.Buffer((65536,), data=T_transpose)\n        for i0 in T.parallel(128):\n            for i1_outer, i1_inner in T.grid(32, 16):\n                T_transpose_7[i0 * 512 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_54: T.int32 = i1_outer * 16\n                    cse_var_53: T.int32 = i0 * 512\n                    cse_var_52: T.int32 = cse_var_53 + cse_var_54 + i1_inner\n                    T_matmul_NN_3 = T.Buffer((65536,), data=T_matmul_NN)\n                    ph_14 = T.Buffer((262144,), data=ph_4.data)\n                    T_transpose_7[cse_var_52] = T_transpose_7[cse_var_52] + T_matmul_NN_3[cse_var_53 + k] * ph_14[k * 512 + cse_var_54 + i1_inner]\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_55: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                T_transpose_8 = T.Buffer((65536,), data=T_transpose)\n                T_transpose_8[cse_var_55] = T_transpose_7[cse_var_55] + T_cast_4[cse_var_55]\n        T_cast_7 = T.Buffer((512,), data=T_cast_1)\n        for ax0_outer in T.parallel(32):\n            for ax0_inner in range(16):\n                cse_var_56: T.int32 = ax0_outer * 16 + ax0_inner\n                T_cast_7[cse_var_56] = ph_9[cse_var_56]\n        T_multiply_4 = T.Buffer((65536,), data=T_multiply)\n        T_transpose_8 = T.Buffer((65536,), data=T_transpose)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_57: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                T_multiply_4[cse_var_57] = T_transpose_8[cse_var_57] * T_transpose_8[cse_var_57]\n        T_multiply_red_2 = T.Buffer((128,), data=T_multiply_red)\n        for ax1_outer, ax1_inner in T.grid(8, 16):\n            T_multiply_red_2[ax1_outer * 16 + ax1_inner] = T.float32(0)\n            for k2 in range(512):\n                cse_var_58: T.int32 = ax1_outer * 16 + ax1_inner\n                T_multiply_red_2[cse_var_58] = T_multiply_red_2[cse_var_58] + T_multiply_4[ax1_outer * 8192 + ax1_inner * 512 + k2]\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_60: T.int32 = ax2_outer * 16\n                cse_var_59: T.int32 = ax0_ax1_fused * 512 + cse_var_60 + ax2_inner\n                T_transpose_9 = T.Buffer((65536,), data=T_transpose)\n                T_transpose_9[cse_var_59] = T_transpose_8[cse_var_59] * T_cast_7[cse_var_60 + ax2_inner] * T.rsqrt(T_multiply_red_2[ax0_ax1_fused] * T.float32(0.001953125) + T.float32(1.0000000000000001e-05))\n        T_strided_slice_13 = T.Buffer((24576,), data=T_strided_slice)\n        T_transpose_9 = T.Buffer((65536,), data=T_transpose)\n        for ax0 in T.parallel(48):\n            for ax1_outer, ax1_inner in T.grid(32, 16):\n                cse_var_61: T.int32 = ax0 * 512 + ax1_outer * 16 + ax1_inner\n                T_strided_slice_13[cse_var_61] = T_transpose_9[cse_var_61]\n        T_strided_slice_14 = T.Buffer((66048,), data=T_strided_slice_1)\n        for i0 in T.parallel(48):\n            for i1_outer, i1_inner in T.grid(86, 16):\n                T_strided_slice_14[i0 * 1376 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_63: T.int32 = i1_outer * 16\n                    cse_var_62: T.int32 = i0 * 1376 + cse_var_63 + i1_inner\n                    ph_14 = T.Buffer((704512,), data=ph_5.data)\n                    T_strided_slice_14[cse_var_62] = T_strided_slice_14[cse_var_62] + T_strided_slice_13[i0 * 512 + k] * ph_14[k * 1376 + cse_var_63 + i1_inner]\n        T_multiply_5 = T.Buffer((66048,), data=T_multiply)\n        for i0 in T.parallel(48):\n            for i1_outer, i1_inner in T.grid(86, 16):\n                cse_var_64: T.int32 = i0 * 1376 + i1_outer * 16 + i1_inner\n                T_multiply_5[cse_var_64] = T.sigmoid(T_strided_slice_14[cse_var_64])\n        T_multiply_6 = T.Buffer((66048,), data=T_multiply)\n        for ax0 in T.parallel(48):\n            for ax1_outer, ax1_inner in T.grid(86, 16):\n                cse_var_65: T.int32 = ax0 * 1376 + ax1_outer * 16 + ax1_inner\n                T_multiply_6[cse_var_65] = T_strided_slice_14[cse_var_65] * T_multiply_5[cse_var_65]\n        T_strided_slice_15 = T.Buffer((66048,), data=T_strided_slice_1)\n        for i0 in T.parallel(48):\n            for i1_outer, i1_inner in T.grid(86, 16):\n                T_strided_slice_15[i0 * 1376 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(512):\n                    cse_var_67: T.int32 = i1_outer * 16\n                    cse_var_66: T.int32 = i0 * 1376 + cse_var_67 + i1_inner\n                    ph_14 = T.Buffer((704512,), data=ph_6.data)\n                    T_strided_slice_15[cse_var_66] = T_strided_slice_15[cse_var_66] + T_strided_slice_13[i0 * 512 + k] * ph_14[k * 1376 + cse_var_67 + i1_inner]\n        T_multiply_7 = T.Buffer((66048,), data=T_multiply)\n        for ax0 in T.parallel(48):\n            for ax1_outer, ax1_inner in T.grid(86, 16):\n                cse_var_68: T.int32 = ax0 * 1376 + ax1_outer * 16 + ax1_inner\n                T_multiply_7[cse_var_68] = T_multiply_6[cse_var_68] * T_strided_slice_15[cse_var_68]\n        T_strided_slice_16 = T.Buffer((66048,), data=T_strided_slice_1)\n        for i0 in T.parallel(48):\n            for i1_outer, i1_inner in T.grid(86, 16):\n                T_strided_slice_16[i0 * 1376 + i1_outer * 16 + i1_inner] = T.float32(0)\n                for k in range(1376):\n                    cse_var_71: T.int32 = i1_outer * 16\n                    cse_var_70: T.int32 = i0 * 1376\n                    cse_var_69: T.int32 = cse_var_70 + cse_var_71 + i1_inner\n                    ph_14 = T.Buffer((1893376,), data=ph_7.data)\n                    T_strided_slice_16[cse_var_69] = T_strided_slice_16[cse_var_69] + T_multiply_7[cse_var_70 + k] * ph_14[k * 1376 + cse_var_71 + i1_inner]\n        T_matmul_NN_3 = T.Buffer((65536,), data=T_matmul_NN)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_72: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                T_matmul_NN_3[cse_var_72] = T_strided_slice_16[cse_var_72]\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(32, 16):\n                cse_var_73: T.int32 = ax0_ax1_fused * 512 + ax2_outer * 16 + ax2_inner\n                T_add_1 = T.Buffer((65536,), data=T_add.data)\n                T_add_1[cse_var_73] = T_transpose_9[cse_var_73] + T_matmul_NN_3[cse_var_73]",
        "op_args": "None",
        "input_shape": [
            [
                1,
                128,
                512
            ],
            [
                512,
                512
            ],
            [
                512,
                512
            ],
            [
                512,
                512
            ],
            [
                512,
                512
            ],
            [
                512,
                1376
            ],
            [
                512,
                1376
            ],
            [
                1376,
                1376
            ],
            [
                512
            ],
            [
                512
            ],
            [
                1,
                1,
                128,
                64
            ],
            [
                1,
                1,
                128,
                64
            ]
        ],
        "output_shape": [
            [
                1,
                128,
                512
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "T_add"
        ]
    },
    {
        "op_name": "wavenet",
        "c_code": "void default_function_kernel(float* T_concat, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10, float* ph_11, float* ph_12, float* ph_13, float* ph_14, float* ph_15, float* ph_16, float* ph_17) {\n  float pad_temp[14848];\n  float conv1d_ncw[12800];\n  float conv1d_ncw_1[12800];\n  float conv1d_ncw_2[12800];\n  float conv1d_ncw_3[12800];\n  float conv1d_ncw_4[12800];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2_outer = 0; i2_outer < 7; ++i2_outer) {\n      for (int32_t i2_inner = 0; i2_inner < 16; ++i2_inner) {\n        if (((i2_outer * 8) + (i2_inner >> 1)) < 51) {\n          pad_temp[(((i0_i1_fused * 102) + (i2_outer * 16)) + i2_inner)] = ((1 <= ((i2_outer * 8) + (i2_inner >> 1))) ? ph_9[((((i0_i1_fused * 100) + (i2_outer * 16)) + i2_inner) - 2)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused = 0; nn_ff_fused < 128; ++nn_ff_fused) {\n    for (int32_t yy_outer = 0; yy_outer < 7; ++yy_outer) {\n      for (int32_t yy_inner = 0; yy_inner < 16; ++yy_inner) {\n        if (((yy_outer * 4) + (yy_inner >> 2)) < 25) {\n          conv1d_ncw[(((nn_ff_fused * 100) + (yy_outer * 16)) + yy_inner)] = 0.000000e+00f;\n        }\n        if (((yy_outer * 4) + (yy_inner >> 2)) < 25) {\n          for (int32_t rc = 0; rc < 64; ++rc) {\n            for (int32_t ry = 0; ry < 3; ++ry) {\n              conv1d_ncw[(((nn_ff_fused * 100) + (yy_outer * 16)) + yy_inner)] = (conv1d_ncw[(((nn_ff_fused * 100) + (yy_outer * 16)) + yy_inner)] + (pad_temp[((((rc * 102) + (yy_outer * 16)) + yy_inner) + ry)] * ph_17[(((nn_ff_fused * 192) + (rc * 3)) + ry)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 128; ++i0_i1_fused_1) {\n    for (int32_t i2_outer_1 = 0; i2_outer_1 < 7; ++i2_outer_1) {\n      for (int32_t i2_inner_1 = 0; i2_inner_1 < 16; ++i2_inner_1) {\n        if (((i2_outer_1 * 8) + (i2_inner_1 >> 1)) < 51) {\n          pad_temp[(((i0_i1_fused_1 * 102) + (i2_outer_1 * 16)) + i2_inner_1)] = ((1 <= ((i2_outer_1 * 8) + (i2_inner_1 >> 1))) ? conv1d_ncw[((((i0_i1_fused_1 * 100) + (i2_outer_1 * 16)) + i2_inner_1) - 2)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_1 = 0; nn_ff_fused_1 < 128; ++nn_ff_fused_1) {\n    for (int32_t yy_outer_1 = 0; yy_outer_1 < 7; ++yy_outer_1) {\n      for (int32_t yy_inner_1 = 0; yy_inner_1 < 16; ++yy_inner_1) {\n        if (((yy_outer_1 * 4) + (yy_inner_1 >> 2)) < 25) {\n          conv1d_ncw_1[(((nn_ff_fused_1 * 100) + (yy_outer_1 * 16)) + yy_inner_1)] = 0.000000e+00f;\n        }\n        if (((yy_outer_1 * 4) + (yy_inner_1 >> 2)) < 25) {\n          for (int32_t rc_1 = 0; rc_1 < 128; ++rc_1) {\n            for (int32_t ry_1 = 0; ry_1 < 3; ++ry_1) {\n              conv1d_ncw_1[(((nn_ff_fused_1 * 100) + (yy_outer_1 * 16)) + yy_inner_1)] = (conv1d_ncw_1[(((nn_ff_fused_1 * 100) + (yy_outer_1 * 16)) + yy_inner_1)] + (pad_temp[((((rc_1 * 102) + (yy_outer_1 * 16)) + yy_inner_1) + ry_1)] * ph_16[(((nn_ff_fused_1 * 384) + (rc_1 * 3)) + ry_1)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_2 = 0; i0_i1_fused_2 < 128; ++i0_i1_fused_2) {\n    for (int32_t i2_outer_2 = 0; i2_outer_2 < 7; ++i2_outer_2) {\n      for (int32_t i2_inner_2 = 0; i2_inner_2 < 16; ++i2_inner_2) {\n        if (((i2_outer_2 * 4) + (i2_inner_2 >> 2)) < 25) {\n          conv1d_ncw_1[(((i0_i1_fused_2 * 100) + (i2_outer_2 * 16)) + i2_inner_2)] = tanhf(conv1d_ncw_1[(((i0_i1_fused_2 * 100) + (i2_outer_2 * 16)) + i2_inner_2)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_3 = 0; i0_i1_fused_3 < 128; ++i0_i1_fused_3) {\n    for (int32_t i2_outer_3 = 0; i2_outer_3 < 7; ++i2_outer_3) {\n      for (int32_t i2_inner_3 = 0; i2_inner_3 < 16; ++i2_inner_3) {\n        if (((i2_outer_3 * 8) + (i2_inner_3 >> 1)) < 51) {\n          pad_temp[(((i0_i1_fused_3 * 102) + (i2_outer_3 * 16)) + i2_inner_3)] = ((1 <= ((i2_outer_3 * 8) + (i2_inner_3 >> 1))) ? conv1d_ncw[((((i0_i1_fused_3 * 100) + (i2_outer_3 * 16)) + i2_inner_3) - 2)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_2 = 0; nn_ff_fused_2 < 128; ++nn_ff_fused_2) {\n    for (int32_t yy_outer_2 = 0; yy_outer_2 < 7; ++yy_outer_2) {\n      for (int32_t yy_inner_2 = 0; yy_inner_2 < 16; ++yy_inner_2) {\n        if (((yy_outer_2 * 4) + (yy_inner_2 >> 2)) < 25) {\n          conv1d_ncw[(((nn_ff_fused_2 * 100) + (yy_outer_2 * 16)) + yy_inner_2)] = 0.000000e+00f;\n        }\n        if (((yy_outer_2 * 4) + (yy_inner_2 >> 2)) < 25) {\n          for (int32_t rc_2 = 0; rc_2 < 128; ++rc_2) {\n            for (int32_t ry_2 = 0; ry_2 < 3; ++ry_2) {\n              conv1d_ncw[(((nn_ff_fused_2 * 100) + (yy_outer_2 * 16)) + yy_inner_2)] = (conv1d_ncw[(((nn_ff_fused_2 * 100) + (yy_outer_2 * 16)) + yy_inner_2)] + (pad_temp[((((rc_2 * 102) + (yy_outer_2 * 16)) + yy_inner_2) + ry_2)] * ph_15[(((nn_ff_fused_2 * 384) + (rc_2 * 3)) + ry_2)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_4 = 0; i0_i1_fused_4 < 128; ++i0_i1_fused_4) {\n    for (int32_t i2_outer_4 = 0; i2_outer_4 < 7; ++i2_outer_4) {\n      for (int32_t i2_inner_4 = 0; i2_inner_4 < 16; ++i2_inner_4) {\n        if (((i2_outer_4 * 4) + (i2_inner_4 >> 2)) < 25) {\n          conv1d_ncw[(((i0_i1_fused_4 * 100) + (i2_outer_4 * 16)) + i2_inner_4)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - conv1d_ncw[(((i0_i1_fused_4 * 100) + (i2_outer_4 * 16)) + i2_inner_4)]))));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2_outer = 0; ax2_outer < 7; ++ax2_outer) {\n      for (int32_t ax2_inner = 0; ax2_inner < 16; ++ax2_inner) {\n        if (((ax2_outer * 4) + (ax2_inner >> 2)) < 25) {\n          conv1d_ncw_1[(((ax0_ax1_fused * 100) + (ax2_outer * 16)) + ax2_inner)] = (conv1d_ncw_1[(((ax0_ax1_fused * 100) + (ax2_outer * 16)) + ax2_inner)] + conv1d_ncw[(((ax0_ax1_fused * 100) + (ax2_outer * 16)) + ax2_inner)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_5 = 0; i0_i1_fused_5 < 128; ++i0_i1_fused_5) {\n    for (int32_t i2_outer_5 = 0; i2_outer_5 < 7; ++i2_outer_5) {\n      for (int32_t i2_inner_5 = 0; i2_inner_5 < 16; ++i2_inner_5) {\n        if (((i2_outer_5 * 4) + (i2_inner_5 >> 2)) < 25) {\n          conv1d_ncw[(((i0_i1_fused_5 * 100) + (i2_outer_5 * 16)) + i2_inner_5)] = conv1d_ncw_1[(((i0_i1_fused_5 * 100) + (i2_outer_5 * 16)) + i2_inner_5)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_3 = 0; nn_ff_fused_3 < 128; ++nn_ff_fused_3) {\n    for (int32_t yy_outer_3 = 0; yy_outer_3 < 7; ++yy_outer_3) {\n      for (int32_t yy_inner_3 = 0; yy_inner_3 < 16; ++yy_inner_3) {\n        if (((yy_outer_3 * 4) + (yy_inner_3 >> 2)) < 25) {\n          pad_temp[(((nn_ff_fused_3 * 100) + (yy_outer_3 * 16)) + yy_inner_3)] = 0.000000e+00f;\n        }\n        if (((yy_outer_3 * 4) + (yy_inner_3 >> 2)) < 25) {\n          for (int32_t rc_3 = 0; rc_3 < 128; ++rc_3) {\n            pad_temp[(((nn_ff_fused_3 * 100) + (yy_outer_3 * 16)) + yy_inner_3)] = (pad_temp[(((nn_ff_fused_3 * 100) + (yy_outer_3 * 16)) + yy_inner_3)] + (conv1d_ncw[(((rc_3 * 100) + (yy_outer_3 * 16)) + yy_inner_3)] * ph_14[((nn_ff_fused_3 * 128) + rc_3)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_4 = 0; nn_ff_fused_4 < 128; ++nn_ff_fused_4) {\n    for (int32_t yy_outer_4 = 0; yy_outer_4 < 7; ++yy_outer_4) {\n      for (int32_t yy_inner_4 = 0; yy_inner_4 < 16; ++yy_inner_4) {\n        if (((yy_outer_4 * 4) + (yy_inner_4 >> 2)) < 25) {\n          conv1d_ncw[(((nn_ff_fused_4 * 100) + (yy_outer_4 * 16)) + yy_inner_4)] = 0.000000e+00f;\n        }\n        if (((yy_outer_4 * 4) + (yy_inner_4 >> 2)) < 25) {\n          for (int32_t rc_4 = 0; rc_4 < 128; ++rc_4) {\n            conv1d_ncw[(((nn_ff_fused_4 * 100) + (yy_outer_4 * 16)) + yy_inner_4)] = (conv1d_ncw[(((nn_ff_fused_4 * 100) + (yy_outer_4 * 16)) + yy_inner_4)] + (conv1d_ncw_1[(((rc_4 * 100) + (yy_outer_4 * 16)) + yy_inner_4)] * ph_13[((nn_ff_fused_4 * 64) + rc_4)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 128; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_outer_1 = 0; ax2_outer_1 < 7; ++ax2_outer_1) {\n      for (int32_t ax2_inner_1 = 0; ax2_inner_1 < 16; ++ax2_inner_1) {\n        if (((ax2_outer_1 * 4) + (ax2_inner_1 >> 2)) < 25) {\n          conv1d_ncw[(((ax0_ax1_fused_1 * 100) + (ax2_outer_1 * 16)) + ax2_inner_1)] = (pad_temp[(((ax0_ax1_fused_1 * 100) + (ax2_outer_1 * 16)) + ax2_inner_1)] + conv1d_ncw[(((ax0_ax1_fused_1 * 100) + (ax2_outer_1 * 16)) + ax2_inner_1)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_6 = 0; i0_i1_fused_6 < 128; ++i0_i1_fused_6) {\n    for (int32_t i2_outer_6 = 0; i2_outer_6 < 7; ++i2_outer_6) {\n      for (int32_t i2_inner_6 = 0; i2_inner_6 < 16; ++i2_inner_6) {\n        if (((i2_outer_6 * 2) + (i2_inner_6 >> 3)) < 13) {\n          pad_temp[(((i0_i1_fused_6 * 104) + (i2_outer_6 * 16)) + i2_inner_6)] = ((1 <= ((i2_outer_6 * 4) + (i2_inner_6 >> 2))) ? conv1d_ncw[((((i0_i1_fused_6 * 100) + (i2_outer_6 * 16)) + i2_inner_6) - 4)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_5 = 0; nn_ff_fused_5 < 128; ++nn_ff_fused_5) {\n    for (int32_t yy_outer_5 = 0; yy_outer_5 < 7; ++yy_outer_5) {\n      for (int32_t yy_inner_5 = 0; yy_inner_5 < 16; ++yy_inner_5) {\n        if (((yy_outer_5 * 4) + (yy_inner_5 >> 2)) < 25) {\n          conv1d_ncw_1[(((nn_ff_fused_5 * 100) + (yy_outer_5 * 16)) + yy_inner_5)] = 0.000000e+00f;\n        }\n        if (((yy_outer_5 * 4) + (yy_inner_5 >> 2)) < 25) {\n          for (int32_t rc_5 = 0; rc_5 < 128; ++rc_5) {\n            for (int32_t ry_3 = 0; ry_3 < 3; ++ry_3) {\n              conv1d_ncw_1[(((nn_ff_fused_5 * 100) + (yy_outer_5 * 16)) + yy_inner_5)] = (conv1d_ncw_1[(((nn_ff_fused_5 * 100) + (yy_outer_5 * 16)) + yy_inner_5)] + (pad_temp[((((rc_5 * 104) + (yy_outer_5 * 16)) + (ry_3 * 2)) + yy_inner_5)] * ph_12[(((nn_ff_fused_5 * 384) + (rc_5 * 3)) + ry_3)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_7 = 0; i0_i1_fused_7 < 128; ++i0_i1_fused_7) {\n    for (int32_t i2_outer_7 = 0; i2_outer_7 < 7; ++i2_outer_7) {\n      for (int32_t i2_inner_7 = 0; i2_inner_7 < 16; ++i2_inner_7) {\n        if (((i2_outer_7 * 4) + (i2_inner_7 >> 2)) < 25) {\n          conv1d_ncw_1[(((i0_i1_fused_7 * 100) + (i2_outer_7 * 16)) + i2_inner_7)] = tanhf(conv1d_ncw_1[(((i0_i1_fused_7 * 100) + (i2_outer_7 * 16)) + i2_inner_7)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_8 = 0; i0_i1_fused_8 < 128; ++i0_i1_fused_8) {\n    for (int32_t i2_outer_8 = 0; i2_outer_8 < 7; ++i2_outer_8) {\n      for (int32_t i2_inner_8 = 0; i2_inner_8 < 16; ++i2_inner_8) {\n        if (((i2_outer_8 * 2) + (i2_inner_8 >> 3)) < 13) {\n          pad_temp[(((i0_i1_fused_8 * 104) + (i2_outer_8 * 16)) + i2_inner_8)] = ((1 <= ((i2_outer_8 * 4) + (i2_inner_8 >> 2))) ? conv1d_ncw[((((i0_i1_fused_8 * 100) + (i2_outer_8 * 16)) + i2_inner_8) - 4)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_6 = 0; nn_ff_fused_6 < 128; ++nn_ff_fused_6) {\n    for (int32_t yy_outer_6 = 0; yy_outer_6 < 7; ++yy_outer_6) {\n      for (int32_t yy_inner_6 = 0; yy_inner_6 < 16; ++yy_inner_6) {\n        if (((yy_outer_6 * 4) + (yy_inner_6 >> 2)) < 25) {\n          conv1d_ncw_2[(((nn_ff_fused_6 * 100) + (yy_outer_6 * 16)) + yy_inner_6)] = 0.000000e+00f;\n        }\n        if (((yy_outer_6 * 4) + (yy_inner_6 >> 2)) < 25) {\n          for (int32_t rc_6 = 0; rc_6 < 128; ++rc_6) {\n            for (int32_t ry_4 = 0; ry_4 < 3; ++ry_4) {\n              conv1d_ncw_2[(((nn_ff_fused_6 * 100) + (yy_outer_6 * 16)) + yy_inner_6)] = (conv1d_ncw_2[(((nn_ff_fused_6 * 100) + (yy_outer_6 * 16)) + yy_inner_6)] + (pad_temp[((((rc_6 * 104) + (yy_outer_6 * 16)) + (ry_4 * 2)) + yy_inner_6)] * ph_11[(((nn_ff_fused_6 * 384) + (rc_6 * 3)) + ry_4)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_9 = 0; i0_i1_fused_9 < 128; ++i0_i1_fused_9) {\n    for (int32_t i2_outer_9 = 0; i2_outer_9 < 7; ++i2_outer_9) {\n      for (int32_t i2_inner_9 = 0; i2_inner_9 < 16; ++i2_inner_9) {\n        if (((i2_outer_9 * 4) + (i2_inner_9 >> 2)) < 25) {\n          conv1d_ncw_2[(((i0_i1_fused_9 * 100) + (i2_outer_9 * 16)) + i2_inner_9)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - conv1d_ncw_2[(((i0_i1_fused_9 * 100) + (i2_outer_9 * 16)) + i2_inner_9)]))));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 128; ++ax0_ax1_fused_2) {\n    for (int32_t ax2_outer_2 = 0; ax2_outer_2 < 7; ++ax2_outer_2) {\n      for (int32_t ax2_inner_2 = 0; ax2_inner_2 < 16; ++ax2_inner_2) {\n        if (((ax2_outer_2 * 4) + (ax2_inner_2 >> 2)) < 25) {\n          conv1d_ncw_1[(((ax0_ax1_fused_2 * 100) + (ax2_outer_2 * 16)) + ax2_inner_2)] = (conv1d_ncw_1[(((ax0_ax1_fused_2 * 100) + (ax2_outer_2 * 16)) + ax2_inner_2)] + conv1d_ncw_2[(((ax0_ax1_fused_2 * 100) + (ax2_outer_2 * 16)) + ax2_inner_2)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_10 = 0; i0_i1_fused_10 < 128; ++i0_i1_fused_10) {\n    for (int32_t i2_outer_10 = 0; i2_outer_10 < 7; ++i2_outer_10) {\n      for (int32_t i2_inner_10 = 0; i2_inner_10 < 16; ++i2_inner_10) {\n        if (((i2_outer_10 * 4) + (i2_inner_10 >> 2)) < 25) {\n          conv1d_ncw_2[(((i0_i1_fused_10 * 100) + (i2_outer_10 * 16)) + i2_inner_10)] = conv1d_ncw_1[(((i0_i1_fused_10 * 100) + (i2_outer_10 * 16)) + i2_inner_10)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_7 = 0; nn_ff_fused_7 < 128; ++nn_ff_fused_7) {\n    for (int32_t yy_outer_7 = 0; yy_outer_7 < 7; ++yy_outer_7) {\n      for (int32_t yy_inner_7 = 0; yy_inner_7 < 16; ++yy_inner_7) {\n        if (((yy_outer_7 * 4) + (yy_inner_7 >> 2)) < 25) {\n          pad_temp[(((nn_ff_fused_7 * 100) + (yy_outer_7 * 16)) + yy_inner_7)] = 0.000000e+00f;\n        }\n        if (((yy_outer_7 * 4) + (yy_inner_7 >> 2)) < 25) {\n          for (int32_t rc_7 = 0; rc_7 < 128; ++rc_7) {\n            pad_temp[(((nn_ff_fused_7 * 100) + (yy_outer_7 * 16)) + yy_inner_7)] = (pad_temp[(((nn_ff_fused_7 * 100) + (yy_outer_7 * 16)) + yy_inner_7)] + (conv1d_ncw_2[(((rc_7 * 100) + (yy_outer_7 * 16)) + yy_inner_7)] * ph_10[((nn_ff_fused_7 * 128) + rc_7)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_8 = 0; nn_ff_fused_8 < 128; ++nn_ff_fused_8) {\n    for (int32_t yy_outer_8 = 0; yy_outer_8 < 7; ++yy_outer_8) {\n      for (int32_t yy_inner_8 = 0; yy_inner_8 < 16; ++yy_inner_8) {\n        if (((yy_outer_8 * 4) + (yy_inner_8 >> 2)) < 25) {\n          conv1d_ncw_2[(((nn_ff_fused_8 * 100) + (yy_outer_8 * 16)) + yy_inner_8)] = 0.000000e+00f;\n        }\n        if (((yy_outer_8 * 4) + (yy_inner_8 >> 2)) < 25) {\n          for (int32_t rc_8 = 0; rc_8 < 128; ++rc_8) {\n            conv1d_ncw_2[(((nn_ff_fused_8 * 100) + (yy_outer_8 * 16)) + yy_inner_8)] = (conv1d_ncw_2[(((nn_ff_fused_8 * 100) + (yy_outer_8 * 16)) + yy_inner_8)] + (conv1d_ncw_1[(((rc_8 * 100) + (yy_outer_8 * 16)) + yy_inner_8)] * ph[((nn_ff_fused_8 * 64) + rc_8)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_3 = 0; ax0_ax1_fused_3 < 128; ++ax0_ax1_fused_3) {\n    for (int32_t ax2_outer_3 = 0; ax2_outer_3 < 7; ++ax2_outer_3) {\n      for (int32_t ax2_inner_3 = 0; ax2_inner_3 < 16; ++ax2_inner_3) {\n        if (((ax2_outer_3 * 4) + (ax2_inner_3 >> 2)) < 25) {\n          conv1d_ncw_2[(((ax0_ax1_fused_3 * 100) + (ax2_outer_3 * 16)) + ax2_inner_3)] = (pad_temp[(((ax0_ax1_fused_3 * 100) + (ax2_outer_3 * 16)) + ax2_inner_3)] + conv1d_ncw_2[(((ax0_ax1_fused_3 * 100) + (ax2_outer_3 * 16)) + ax2_inner_3)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_11 = 0; i0_i1_fused_11 < 128; ++i0_i1_fused_11) {\n    for (int32_t i2_outer_11 = 0; i2_outer_11 < 7; ++i2_outer_11) {\n      for (int32_t i2_inner_11 = 0; i2_inner_11 < 16; ++i2_inner_11) {\n        if (((i2_outer_11 * 4) + (i2_inner_11 >> 2)) < 27) {\n          pad_temp[(((i0_i1_fused_11 * 108) + (i2_outer_11 * 16)) + i2_inner_11)] = ((1 <= ((i2_outer_11 * 2) + (i2_inner_11 >> 3))) ? conv1d_ncw_2[((((i0_i1_fused_11 * 100) + (i2_outer_11 * 16)) + i2_inner_11) - 8)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_9 = 0; nn_ff_fused_9 < 128; ++nn_ff_fused_9) {\n    for (int32_t yy_outer_9 = 0; yy_outer_9 < 7; ++yy_outer_9) {\n      for (int32_t yy_inner_9 = 0; yy_inner_9 < 16; ++yy_inner_9) {\n        if (((yy_outer_9 * 4) + (yy_inner_9 >> 2)) < 25) {\n          conv1d_ncw_1[(((nn_ff_fused_9 * 100) + (yy_outer_9 * 16)) + yy_inner_9)] = 0.000000e+00f;\n        }\n        if (((yy_outer_9 * 4) + (yy_inner_9 >> 2)) < 25) {\n          for (int32_t rc_9 = 0; rc_9 < 128; ++rc_9) {\n            for (int32_t ry_5 = 0; ry_5 < 3; ++ry_5) {\n              conv1d_ncw_1[(((nn_ff_fused_9 * 100) + (yy_outer_9 * 16)) + yy_inner_9)] = (conv1d_ncw_1[(((nn_ff_fused_9 * 100) + (yy_outer_9 * 16)) + yy_inner_9)] + (pad_temp[((((rc_9 * 108) + (yy_outer_9 * 16)) + (ry_5 * 4)) + yy_inner_9)] * ph_8[(((nn_ff_fused_9 * 384) + (rc_9 * 3)) + ry_5)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_12 = 0; i0_i1_fused_12 < 128; ++i0_i1_fused_12) {\n    for (int32_t i2_outer_12 = 0; i2_outer_12 < 7; ++i2_outer_12) {\n      for (int32_t i2_inner_12 = 0; i2_inner_12 < 16; ++i2_inner_12) {\n        if (((i2_outer_12 * 4) + (i2_inner_12 >> 2)) < 25) {\n          conv1d_ncw_1[(((i0_i1_fused_12 * 100) + (i2_outer_12 * 16)) + i2_inner_12)] = tanhf(conv1d_ncw_1[(((i0_i1_fused_12 * 100) + (i2_outer_12 * 16)) + i2_inner_12)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_13 = 0; i0_i1_fused_13 < 128; ++i0_i1_fused_13) {\n    for (int32_t i2_outer_13 = 0; i2_outer_13 < 7; ++i2_outer_13) {\n      for (int32_t i2_inner_13 = 0; i2_inner_13 < 16; ++i2_inner_13) {\n        if (((i2_outer_13 * 4) + (i2_inner_13 >> 2)) < 27) {\n          pad_temp[(((i0_i1_fused_13 * 108) + (i2_outer_13 * 16)) + i2_inner_13)] = ((1 <= ((i2_outer_13 * 2) + (i2_inner_13 >> 3))) ? conv1d_ncw_2[((((i0_i1_fused_13 * 100) + (i2_outer_13 * 16)) + i2_inner_13) - 8)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_10 = 0; nn_ff_fused_10 < 128; ++nn_ff_fused_10) {\n    for (int32_t yy_outer_10 = 0; yy_outer_10 < 7; ++yy_outer_10) {\n      for (int32_t yy_inner_10 = 0; yy_inner_10 < 16; ++yy_inner_10) {\n        if (((yy_outer_10 * 4) + (yy_inner_10 >> 2)) < 25) {\n          conv1d_ncw_3[(((nn_ff_fused_10 * 100) + (yy_outer_10 * 16)) + yy_inner_10)] = 0.000000e+00f;\n        }\n        if (((yy_outer_10 * 4) + (yy_inner_10 >> 2)) < 25) {\n          for (int32_t rc_10 = 0; rc_10 < 128; ++rc_10) {\n            for (int32_t ry_6 = 0; ry_6 < 3; ++ry_6) {\n              conv1d_ncw_3[(((nn_ff_fused_10 * 100) + (yy_outer_10 * 16)) + yy_inner_10)] = (conv1d_ncw_3[(((nn_ff_fused_10 * 100) + (yy_outer_10 * 16)) + yy_inner_10)] + (pad_temp[((((rc_10 * 108) + (yy_outer_10 * 16)) + (ry_6 * 4)) + yy_inner_10)] * ph_7[(((nn_ff_fused_10 * 384) + (rc_10 * 3)) + ry_6)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_14 = 0; i0_i1_fused_14 < 128; ++i0_i1_fused_14) {\n    for (int32_t i2_outer_14 = 0; i2_outer_14 < 7; ++i2_outer_14) {\n      for (int32_t i2_inner_14 = 0; i2_inner_14 < 16; ++i2_inner_14) {\n        if (((i2_outer_14 * 4) + (i2_inner_14 >> 2)) < 25) {\n          conv1d_ncw_3[(((i0_i1_fused_14 * 100) + (i2_outer_14 * 16)) + i2_inner_14)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - conv1d_ncw_3[(((i0_i1_fused_14 * 100) + (i2_outer_14 * 16)) + i2_inner_14)]))));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_4 = 0; ax0_ax1_fused_4 < 128; ++ax0_ax1_fused_4) {\n    for (int32_t ax2_outer_4 = 0; ax2_outer_4 < 7; ++ax2_outer_4) {\n      for (int32_t ax2_inner_4 = 0; ax2_inner_4 < 16; ++ax2_inner_4) {\n        if (((ax2_outer_4 * 4) + (ax2_inner_4 >> 2)) < 25) {\n          conv1d_ncw_1[(((ax0_ax1_fused_4 * 100) + (ax2_outer_4 * 16)) + ax2_inner_4)] = (conv1d_ncw_1[(((ax0_ax1_fused_4 * 100) + (ax2_outer_4 * 16)) + ax2_inner_4)] + conv1d_ncw_3[(((ax0_ax1_fused_4 * 100) + (ax2_outer_4 * 16)) + ax2_inner_4)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_15 = 0; i0_i1_fused_15 < 128; ++i0_i1_fused_15) {\n    for (int32_t i2_outer_15 = 0; i2_outer_15 < 7; ++i2_outer_15) {\n      for (int32_t i2_inner_15 = 0; i2_inner_15 < 16; ++i2_inner_15) {\n        if (((i2_outer_15 * 4) + (i2_inner_15 >> 2)) < 25) {\n          conv1d_ncw_3[(((i0_i1_fused_15 * 100) + (i2_outer_15 * 16)) + i2_inner_15)] = conv1d_ncw_1[(((i0_i1_fused_15 * 100) + (i2_outer_15 * 16)) + i2_inner_15)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_11 = 0; nn_ff_fused_11 < 128; ++nn_ff_fused_11) {\n    for (int32_t yy_outer_11 = 0; yy_outer_11 < 7; ++yy_outer_11) {\n      for (int32_t yy_inner_11 = 0; yy_inner_11 < 16; ++yy_inner_11) {\n        if (((yy_outer_11 * 4) + (yy_inner_11 >> 2)) < 25) {\n          pad_temp[(((nn_ff_fused_11 * 100) + (yy_outer_11 * 16)) + yy_inner_11)] = 0.000000e+00f;\n        }\n        if (((yy_outer_11 * 4) + (yy_inner_11 >> 2)) < 25) {\n          for (int32_t rc_11 = 0; rc_11 < 128; ++rc_11) {\n            pad_temp[(((nn_ff_fused_11 * 100) + (yy_outer_11 * 16)) + yy_inner_11)] = (pad_temp[(((nn_ff_fused_11 * 100) + (yy_outer_11 * 16)) + yy_inner_11)] + (conv1d_ncw_3[(((rc_11 * 100) + (yy_outer_11 * 16)) + yy_inner_11)] * ph_6[((nn_ff_fused_11 * 128) + rc_11)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_12 = 0; nn_ff_fused_12 < 128; ++nn_ff_fused_12) {\n    for (int32_t yy_outer_12 = 0; yy_outer_12 < 7; ++yy_outer_12) {\n      for (int32_t yy_inner_12 = 0; yy_inner_12 < 16; ++yy_inner_12) {\n        if (((yy_outer_12 * 4) + (yy_inner_12 >> 2)) < 25) {\n          conv1d_ncw_3[(((nn_ff_fused_12 * 100) + (yy_outer_12 * 16)) + yy_inner_12)] = 0.000000e+00f;\n        }\n        if (((yy_outer_12 * 4) + (yy_inner_12 >> 2)) < 25) {\n          for (int32_t rc_12 = 0; rc_12 < 128; ++rc_12) {\n            conv1d_ncw_3[(((nn_ff_fused_12 * 100) + (yy_outer_12 * 16)) + yy_inner_12)] = (conv1d_ncw_3[(((nn_ff_fused_12 * 100) + (yy_outer_12 * 16)) + yy_inner_12)] + (conv1d_ncw_1[(((rc_12 * 100) + (yy_outer_12 * 16)) + yy_inner_12)] * ph_5[((nn_ff_fused_12 * 64) + rc_12)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_5 = 0; ax0_ax1_fused_5 < 128; ++ax0_ax1_fused_5) {\n    for (int32_t ax2_outer_5 = 0; ax2_outer_5 < 7; ++ax2_outer_5) {\n      for (int32_t ax2_inner_5 = 0; ax2_inner_5 < 16; ++ax2_inner_5) {\n        if (((ax2_outer_5 * 4) + (ax2_inner_5 >> 2)) < 25) {\n          conv1d_ncw_3[(((ax0_ax1_fused_5 * 100) + (ax2_outer_5 * 16)) + ax2_inner_5)] = (pad_temp[(((ax0_ax1_fused_5 * 100) + (ax2_outer_5 * 16)) + ax2_inner_5)] + conv1d_ncw_3[(((ax0_ax1_fused_5 * 100) + (ax2_outer_5 * 16)) + ax2_inner_5)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_16 = 0; i0_i1_fused_16 < 128; ++i0_i1_fused_16) {\n    for (int32_t i2_outer_16 = 0; i2_outer_16 < 8; ++i2_outer_16) {\n      for (int32_t i2_inner_16 = 0; i2_inner_16 < 16; ++i2_inner_16) {\n        if (((i2_outer_16 * 4) + (i2_inner_16 >> 2)) < 29) {\n          pad_temp[(((i0_i1_fused_16 * 116) + (i2_outer_16 * 16)) + i2_inner_16)] = ((1 <= i2_outer_16) ? conv1d_ncw_3[((((i0_i1_fused_16 * 100) + (i2_outer_16 * 16)) + i2_inner_16) - 16)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_13 = 0; nn_ff_fused_13 < 128; ++nn_ff_fused_13) {\n    for (int32_t yy_outer_13 = 0; yy_outer_13 < 7; ++yy_outer_13) {\n      for (int32_t yy_inner_13 = 0; yy_inner_13 < 16; ++yy_inner_13) {\n        if (((yy_outer_13 * 4) + (yy_inner_13 >> 2)) < 25) {\n          conv1d_ncw_1[(((nn_ff_fused_13 * 100) + (yy_outer_13 * 16)) + yy_inner_13)] = 0.000000e+00f;\n        }\n        if (((yy_outer_13 * 4) + (yy_inner_13 >> 2)) < 25) {\n          for (int32_t rc_13 = 0; rc_13 < 128; ++rc_13) {\n            for (int32_t ry_7 = 0; ry_7 < 3; ++ry_7) {\n              conv1d_ncw_1[(((nn_ff_fused_13 * 100) + (yy_outer_13 * 16)) + yy_inner_13)] = (conv1d_ncw_1[(((nn_ff_fused_13 * 100) + (yy_outer_13 * 16)) + yy_inner_13)] + (pad_temp[((((rc_13 * 116) + (yy_outer_13 * 16)) + (ry_7 * 8)) + yy_inner_13)] * ph_4[(((nn_ff_fused_13 * 384) + (rc_13 * 3)) + ry_7)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_17 = 0; i0_i1_fused_17 < 128; ++i0_i1_fused_17) {\n    for (int32_t i2_outer_17 = 0; i2_outer_17 < 7; ++i2_outer_17) {\n      for (int32_t i2_inner_17 = 0; i2_inner_17 < 16; ++i2_inner_17) {\n        if (((i2_outer_17 * 4) + (i2_inner_17 >> 2)) < 25) {\n          conv1d_ncw_1[(((i0_i1_fused_17 * 100) + (i2_outer_17 * 16)) + i2_inner_17)] = tanhf(conv1d_ncw_1[(((i0_i1_fused_17 * 100) + (i2_outer_17 * 16)) + i2_inner_17)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_18 = 0; i0_i1_fused_18 < 128; ++i0_i1_fused_18) {\n    for (int32_t i2_outer_18 = 0; i2_outer_18 < 8; ++i2_outer_18) {\n      for (int32_t i2_inner_18 = 0; i2_inner_18 < 16; ++i2_inner_18) {\n        if (((i2_outer_18 * 4) + (i2_inner_18 >> 2)) < 29) {\n          pad_temp[(((i0_i1_fused_18 * 116) + (i2_outer_18 * 16)) + i2_inner_18)] = ((1 <= i2_outer_18) ? conv1d_ncw_3[((((i0_i1_fused_18 * 100) + (i2_outer_18 * 16)) + i2_inner_18) - 16)] : 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_14 = 0; nn_ff_fused_14 < 128; ++nn_ff_fused_14) {\n    for (int32_t yy_outer_14 = 0; yy_outer_14 < 7; ++yy_outer_14) {\n      for (int32_t yy_inner_14 = 0; yy_inner_14 < 16; ++yy_inner_14) {\n        if (((yy_outer_14 * 4) + (yy_inner_14 >> 2)) < 25) {\n          conv1d_ncw_4[(((nn_ff_fused_14 * 100) + (yy_outer_14 * 16)) + yy_inner_14)] = 0.000000e+00f;\n        }\n        if (((yy_outer_14 * 4) + (yy_inner_14 >> 2)) < 25) {\n          for (int32_t rc_14 = 0; rc_14 < 128; ++rc_14) {\n            for (int32_t ry_8 = 0; ry_8 < 3; ++ry_8) {\n              conv1d_ncw_4[(((nn_ff_fused_14 * 100) + (yy_outer_14 * 16)) + yy_inner_14)] = (conv1d_ncw_4[(((nn_ff_fused_14 * 100) + (yy_outer_14 * 16)) + yy_inner_14)] + (pad_temp[((((rc_14 * 116) + (yy_outer_14 * 16)) + (ry_8 * 8)) + yy_inner_14)] * ph_3[(((nn_ff_fused_14 * 384) + (rc_14 * 3)) + ry_8)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_19 = 0; i0_i1_fused_19 < 128; ++i0_i1_fused_19) {\n    for (int32_t i2_outer_19 = 0; i2_outer_19 < 7; ++i2_outer_19) {\n      for (int32_t i2_inner_19 = 0; i2_inner_19 < 16; ++i2_inner_19) {\n        if (((i2_outer_19 * 4) + (i2_inner_19 >> 2)) < 25) {\n          conv1d_ncw_4[(((i0_i1_fused_19 * 100) + (i2_outer_19 * 16)) + i2_inner_19)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - conv1d_ncw_4[(((i0_i1_fused_19 * 100) + (i2_outer_19 * 16)) + i2_inner_19)]))));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_6 = 0; ax0_ax1_fused_6 < 128; ++ax0_ax1_fused_6) {\n    for (int32_t ax2_outer_6 = 0; ax2_outer_6 < 7; ++ax2_outer_6) {\n      for (int32_t ax2_inner_6 = 0; ax2_inner_6 < 16; ++ax2_inner_6) {\n        if (((ax2_outer_6 * 4) + (ax2_inner_6 >> 2)) < 25) {\n          conv1d_ncw_1[(((ax0_ax1_fused_6 * 100) + (ax2_outer_6 * 16)) + ax2_inner_6)] = (conv1d_ncw_1[(((ax0_ax1_fused_6 * 100) + (ax2_outer_6 * 16)) + ax2_inner_6)] + conv1d_ncw_4[(((ax0_ax1_fused_6 * 100) + (ax2_outer_6 * 16)) + ax2_inner_6)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_20 = 0; i0_i1_fused_20 < 128; ++i0_i1_fused_20) {\n    for (int32_t i2_outer_20 = 0; i2_outer_20 < 7; ++i2_outer_20) {\n      for (int32_t i2_inner_20 = 0; i2_inner_20 < 16; ++i2_inner_20) {\n        if (((i2_outer_20 * 4) + (i2_inner_20 >> 2)) < 25) {\n          conv1d_ncw_4[(((i0_i1_fused_20 * 100) + (i2_outer_20 * 16)) + i2_inner_20)] = conv1d_ncw_1[(((i0_i1_fused_20 * 100) + (i2_outer_20 * 16)) + i2_inner_20)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_15 = 0; nn_ff_fused_15 < 128; ++nn_ff_fused_15) {\n    for (int32_t yy_outer_15 = 0; yy_outer_15 < 7; ++yy_outer_15) {\n      for (int32_t yy_inner_15 = 0; yy_inner_15 < 16; ++yy_inner_15) {\n        if (((yy_outer_15 * 4) + (yy_inner_15 >> 2)) < 25) {\n          pad_temp[(((nn_ff_fused_15 * 100) + (yy_outer_15 * 16)) + yy_inner_15)] = 0.000000e+00f;\n        }\n        if (((yy_outer_15 * 4) + (yy_inner_15 >> 2)) < 25) {\n          for (int32_t rc_15 = 0; rc_15 < 128; ++rc_15) {\n            pad_temp[(((nn_ff_fused_15 * 100) + (yy_outer_15 * 16)) + yy_inner_15)] = (pad_temp[(((nn_ff_fused_15 * 100) + (yy_outer_15 * 16)) + yy_inner_15)] + (conv1d_ncw_4[(((rc_15 * 100) + (yy_outer_15 * 16)) + yy_inner_15)] * ph_2[((nn_ff_fused_15 * 128) + rc_15)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_16 = 0; nn_ff_fused_16 < 128; ++nn_ff_fused_16) {\n    for (int32_t yy_outer_16 = 0; yy_outer_16 < 7; ++yy_outer_16) {\n      for (int32_t yy_inner_16 = 0; yy_inner_16 < 16; ++yy_inner_16) {\n        if (((yy_outer_16 * 4) + (yy_inner_16 >> 2)) < 25) {\n          conv1d_ncw_4[(((nn_ff_fused_16 * 100) + (yy_outer_16 * 16)) + yy_inner_16)] = 0.000000e+00f;\n        }\n        if (((yy_outer_16 * 4) + (yy_inner_16 >> 2)) < 25) {\n          for (int32_t rc_16 = 0; rc_16 < 128; ++rc_16) {\n            conv1d_ncw_4[(((nn_ff_fused_16 * 100) + (yy_outer_16 * 16)) + yy_inner_16)] = (conv1d_ncw_4[(((nn_ff_fused_16 * 100) + (yy_outer_16 * 16)) + yy_inner_16)] + (conv1d_ncw_1[(((rc_16 * 100) + (yy_outer_16 * 16)) + yy_inner_16)] * ph_1[((nn_ff_fused_16 * 64) + rc_16)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_7 = 0; ax0_ax1_fused_7 < 128; ++ax0_ax1_fused_7) {\n    for (int32_t ax2_outer_7 = 0; ax2_outer_7 < 7; ++ax2_outer_7) {\n      for (int32_t ax2_inner_7 = 0; ax2_inner_7 < 16; ++ax2_inner_7) {\n        if (((ax2_outer_7 * 4) + (ax2_inner_7 >> 2)) < 25) {\n          conv1d_ncw_4[(((ax0_ax1_fused_7 * 100) + (ax2_outer_7 * 16)) + ax2_inner_7)] = (pad_temp[(((ax0_ax1_fused_7 * 100) + (ax2_outer_7 * 16)) + ax2_inner_7)] + conv1d_ncw_4[(((ax0_ax1_fused_7 * 100) + (ax2_outer_7 * 16)) + ax2_inner_7)]);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_8 = 0; ax0_ax1_fused_8 < 512; ++ax0_ax1_fused_8) {\n    for (int32_t ax2_outer_8 = 0; ax2_outer_8 < 7; ++ax2_outer_8) {\n      for (int32_t ax2_inner_8 = 0; ax2_inner_8 < 16; ++ax2_inner_8) {\n        if (((ax2_outer_8 * 4) + (ax2_inner_8 >> 2)) < 25) {\n          T_concat[(((ax0_ax1_fused_8 * 100) + (ax2_outer_8 * 16)) + ax2_inner_8)] = ((384 <= ax0_ax1_fused_8) ? conv1d_ncw_4[((((ax0_ax1_fused_8 * 100) + (ax2_outer_8 * 16)) + ax2_inner_8) - 38400)] : ((256 <= ax0_ax1_fused_8) ? conv1d_ncw_3[((((ax0_ax1_fused_8 * 100) + (ax2_outer_8 * 16)) + ax2_inner_8) - 25600)] : ((128 <= ax0_ax1_fused_8) ? conv1d_ncw_2[((((ax0_ax1_fused_8 * 100) + (ax2_outer_8 * 16)) + ax2_inner_8) - 12800)] : conv1d_ncw[(((ax0_ax1_fused_8 * 100) + (ax2_outer_8 * 16)) + ax2_inner_8)])));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_18(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_42(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_39(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 116) + (ry * 8)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 102) + ry) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_5(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 51) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((1 <= (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 1)) % 51)) ? conv1d_ncw[((((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 51) * 100) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 102)) - 2)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_41(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_25(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 108) + (ry * 4)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_27(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 27) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((8 <= (((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) % 108)) ? conv1d_ncw[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 27) * 100) + (((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) % 108)) - 8)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_30(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_35(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 29) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((16 <= (((((int)blockIdx.x) * 96) + ((int)threadIdx.x)) % 116)) ? conv1d_ncw[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 29) * 100) + (((((int)blockIdx.x) * 96) + ((int)threadIdx.x)) % 116)) - 16)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_17(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 104) + (ry * 2)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanhf(conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_24(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 27) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((8 <= (((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) % 108)) ? conv1d_ncw[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 27) * 100) + (((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) % 108)) - 8)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_28(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 108) + (ry * 4)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_6(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 102) + ry) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_23(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_40(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_2(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 51) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((1 <= (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 1)) % 51)) ? conv1d_ncw[((((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 51) * 100) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 102)) - 2)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_26(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanhf(conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw_1[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 64) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_21(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 128) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_34(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_46(float* __restrict__ T_concat, float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1, float* __restrict__ conv1d_ncw_2, float* __restrict__ conv1d_ncw_3) {\n  T_concat[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((75 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))) ? conv1d_ncw[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) - 38400)] : ((25 <= ((int)blockIdx.x)) ? conv1d_ncw_1[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) - 25600)] : ((25 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9))) ? conv1d_ncw_2[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) - 12800)] : conv1d_ncw_3[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_4(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanhf(conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_33(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw_1[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 64) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_14(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 104) + (ry * 2)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_45(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_16(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((1 <= (((((int)blockIdx.x) * 22) + (((int)threadIdx.x) >> 2)) % 26)) ? conv1d_ncw[((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 13) * 100) + (((((int)blockIdx.x) * 88) + ((int)threadIdx.x)) % 104)) - 4)] : 0.000000e+00f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_8(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_32(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 128) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_43(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 128) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_20(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 51) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((1 <= (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 1)) % 51)) ? ph[((((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 51) * 100) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 102)) - 2)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_38(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 29) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((16 <= (((((int)blockIdx.x) * 96) + ((int)threadIdx.x)) % 116)) ? conv1d_ncw[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 29) * 100) + (((((int)blockIdx.x) * 96) + ((int)threadIdx.x)) % 116)) - 16)] : 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_44(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw_1[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 64) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_29(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_31(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_37(float* __restrict__ conv1d_ncw) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanhf(conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_36(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 116) + (ry * 8)) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 384) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv1d_ncw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_10(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 128) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 64; ++rc) {\n    for (int ry = 0; ry < 3; ++ry) {\n      if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n        conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((rc * 102) + ry) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 192) + (rc * 3)) + ry)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ conv1d_ncw, float* __restrict__ pad_temp) {\n  pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((1 <= (((((int)blockIdx.x) * 22) + (((int)threadIdx.x) >> 2)) % 26)) ? conv1d_ncw[((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 13) * 100) + (((((int)blockIdx.x) * 88) + ((int)threadIdx.x)) % 104)) - 4)] : 0.000000e+00f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_22(float* __restrict__ conv1d_ncw, float* __restrict__ conv1d_ncw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n    conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 128; ++rc) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 9)) < 25) {\n      conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv1d_ncw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv1d_ncw_1[((rc * 100) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 100))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 25) * 64) + rc)]));\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 64, 100), \"float32\"), ph_1: T.Buffer((128, 64, 3), \"float32\"), ph_2: T.Buffer((128, 128, 3), \"float32\"), ph_3: T.Buffer((128, 128, 3), \"float32\"), ph_4: T.Buffer((128, 128, 1), \"float32\"), ph_5: T.Buffer((128, 64, 1), \"float32\"), ph_6: T.Buffer((128, 128, 3), \"float32\"), ph_7: T.Buffer((128, 128, 3), \"float32\"), ph_8: T.Buffer((128, 128, 1), \"float32\"), ph_9: T.Buffer((128, 64, 1), \"float32\"), ph_10: T.Buffer((128, 128, 3), \"float32\"), ph_11: T.Buffer((128, 128, 3), \"float32\"), ph_12: T.Buffer((128, 128, 1), \"float32\"), ph_13: T.Buffer((128, 64, 1), \"float32\"), ph_14: T.Buffer((128, 128, 3), \"float32\"), ph_15: T.Buffer((128, 128, 3), \"float32\"), ph_16: T.Buffer((128, 128, 1), \"float32\"), ph_17: T.Buffer((128, 64, 1), \"float32\"), T_concat: T.Buffer((1, 512, 100), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([14848], \"float32\", \"global\")\n        conv1d_ncw = T.allocate([12800], \"float32\", \"global\")\n        conv1d_ncw_1 = T.allocate([12800], \"float32\", \"global\")\n        conv1d_ncw_2 = T.allocate([12800], \"float32\", \"global\")\n        conv1d_ncw_3 = T.allocate([12800], \"float32\", \"global\")\n        conv1d_ncw_4 = T.allocate([12800], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((6528,), data=pad_temp)\n        for i0_i1_fused in T.parallel(64):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 8 + i2_inner // 2 < 51):\n                    ph_18 = T.Buffer((6400,), data=ph.data)\n                    cse_var_1: T.int32 = i2_outer * 16\n                    pad_temp_1[i0_i1_fused * 102 + cse_var_1 + i2_inner] = T.if_then_else(1 <= i2_outer * 8 + i2_inner // 2, ph_18[i0_i1_fused * 100 + cse_var_1 + i2_inner - 2], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_5 = T.Buffer((12800,), data=conv1d_ncw)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_5[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(64, 3):\n                        cse_var_3: T.int32 = yy_outer * 16\n                        cse_var_2: T.int32 = nn_ff_fused * 100 + cse_var_3 + yy_inner\n                        ph_18 = T.Buffer((24576,), data=ph_1.data)\n                        conv1d_ncw_5[cse_var_2] = conv1d_ncw_5[cse_var_2] + pad_temp_1[rc * 102 + cse_var_3 + yy_inner + ry] * ph_18[nn_ff_fused * 192 + rc * 3 + ry]\n        pad_temp_2 = T.Buffer((13056,), data=pad_temp)\n        conv1d_ncw_5 = T.Buffer((12800,), data=conv1d_ncw)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 8 + i2_inner // 2 < 51):\n                    cse_var_4: T.int32 = i2_outer * 16\n                    pad_temp_2[i0_i1_fused * 102 + cse_var_4 + i2_inner] = T.if_then_else(1 <= i2_outer * 8 + i2_inner // 2, conv1d_ncw_5[i0_i1_fused * 100 + cse_var_4 + i2_inner - 2], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_6 = T.Buffer((12800,), data=conv1d_ncw_1)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_6[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_6: T.int32 = yy_outer * 16\n                        cse_var_5: T.int32 = nn_ff_fused * 100 + cse_var_6 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_2.data)\n                        conv1d_ncw_6[cse_var_5] = conv1d_ncw_6[cse_var_5] + pad_temp_2[rc * 102 + cse_var_6 + yy_inner + ry] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_6 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_7 = T.Buffer((12800,), data=conv1d_ncw_1)\n                    cse_var_7: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_6[cse_var_7] = T.tanh(conv1d_ncw_7[cse_var_7])\n        pad_temp_3 = T.Buffer((13056,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 8 + i2_inner // 2 < 51):\n                    cse_var_8: T.int32 = i2_outer * 16\n                    pad_temp_3[i0_i1_fused * 102 + cse_var_8 + i2_inner] = T.if_then_else(1 <= i2_outer * 8 + i2_inner // 2, conv1d_ncw_5[i0_i1_fused * 100 + cse_var_8 + i2_inner - 2], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_7 = T.Buffer((12800,), data=conv1d_ncw)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_7[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_10: T.int32 = yy_outer * 16\n                        cse_var_9: T.int32 = nn_ff_fused * 100 + cse_var_10 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_3.data)\n                        conv1d_ncw_7[cse_var_9] = conv1d_ncw_7[cse_var_9] + pad_temp_3[rc * 102 + cse_var_10 + yy_inner + ry] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_7 = T.Buffer((12800,), data=conv1d_ncw)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_8 = T.Buffer((12800,), data=conv1d_ncw)\n                    cse_var_11: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_7[cse_var_11] = T.sigmoid(conv1d_ncw_8[cse_var_11])\n        conv1d_ncw_8 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_12: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_8[cse_var_12] = conv1d_ncw_6[cse_var_12] + conv1d_ncw_7[cse_var_12]\n        conv1d_ncw_9 = T.Buffer((12800,), data=conv1d_ncw)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    cse_var_13: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_9[cse_var_13] = conv1d_ncw_8[cse_var_13]\n        pad_temp_4 = T.Buffer((12800,), data=pad_temp)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    pad_temp_4[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_15: T.int32 = yy_outer * 16\n                        cse_var_14: T.int32 = nn_ff_fused * 100 + cse_var_15 + yy_inner\n                        ph_18 = T.Buffer((16384,), data=ph_4.data)\n                        pad_temp_4[cse_var_14] = pad_temp_4[cse_var_14] + conv1d_ncw_9[rc * 100 + cse_var_15 + yy_inner] * ph_18[nn_ff_fused * 128 + rc]\n        conv1d_ncw_10 = T.Buffer((12800,), data=conv1d_ncw)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_10[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_17: T.int32 = yy_outer * 16\n                        cse_var_16: T.int32 = nn_ff_fused * 100 + cse_var_17 + yy_inner\n                        conv1d_ncw_11 = T.Buffer((12800,), data=conv1d_ncw_1)\n                        ph_18 = T.Buffer((8192,), data=ph_5.data)\n                        conv1d_ncw_10[cse_var_16] = conv1d_ncw_10[cse_var_16] + conv1d_ncw_11[rc * 100 + cse_var_17 + yy_inner] * ph_18[nn_ff_fused * 64 + rc]\n        conv1d_ncw_11 = T.Buffer((12800,), data=conv1d_ncw)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_18: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_11[cse_var_18] = pad_temp_4[cse_var_18] + conv1d_ncw_10[cse_var_18]\n        pad_temp_5 = T.Buffer((13312,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 2 + i2_inner // 8 < 13):\n                    cse_var_19: T.int32 = i2_outer * 16\n                    pad_temp_5[i0_i1_fused * 104 + cse_var_19 + i2_inner] = T.if_then_else(1 <= i2_outer * 4 + i2_inner // 4, conv1d_ncw_11[i0_i1_fused * 100 + cse_var_19 + i2_inner - 4], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_12 = T.Buffer((12800,), data=conv1d_ncw_1)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_12[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_21: T.int32 = yy_outer * 16\n                        cse_var_20: T.int32 = nn_ff_fused * 100 + cse_var_21 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_6.data)\n                        conv1d_ncw_12[cse_var_20] = conv1d_ncw_12[cse_var_20] + pad_temp_5[rc * 104 + cse_var_21 + ry * 2 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_12 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_13 = T.Buffer((12800,), data=conv1d_ncw_1)\n                    cse_var_22: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_12[cse_var_22] = T.tanh(conv1d_ncw_13[cse_var_22])\n        pad_temp_6 = T.Buffer((13312,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 2 + i2_inner // 8 < 13):\n                    cse_var_23: T.int32 = i2_outer * 16\n                    pad_temp_6[i0_i1_fused * 104 + cse_var_23 + i2_inner] = T.if_then_else(1 <= i2_outer * 4 + i2_inner // 4, conv1d_ncw_11[i0_i1_fused * 100 + cse_var_23 + i2_inner - 4], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_13 = T.Buffer((12800,), data=conv1d_ncw_2)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_13[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_25: T.int32 = yy_outer * 16\n                        cse_var_24: T.int32 = nn_ff_fused * 100 + cse_var_25 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_7.data)\n                        conv1d_ncw_13[cse_var_24] = conv1d_ncw_13[cse_var_24] + pad_temp_6[rc * 104 + cse_var_25 + ry * 2 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_13 = T.Buffer((12800,), data=conv1d_ncw_2)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_14 = T.Buffer((12800,), data=conv1d_ncw_2)\n                    cse_var_26: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_13[cse_var_26] = T.sigmoid(conv1d_ncw_14[cse_var_26])\n        conv1d_ncw_14 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_27: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_14[cse_var_27] = conv1d_ncw_12[cse_var_27] + conv1d_ncw_13[cse_var_27]\n        conv1d_ncw_15 = T.Buffer((12800,), data=conv1d_ncw_2)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    cse_var_28: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_15[cse_var_28] = conv1d_ncw_14[cse_var_28]\n        pad_temp_7 = T.Buffer((12800,), data=pad_temp)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    pad_temp_7[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_30: T.int32 = yy_outer * 16\n                        cse_var_29: T.int32 = nn_ff_fused * 100 + cse_var_30 + yy_inner\n                        ph_18 = T.Buffer((16384,), data=ph_8.data)\n                        pad_temp_7[cse_var_29] = pad_temp_7[cse_var_29] + conv1d_ncw_15[rc * 100 + cse_var_30 + yy_inner] * ph_18[nn_ff_fused * 128 + rc]\n        conv1d_ncw_16 = T.Buffer((12800,), data=conv1d_ncw_2)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_16[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_32: T.int32 = yy_outer * 16\n                        cse_var_31: T.int32 = nn_ff_fused * 100 + cse_var_32 + yy_inner\n                        conv1d_ncw_17 = T.Buffer((12800,), data=conv1d_ncw_1)\n                        ph_18 = T.Buffer((8192,), data=ph_9.data)\n                        conv1d_ncw_16[cse_var_31] = conv1d_ncw_16[cse_var_31] + conv1d_ncw_17[rc * 100 + cse_var_32 + yy_inner] * ph_18[nn_ff_fused * 64 + rc]\n        conv1d_ncw_17 = T.Buffer((12800,), data=conv1d_ncw_2)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_33: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_17[cse_var_33] = pad_temp_7[cse_var_33] + conv1d_ncw_16[cse_var_33]\n        pad_temp_8 = T.Buffer((13824,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 27):\n                    cse_var_34: T.int32 = i2_outer * 16\n                    pad_temp_8[i0_i1_fused * 108 + cse_var_34 + i2_inner] = T.if_then_else(1 <= i2_outer * 2 + i2_inner // 8, conv1d_ncw_17[i0_i1_fused * 100 + cse_var_34 + i2_inner - 8], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_18 = T.Buffer((12800,), data=conv1d_ncw_1)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_18[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_36: T.int32 = yy_outer * 16\n                        cse_var_35: T.int32 = nn_ff_fused * 100 + cse_var_36 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_10.data)\n                        conv1d_ncw_18[cse_var_35] = conv1d_ncw_18[cse_var_35] + pad_temp_8[rc * 108 + cse_var_36 + ry * 4 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_18 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_19 = T.Buffer((12800,), data=conv1d_ncw_1)\n                    cse_var_37: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_18[cse_var_37] = T.tanh(conv1d_ncw_19[cse_var_37])\n        pad_temp_9 = T.Buffer((13824,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 27):\n                    cse_var_38: T.int32 = i2_outer * 16\n                    pad_temp_9[i0_i1_fused * 108 + cse_var_38 + i2_inner] = T.if_then_else(1 <= i2_outer * 2 + i2_inner // 8, conv1d_ncw_17[i0_i1_fused * 100 + cse_var_38 + i2_inner - 8], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_19 = T.Buffer((12800,), data=conv1d_ncw_3)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_19[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_40: T.int32 = yy_outer * 16\n                        cse_var_39: T.int32 = nn_ff_fused * 100 + cse_var_40 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_11.data)\n                        conv1d_ncw_19[cse_var_39] = conv1d_ncw_19[cse_var_39] + pad_temp_9[rc * 108 + cse_var_40 + ry * 4 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_19 = T.Buffer((12800,), data=conv1d_ncw_3)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_20 = T.Buffer((12800,), data=conv1d_ncw_3)\n                    cse_var_41: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_19[cse_var_41] = T.sigmoid(conv1d_ncw_20[cse_var_41])\n        conv1d_ncw_20 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_42: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_20[cse_var_42] = conv1d_ncw_18[cse_var_42] + conv1d_ncw_19[cse_var_42]\n        conv1d_ncw_21 = T.Buffer((12800,), data=conv1d_ncw_3)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    cse_var_43: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_21[cse_var_43] = conv1d_ncw_20[cse_var_43]\n        pad_temp_10 = T.Buffer((12800,), data=pad_temp)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    pad_temp_10[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_45: T.int32 = yy_outer * 16\n                        cse_var_44: T.int32 = nn_ff_fused * 100 + cse_var_45 + yy_inner\n                        ph_18 = T.Buffer((16384,), data=ph_12.data)\n                        pad_temp_10[cse_var_44] = pad_temp_10[cse_var_44] + conv1d_ncw_21[rc * 100 + cse_var_45 + yy_inner] * ph_18[nn_ff_fused * 128 + rc]\n        conv1d_ncw_22 = T.Buffer((12800,), data=conv1d_ncw_3)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_22[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_47: T.int32 = yy_outer * 16\n                        cse_var_46: T.int32 = nn_ff_fused * 100 + cse_var_47 + yy_inner\n                        conv1d_ncw_23 = T.Buffer((12800,), data=conv1d_ncw_1)\n                        ph_18 = T.Buffer((8192,), data=ph_13.data)\n                        conv1d_ncw_22[cse_var_46] = conv1d_ncw_22[cse_var_46] + conv1d_ncw_23[rc * 100 + cse_var_47 + yy_inner] * ph_18[nn_ff_fused * 64 + rc]\n        conv1d_ncw_23 = T.Buffer((12800,), data=conv1d_ncw_3)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_48: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_23[cse_var_48] = pad_temp_10[cse_var_48] + conv1d_ncw_22[cse_var_48]\n        pad_temp_11 = T.Buffer((14848,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(8, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 29):\n                    cse_var_49: T.int32 = i2_outer * 16\n                    pad_temp_11[i0_i1_fused * 116 + cse_var_49 + i2_inner] = T.if_then_else(1 <= i2_outer, conv1d_ncw_23[i0_i1_fused * 100 + cse_var_49 + i2_inner - 16], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_24 = T.Buffer((12800,), data=conv1d_ncw_1)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_24[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_51: T.int32 = yy_outer * 16\n                        cse_var_50: T.int32 = nn_ff_fused * 100 + cse_var_51 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_14.data)\n                        conv1d_ncw_24[cse_var_50] = conv1d_ncw_24[cse_var_50] + pad_temp_11[rc * 116 + cse_var_51 + ry * 8 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_24 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_25 = T.Buffer((12800,), data=conv1d_ncw_1)\n                    cse_var_52: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_24[cse_var_52] = T.tanh(conv1d_ncw_25[cse_var_52])\n        pad_temp_12 = T.Buffer((14848,), data=pad_temp)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(8, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 29):\n                    cse_var_53: T.int32 = i2_outer * 16\n                    pad_temp_12[i0_i1_fused * 116 + cse_var_53 + i2_inner] = T.if_then_else(1 <= i2_outer, conv1d_ncw_23[i0_i1_fused * 100 + cse_var_53 + i2_inner - 16], T.float32(0))\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                conv1d_ncw_25 = T.Buffer((12800,), data=conv1d_ncw_4)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_25[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc, ry in T.grid(128, 3):\n                        cse_var_55: T.int32 = yy_outer * 16\n                        cse_var_54: T.int32 = nn_ff_fused * 100 + cse_var_55 + yy_inner\n                        ph_18 = T.Buffer((49152,), data=ph_15.data)\n                        conv1d_ncw_25[cse_var_54] = conv1d_ncw_25[cse_var_54] + pad_temp_12[rc * 116 + cse_var_55 + ry * 8 + yy_inner] * ph_18[nn_ff_fused * 384 + rc * 3 + ry]\n        conv1d_ncw_25 = T.Buffer((12800,), data=conv1d_ncw_4)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    conv1d_ncw_26 = T.Buffer((12800,), data=conv1d_ncw_4)\n                    cse_var_56: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_25[cse_var_56] = T.sigmoid(conv1d_ncw_26[cse_var_56])\n        conv1d_ncw_26 = T.Buffer((12800,), data=conv1d_ncw_1)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_57: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_26[cse_var_57] = conv1d_ncw_24[cse_var_57] + conv1d_ncw_25[cse_var_57]\n        conv1d_ncw_27 = T.Buffer((12800,), data=conv1d_ncw_4)\n        for i0_i1_fused in T.parallel(128):\n            for i2_outer, i2_inner in T.grid(7, 16):\n                if T.likely(i2_outer * 4 + i2_inner // 4 < 25):\n                    cse_var_58: T.int32 = i0_i1_fused * 100 + i2_outer * 16 + i2_inner\n                    conv1d_ncw_27[cse_var_58] = conv1d_ncw_26[cse_var_58]\n        pad_temp_13 = T.Buffer((12800,), data=pad_temp)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    pad_temp_13[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_60: T.int32 = yy_outer * 16\n                        cse_var_59: T.int32 = nn_ff_fused * 100 + cse_var_60 + yy_inner\n                        ph_18 = T.Buffer((16384,), data=ph_16.data)\n                        pad_temp_13[cse_var_59] = pad_temp_13[cse_var_59] + conv1d_ncw_27[rc * 100 + cse_var_60 + yy_inner] * ph_18[nn_ff_fused * 128 + rc]\n        conv1d_ncw_28 = T.Buffer((12800,), data=conv1d_ncw_4)\n        for nn_ff_fused in T.parallel(128):\n            for yy_outer, yy_inner in T.grid(7, 16):\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    conv1d_ncw_28[nn_ff_fused * 100 + yy_outer * 16 + yy_inner] = T.float32(0)\n                if T.likely(yy_outer * 4 + yy_inner // 4 < 25):\n                    for rc in range(128):\n                        cse_var_62: T.int32 = yy_outer * 16\n                        cse_var_61: T.int32 = nn_ff_fused * 100 + cse_var_62 + yy_inner\n                        conv1d_ncw_29 = T.Buffer((12800,), data=conv1d_ncw_1)\n                        ph_18 = T.Buffer((8192,), data=ph_17.data)\n                        conv1d_ncw_28[cse_var_61] = conv1d_ncw_28[cse_var_61] + conv1d_ncw_29[rc * 100 + cse_var_62 + yy_inner] * ph_18[nn_ff_fused * 64 + rc]\n        conv1d_ncw_29 = T.Buffer((12800,), data=conv1d_ncw_4)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    cse_var_63: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    conv1d_ncw_29[cse_var_63] = pad_temp_13[cse_var_63] + conv1d_ncw_28[cse_var_63]\n        for ax0_ax1_fused in T.parallel(512):\n            for ax2_outer, ax2_inner in T.grid(7, 16):\n                if T.likely(ax2_outer * 4 + ax2_inner // 4 < 25):\n                    T_concat_1 = T.Buffer((51200,), data=T_concat.data)\n                    cse_var_64: T.int32 = ax0_ax1_fused * 100 + ax2_outer * 16 + ax2_inner\n                    T_concat_1[cse_var_64] = T.if_then_else(384 <= ax0_ax1_fused, conv1d_ncw_29[cse_var_64 - 38400], T.if_then_else(256 <= ax0_ax1_fused, conv1d_ncw_23[cse_var_64 - 25600], T.if_then_else(128 <= ax0_ax1_fused, conv1d_ncw_17[cse_var_64 - 12800], conv1d_ncw_11[cse_var_64])))",
        "op_args": "None",
        "input_shape": [
            [
                1,
                64,
                100
            ],
            [
                128,
                64,
                3
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                1
            ],
            [
                128,
                64,
                1
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                1
            ],
            [
                128,
                64,
                1
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                1
            ],
            [
                128,
                64,
                1
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                3
            ],
            [
                128,
                128,
                1
            ],
            [
                128,
                64,
                1
            ]
        ],
        "output_shape": [
            [
                1,
                512,
                100
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "T_concat"
        ]
    },
    {
        "op_name": "resnet",
        "c_code": "void default_function_kernel(float* compute, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10) {\n  float pad_temp[49152];\n  float conv2d_nchw[1016064];\n  float T_reshape[64];\n  float conv2d_nchw_1[984064];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 8; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          pad_temp[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)] = ph[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused = 0; nn_ff_fused < 64; ++nn_ff_fused) {\n    for (int32_t yy = 0; yy < 126; ++yy) {\n      for (int32_t xx_outer = 0; xx_outer < 8; ++xx_outer) {\n        for (int32_t xx_inner = 0; xx_inner < 16; ++xx_inner) {\n          if (((xx_outer * 8) + (xx_inner >> 1)) < 63) {\n            conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] = 0.000000e+00f;\n          }\n          if (((xx_outer * 8) + (xx_inner >> 1)) < 63) {\n            for (int32_t rc = 0; rc < 3; ++rc) {\n              for (int32_t ry = 0; ry < 3; ++ry) {\n                for (int32_t rx = 0; rx < 3; ++rx) {\n                  conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] = (conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] + (pad_temp[((((((rc * 16384) + (yy * 128)) + (ry * 128)) + (xx_outer * 16)) + xx_inner) + rx)] * ph_1[((((nn_ff_fused * 27) + (rc * 9)) + (ry * 3)) + rx)]));\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    T_reshape[ax0_ax1_fused] = ph_2[ax0_ax1_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 64; ++ax0_ax1_fused_1) {\n    for (int32_t ax2 = 0; ax2 < 126; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 8; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          if (((ax3_outer * 8) + (ax3_inner >> 1)) < 63) {\n            conv2d_nchw[((((ax0_ax1_fused_1 * 15876) + (ax2 * 126)) + (ax3_outer * 16)) + ax3_inner)] = (conv2d_nchw[((((ax0_ax1_fused_1 * 15876) + (ax2 * 126)) + (ax3_outer * 16)) + ax3_inner)] - T_reshape[ax0_ax1_fused_1]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 64; ++ax0_ax1_fused_2) {\n    T_reshape[ax0_ax1_fused_2] = ph_3[ax0_ax1_fused_2];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_3 = 0; ax0_ax1_fused_3 < 64; ++ax0_ax1_fused_3) {\n    T_reshape[ax0_ax1_fused_3] = (T_reshape[ax0_ax1_fused_3] + 1.000000e-05f);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 64; ++i0_i1_fused_1) {\n    T_reshape[i0_i1_fused_1] = sqrtf(T_reshape[i0_i1_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_4 = 0; ax0_ax1_fused_4 < 64; ++ax0_ax1_fused_4) {\n    for (int32_t ax2_1 = 0; ax2_1 < 126; ++ax2_1) {\n      for (int32_t ax3_outer_1 = 0; ax3_outer_1 < 8; ++ax3_outer_1) {\n        for (int32_t ax3_inner_1 = 0; ax3_inner_1 < 16; ++ax3_inner_1) {\n          if (((ax3_outer_1 * 8) + (ax3_inner_1 >> 1)) < 63) {\n            conv2d_nchw[((((ax0_ax1_fused_4 * 15876) + (ax2_1 * 126)) + (ax3_outer_1 * 16)) + ax3_inner_1)] = (conv2d_nchw[((((ax0_ax1_fused_4 * 15876) + (ax2_1 * 126)) + (ax3_outer_1 * 16)) + ax3_inner_1)] / T_reshape[ax0_ax1_fused_4]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_5 = 0; ax0_ax1_fused_5 < 64; ++ax0_ax1_fused_5) {\n    T_reshape[ax0_ax1_fused_5] = ph_4[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_6 = 0; ax0_ax1_fused_6 < 64; ++ax0_ax1_fused_6) {\n    for (int32_t ax2_2 = 0; ax2_2 < 126; ++ax2_2) {\n      for (int32_t ax3_outer_2 = 0; ax3_outer_2 < 8; ++ax3_outer_2) {\n        for (int32_t ax3_inner_2 = 0; ax3_inner_2 < 16; ++ax3_inner_2) {\n          if (((ax3_outer_2 * 8) + (ax3_inner_2 >> 1)) < 63) {\n            conv2d_nchw[((((ax0_ax1_fused_6 * 15876) + (ax2_2 * 126)) + (ax3_outer_2 * 16)) + ax3_inner_2)] = (conv2d_nchw[((((ax0_ax1_fused_6 * 15876) + (ax2_2 * 126)) + (ax3_outer_2 * 16)) + ax3_inner_2)] * T_reshape[ax0_ax1_fused_6]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_7 = 0; ax0_ax1_fused_7 < 64; ++ax0_ax1_fused_7) {\n    T_reshape[ax0_ax1_fused_7] = ph_5[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_8 = 0; ax0_ax1_fused_8 < 64; ++ax0_ax1_fused_8) {\n    for (int32_t ax2_3 = 0; ax2_3 < 126; ++ax2_3) {\n      for (int32_t ax3_outer_3 = 0; ax3_outer_3 < 8; ++ax3_outer_3) {\n        for (int32_t ax3_inner_3 = 0; ax3_inner_3 < 16; ++ax3_inner_3) {\n          if (((ax3_outer_3 * 8) + (ax3_inner_3 >> 1)) < 63) {\n            conv2d_nchw[((((ax0_ax1_fused_8 * 15876) + (ax2_3 * 126)) + (ax3_outer_3 * 16)) + ax3_inner_3)] = (conv2d_nchw[((((ax0_ax1_fused_8 * 15876) + (ax2_3 * 126)) + (ax3_outer_3 * 16)) + ax3_inner_3)] + T_reshape[ax0_ax1_fused_8]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_2 = 0; i0_i1_fused_2 < 64; ++i0_i1_fused_2) {\n    for (int32_t i2_1 = 0; i2_1 < 126; ++i2_1) {\n      for (int32_t i3_outer_1 = 0; i3_outer_1 < 8; ++i3_outer_1) {\n        for (int32_t i3_inner_1 = 0; i3_inner_1 < 16; ++i3_inner_1) {\n          if (((i3_outer_1 * 8) + (i3_inner_1 >> 1)) < 63) {\n            conv2d_nchw[((((i0_i1_fused_2 * 15876) + (i2_1 * 126)) + (i3_outer_1 * 16)) + i3_inner_1)] = max(conv2d_nchw[((((i0_i1_fused_2 * 15876) + (i2_1 * 126)) + (i3_outer_1 * 16)) + i3_inner_1)], 0.000000e+00f);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_1 = 0; nn_ff_fused_1 < 64; ++nn_ff_fused_1) {\n    for (int32_t yy_1 = 0; yy_1 < 124; ++yy_1) {\n      for (int32_t xx_outer_1 = 0; xx_outer_1 < 8; ++xx_outer_1) {\n        for (int32_t xx_inner_1 = 0; xx_inner_1 < 16; ++xx_inner_1) {\n          if (((xx_outer_1 * 4) + (xx_inner_1 >> 2)) < 31) {\n            conv2d_nchw_1[((((nn_ff_fused_1 * 15376) + (yy_1 * 124)) + (xx_outer_1 * 16)) + xx_inner_1)] = 0.000000e+00f;\n          }\n          if (((xx_outer_1 * 4) + (xx_inner_1 >> 2)) < 31) {\n            for (int32_t rc_1 = 0; rc_1 < 64; ++rc_1) {\n              for (int32_t ry_1 = 0; ry_1 < 3; ++ry_1) {\n                for (int32_t rx_1 = 0; rx_1 < 3; ++rx_1) {\n                  conv2d_nchw_1[((((nn_ff_fused_1 * 15376) + (yy_1 * 124)) + (xx_outer_1 * 16)) + xx_inner_1)] = (conv2d_nchw_1[((((nn_ff_fused_1 * 15376) + (yy_1 * 124)) + (xx_outer_1 * 16)) + xx_inner_1)] + (conv2d_nchw[((((((rc_1 * 15876) + (yy_1 * 126)) + (ry_1 * 126)) + (xx_outer_1 * 16)) + xx_inner_1) + rx_1)] * ph_6[((((nn_ff_fused_1 * 576) + (rc_1 * 9)) + (ry_1 * 3)) + rx_1)]));\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_9 = 0; ax0_ax1_fused_9 < 64; ++ax0_ax1_fused_9) {\n    T_reshape[ax0_ax1_fused_9] = ph_7[ax0_ax1_fused_9];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_10 = 0; ax0_ax1_fused_10 < 64; ++ax0_ax1_fused_10) {\n    for (int32_t ax2_4 = 0; ax2_4 < 124; ++ax2_4) {\n      for (int32_t ax3_outer_4 = 0; ax3_outer_4 < 8; ++ax3_outer_4) {\n        for (int32_t ax3_inner_4 = 0; ax3_inner_4 < 16; ++ax3_inner_4) {\n          if (((ax3_outer_4 * 4) + (ax3_inner_4 >> 2)) < 31) {\n            conv2d_nchw_1[((((ax0_ax1_fused_10 * 15376) + (ax2_4 * 124)) + (ax3_outer_4 * 16)) + ax3_inner_4)] = (conv2d_nchw_1[((((ax0_ax1_fused_10 * 15376) + (ax2_4 * 124)) + (ax3_outer_4 * 16)) + ax3_inner_4)] - T_reshape[ax0_ax1_fused_10]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_11 = 0; ax0_ax1_fused_11 < 64; ++ax0_ax1_fused_11) {\n    T_reshape[ax0_ax1_fused_11] = ph_8[ax0_ax1_fused_11];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_12 = 0; ax0_ax1_fused_12 < 64; ++ax0_ax1_fused_12) {\n    T_reshape[ax0_ax1_fused_12] = (T_reshape[ax0_ax1_fused_12] + 1.000000e-05f);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_3 = 0; i0_i1_fused_3 < 64; ++i0_i1_fused_3) {\n    T_reshape[i0_i1_fused_3] = sqrtf(T_reshape[i0_i1_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_13 = 0; ax0_ax1_fused_13 < 64; ++ax0_ax1_fused_13) {\n    for (int32_t ax2_5 = 0; ax2_5 < 124; ++ax2_5) {\n      for (int32_t ax3_outer_5 = 0; ax3_outer_5 < 8; ++ax3_outer_5) {\n        for (int32_t ax3_inner_5 = 0; ax3_inner_5 < 16; ++ax3_inner_5) {\n          if (((ax3_outer_5 * 4) + (ax3_inner_5 >> 2)) < 31) {\n            conv2d_nchw_1[((((ax0_ax1_fused_13 * 15376) + (ax2_5 * 124)) + (ax3_outer_5 * 16)) + ax3_inner_5)] = (conv2d_nchw_1[((((ax0_ax1_fused_13 * 15376) + (ax2_5 * 124)) + (ax3_outer_5 * 16)) + ax3_inner_5)] / T_reshape[ax0_ax1_fused_13]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_14 = 0; ax0_ax1_fused_14 < 64; ++ax0_ax1_fused_14) {\n    T_reshape[ax0_ax1_fused_14] = ph_9[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_15 = 0; ax0_ax1_fused_15 < 64; ++ax0_ax1_fused_15) {\n    for (int32_t ax2_6 = 0; ax2_6 < 124; ++ax2_6) {\n      for (int32_t ax3_outer_6 = 0; ax3_outer_6 < 8; ++ax3_outer_6) {\n        for (int32_t ax3_inner_6 = 0; ax3_inner_6 < 16; ++ax3_inner_6) {\n          if (((ax3_outer_6 * 4) + (ax3_inner_6 >> 2)) < 31) {\n            conv2d_nchw_1[((((ax0_ax1_fused_15 * 15376) + (ax2_6 * 124)) + (ax3_outer_6 * 16)) + ax3_inner_6)] = (conv2d_nchw_1[((((ax0_ax1_fused_15 * 15376) + (ax2_6 * 124)) + (ax3_outer_6 * 16)) + ax3_inner_6)] * T_reshape[ax0_ax1_fused_15]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_16 = 0; ax0_ax1_fused_16 < 64; ++ax0_ax1_fused_16) {\n    T_reshape[ax0_ax1_fused_16] = ph_10[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_17 = 0; ax0_ax1_fused_17 < 64; ++ax0_ax1_fused_17) {\n    for (int32_t ax2_7 = 0; ax2_7 < 124; ++ax2_7) {\n      for (int32_t ax3_outer_7 = 0; ax3_outer_7 < 8; ++ax3_outer_7) {\n        for (int32_t ax3_inner_7 = 0; ax3_inner_7 < 16; ++ax3_inner_7) {\n          if (((ax3_outer_7 * 4) + (ax3_inner_7 >> 2)) < 31) {\n            conv2d_nchw_1[((((ax0_ax1_fused_17 * 15376) + (ax2_7 * 124)) + (ax3_outer_7 * 16)) + ax3_inner_7)] = (conv2d_nchw_1[((((ax0_ax1_fused_17 * 15376) + (ax2_7 * 124)) + (ax3_outer_7 * 16)) + ax3_inner_7)] + T_reshape[ax0_ax1_fused_17]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_4 = 0; i0_i1_fused_4 < 64; ++i0_i1_fused_4) {\n    for (int32_t i2_2 = 0; i2_2 < 124; ++i2_2) {\n      for (int32_t i3_outer_2 = 0; i3_outer_2 < 8; ++i3_outer_2) {\n        for (int32_t i3_inner_2 = 0; i3_inner_2 < 16; ++i3_inner_2) {\n          if (((i3_outer_2 * 4) + (i3_inner_2 >> 2)) < 31) {\n            compute[((((i0_i1_fused_4 * 15376) + (i2_2 * 124)) + (i3_outer_2 * 16)) + i3_inner_2)] = max(conv2d_nchw_1[((((i0_i1_fused_4 * 15376) + (i2_2 * 124)) + (i3_outer_2 * 16)) + i3_inner_2)], 0.000000e+00f);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] - T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 3969)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ conv2d_nchw) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 4; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    if ((((i0_i1_fused_i2_fused_i3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nchw[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] / T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 961)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  for (int nn_ff_fused_yy_fused_xx_fused_outer = 0; nn_ff_fused_yy_fused_xx_fused_outer < 4; ++nn_ff_fused_yy_fused_xx_fused_outer) {\n    if (((nn_ff_fused_yy_fused_xx_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    }\n    for (int rc = 0; rc < 64; ++rc) {\n      for (int ry = 0; ry < 3; ++ry) {\n        for (int rx = 0; rx < 3; ++rx) {\n          if (((nn_ff_fused_yy_fused_xx_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n            conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((rc * 15876) + ((((((nn_ff_fused_yy_fused_xx_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) % 3844) / 31) * 126)) + (ry * 126)) + rx) + ((((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 124))] * ph[(((((((((nn_ff_fused_yy_fused_xx_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) % 61504) / 961) * 576) + (rc * 9)) + (ry * 3)) + rx)]));\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_10(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_18(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = sqrtf(T_reshape[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_6(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = sqrtf(T_reshape[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_24(float* __restrict__ compute, float* __restrict__ conv2d_nchw) {\n  for (int i0_i1_fused_i2_fused_i3_fused_outer = 0; i0_i1_fused_i2_fused_i3_fused_outer < 4; ++i0_i1_fused_i2_fused_i3_fused_outer) {\n    if (((i0_i1_fused_i2_fused_i3_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      compute[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = max(conv2d_nchw[(((i0_i1_fused_i2_fused_i3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))], 0.000000e+00f);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_17(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = (T_reshape[((int)threadIdx.x)] + 1.000000e-05f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 3969)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_20(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_8(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_23(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 961)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] - T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 961)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 3969)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_14(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  for (int nn_ff_fused_yy_fused_xx_fused_outer = 0; nn_ff_fused_yy_fused_xx_fused_outer < 4; ++nn_ff_fused_yy_fused_xx_fused_outer) {\n    if ((((nn_ff_fused_yy_fused_xx_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = 0.000000e+00f;\n    }\n    for (int rc = 0; rc < 3; ++rc) {\n      for (int ry = 0; ry < 3; ++ry) {\n        for (int rx = 0; rx < 3; ++rx) {\n          if ((((nn_ff_fused_yy_fused_xx_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n            conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] + (pad_temp[(((((rc * 16384) + ((((((nn_ff_fused_yy_fused_xx_fused_outer * 131072) + (((int)blockIdx.x) * 512)) + (((int)threadIdx.x) >> 1)) % 7938) / 63) * 128)) + (ry * 128)) + rx) + ((((nn_ff_fused_yy_fused_xx_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 126))] * ph[((((((((nn_ff_fused_yy_fused_xx_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 3969) * 27) + (rc * 9)) + (ry * 3)) + rx)]));\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_5(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = (T_reshape[((int)threadIdx.x)] + 1.000000e-05f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 3969) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] / T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 3969)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_21(float* __restrict__ T_reshape, float* __restrict__ conv2d_nchw) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 256) + ((int)blockIdx.x)) < 961) {\n      conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (conv2d_nchw[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * T_reshape[((((ax0_ax1_fused_ax2_fused_ax3_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 961)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ph[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_22(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_16(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 3, 128, 128), \"float32\"), ph_1: T.Buffer((64, 3, 3, 3), \"float32\"), ph_2: T.Buffer((64, 64, 3, 3), \"float32\"), ph_3: T.Buffer((1, 64, 126, 126), \"float32\"), ph_4: T.Buffer((1, 64, 126, 126), \"float32\"), ph_5: T.Buffer((1,), \"float32\"), ph_6: T.Buffer((1,), \"float32\"), ph_7: T.Buffer((1, 64, 124, 124), \"float32\"), ph_8: T.Buffer((1, 64, 124, 124), \"float32\"), ph_9: T.Buffer((1,), \"float32\"), ph_10: T.Buffer((1,), \"float32\"), compute: T.Buffer((1, 64, 124, 124), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([49152], \"float32\", \"global\")\n        conv2d_nchw = T.allocate([1016064], \"float32\", \"global\")\n        T_reshape = T.allocate([64], \"float32\", \"global\")\n        conv2d_nchw_1 = T.allocate([984064], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((49152,), data=pad_temp)\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                ph_11 = T.Buffer((49152,), data=ph.data)\n                pad_temp_1[cse_var_1] = ph_11[cse_var_1]\n        conv2d_nchw_2 = T.Buffer((1016064,), data=conv2d_nchw)\n        for nn_ff_fused in T.parallel(64):\n            for yy, xx_outer, xx_inner in T.grid(126, 8, 16):\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    conv2d_nchw_2[nn_ff_fused * 15876 + yy * 126 + xx_outer * 16 + xx_inner] = T.float32(0)\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    for rc, ry, rx in T.grid(3, 3, 3):\n                        cse_var_3: T.int32 = xx_outer * 16\n                        cse_var_2: T.int32 = nn_ff_fused * 15876 + yy * 126 + cse_var_3 + xx_inner\n                        ph_11 = T.Buffer((1728,), data=ph_1.data)\n                        conv2d_nchw_2[cse_var_2] = conv2d_nchw_2[cse_var_2] + pad_temp_1[rc * 16384 + yy * 128 + ry * 128 + cse_var_3 + xx_inner + rx] * ph_11[nn_ff_fused * 27 + rc * 9 + ry * 3 + rx]\n        T_reshape_1 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            ph_11 = T.Buffer((1016064,), data=ph_3.data)\n            T_reshape_1[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        conv2d_nchw_3 = T.Buffer((1016064,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_4: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_3[cse_var_4] = conv2d_nchw_2[cse_var_4] - T_reshape_1[ax0_ax1_fused]\n        T_reshape_2 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            ph_11 = T.Buffer((1016064,), data=ph_4.data)\n            T_reshape_2[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        T_reshape_3 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_3[ax0_ax1_fused] = T_reshape_2[ax0_ax1_fused] + T.float32(1.0000000000000001e-05)\n        T_reshape_4 = T.Buffer((64,), data=T_reshape)\n        for i0_i1_fused in T.parallel(64):\n            T_reshape_4[i0_i1_fused] = T.sqrt(T_reshape_3[i0_i1_fused])\n        conv2d_nchw_4 = T.Buffer((1016064,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_5: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_4[cse_var_5] = conv2d_nchw_3[cse_var_5] / T_reshape_4[ax0_ax1_fused]\n        T_reshape_5 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_5[ax0_ax1_fused] = ph_5[0]\n        conv2d_nchw_5 = T.Buffer((1016064,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_6: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_5[cse_var_6] = conv2d_nchw_4[cse_var_6] * T_reshape_5[ax0_ax1_fused]\n        T_reshape_6 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_6[ax0_ax1_fused] = ph_6[0]\n        conv2d_nchw_6 = T.Buffer((1016064,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_7: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_6[cse_var_7] = conv2d_nchw_5[cse_var_7] + T_reshape_6[ax0_ax1_fused]\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3_outer, i3_inner in T.grid(126, 8, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 63):\n                    conv2d_nchw_7 = T.Buffer((1016064,), data=conv2d_nchw)\n                    cse_var_8: T.int32 = i0_i1_fused * 15876 + i2 * 126 + i3_outer * 16 + i3_inner\n                    conv2d_nchw_7[cse_var_8] = T.max(conv2d_nchw_6[cse_var_8], T.float32(0))\n        conv2d_nchw_7 = T.Buffer((984064,), data=conv2d_nchw_1)\n        for nn_ff_fused in T.parallel(64):\n            for yy, xx_outer, xx_inner in T.grid(124, 8, 16):\n                if T.likely(xx_outer * 4 + xx_inner // 4 < 31):\n                    conv2d_nchw_7[nn_ff_fused * 15376 + yy * 124 + xx_outer * 16 + xx_inner] = T.float32(0)\n                if T.likely(xx_outer * 4 + xx_inner // 4 < 31):\n                    for rc, ry, rx in T.grid(64, 3, 3):\n                        cse_var_10: T.int32 = xx_outer * 16\n                        cse_var_9: T.int32 = nn_ff_fused * 15376 + yy * 124 + cse_var_10 + xx_inner\n                        conv2d_nchw_8 = T.Buffer((1016064,), data=conv2d_nchw)\n                        ph_11 = T.Buffer((36864,), data=ph_2.data)\n                        conv2d_nchw_7[cse_var_9] = conv2d_nchw_7[cse_var_9] + conv2d_nchw_8[rc * 15876 + yy * 126 + ry * 126 + cse_var_10 + xx_inner + rx] * ph_11[nn_ff_fused * 576 + rc * 9 + ry * 3 + rx]\n        T_reshape_7 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            ph_11 = T.Buffer((984064,), data=ph_7.data)\n            T_reshape_7[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        conv2d_nchw_8 = T.Buffer((984064,), data=conv2d_nchw_1)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_11: T.int32 = ax0_ax1_fused * 15376 + ax2 * 124 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_8[cse_var_11] = conv2d_nchw_7[cse_var_11] - T_reshape_7[ax0_ax1_fused]\n        T_reshape_8 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            ph_11 = T.Buffer((984064,), data=ph_8.data)\n            T_reshape_8[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        T_reshape_9 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_9[ax0_ax1_fused] = T_reshape_8[ax0_ax1_fused] + T.float32(1.0000000000000001e-05)\n        T_reshape_10 = T.Buffer((64,), data=T_reshape)\n        for i0_i1_fused in T.parallel(64):\n            T_reshape_10[i0_i1_fused] = T.sqrt(T_reshape_9[i0_i1_fused])\n        conv2d_nchw_9 = T.Buffer((984064,), data=conv2d_nchw_1)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_12: T.int32 = ax0_ax1_fused * 15376 + ax2 * 124 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_9[cse_var_12] = conv2d_nchw_8[cse_var_12] / T_reshape_10[ax0_ax1_fused]\n        T_reshape_11 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_11[ax0_ax1_fused] = ph_9[0]\n        conv2d_nchw_10 = T.Buffer((984064,), data=conv2d_nchw_1)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_13: T.int32 = ax0_ax1_fused * 15376 + ax2 * 124 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_10[cse_var_13] = conv2d_nchw_9[cse_var_13] * T_reshape_11[ax0_ax1_fused]\n        T_reshape_12 = T.Buffer((64,), data=T_reshape)\n        for ax0_ax1_fused in T.parallel(64):\n            T_reshape_12[ax0_ax1_fused] = ph_10[0]\n        conv2d_nchw_11 = T.Buffer((984064,), data=conv2d_nchw_1)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_14: T.int32 = ax0_ax1_fused * 15376 + ax2 * 124 + ax3_outer * 16 + ax3_inner\n                    conv2d_nchw_11[cse_var_14] = conv2d_nchw_10[cse_var_14] + T_reshape_12[ax0_ax1_fused]\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3_outer, i3_inner in T.grid(124, 8, 16):\n                if T.likely(i3_outer * 4 + i3_inner // 4 < 31):\n                    compute_1 = T.Buffer((984064,), data=compute.data)\n                    cse_var_15: T.int32 = i0_i1_fused * 15376 + i2 * 124 + i3_outer * 16 + i3_inner\n                    compute_1[cse_var_15] = T.max(conv2d_nchw_11[cse_var_15], T.float32(0))",
        "op_args": "None",
        "input_shape": [
            [
                1,
                3,
                128,
                128
            ],
            [
                64,
                3,
                3,
                3
            ],
            [
                64,
                64,
                3,
                3
            ],
            [
                1,
                64,
                126,
                126
            ],
            [
                1,
                64,
                126,
                126
            ],
            [
                1
            ],
            [
                1
            ],
            [
                1,
                64,
                124,
                124
            ],
            [
                1,
                64,
                124,
                124
            ],
            [
                1
            ],
            [
                1
            ]
        ],
        "output_shape": [
            [
                1,
                64,
                124,
                124
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "compute"
        ]
    },
    {
        "op_name": "squeeze",
        "c_code": "void default_function_kernel(float* T_concat, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10, float* ph_11, float* ph_12, float* ph_13, float* ph_14, float* ph_15, float* ph_16, float* ph_17, float* ph_18, float* ph_19, float* ph_20, float* ph_21, float* ph_22, float* ph_23, float* ph_24, float* ph_25) {\n  float pad_temp[49152];\n  float conv2d_nchw[29768];\n  float conv2d_nchw_1[14400];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 8; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          pad_temp[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)] = ph_13[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)];\n        }\n      }\n    }\n  }\n  for (int32_t yy = 0; yy < 122; ++yy) {\n    for (int32_t xx_outer = 0; xx_outer < 8; ++xx_outer) {\n      for (int32_t xx_inner = 0; xx_inner < 16; ++xx_inner) {\n        if (((xx_outer * 8) + (xx_inner >> 1)) < 61) {\n          conv2d_nchw[(((yy * 122) + (xx_outer * 16)) + xx_inner)] = 0.000000e+00f;\n        }\n        if (((xx_outer * 8) + (xx_inner >> 1)) < 61) {\n          for (int32_t rc = 0; rc < 3; ++rc) {\n            for (int32_t ry = 0; ry < 7; ++ry) {\n              for (int32_t rx = 0; rx < 7; ++rx) {\n                conv2d_nchw[(((yy * 122) + (xx_outer * 16)) + xx_inner)] = (conv2d_nchw[(((yy * 122) + (xx_outer * 16)) + xx_inner)] + (pad_temp[((((((rc * 16384) + (yy * 128)) + (ry * 128)) + (xx_outer * 16)) + xx_inner) + rx)] * ph_25[(((rc * 49) + (ry * 7)) + rx)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_1 = 0; i2_1 < 122; ++i2_1) {\n    for (int32_t i3_outer_1 = 0; i3_outer_1 < 8; ++i3_outer_1) {\n      for (int32_t i3_inner_1 = 0; i3_inner_1 < 16; ++i3_inner_1) {\n        if (((i3_outer_1 * 8) + (i3_inner_1 >> 1)) < 61) {\n          conv2d_nchw[(((i2_1 * 122) + (i3_outer_1 * 16)) + i3_inner_1)] = max(conv2d_nchw[(((i2_1 * 122) + (i3_outer_1 * 16)) + i3_inner_1)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t ax2 = 0; ax2 < 124; ++ax2) {\n    for (int32_t ax3_outer = 0; ax3_outer < 8; ++ax3_outer) {\n      for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n        if (((ax3_outer * 4) + (ax3_inner >> 2)) < 31) {\n          pad_temp[(((ax2 * 124) + (ax3_outer * 16)) + ax3_inner)] = (((((1 <= ax2) && (ax2 < 123)) && (1 <= ((ax3_outer * 16) + ax3_inner))) && (((ax3_outer * 16) + ax3_inner) < 123)) ? conv2d_nchw[((((ax2 * 122) + (ax3_outer * 16)) + ax3_inner) - 123)] : -3.402823e+38f);\n        }\n      }\n    }\n  }\n  for (int32_t ax2_1 = 0; ax2_1 < 122; ++ax2_1) {\n    for (int32_t ax3_outer_1 = 0; ax3_outer_1 < 8; ++ax3_outer_1) {\n      for (int32_t ax3_inner_1 = 0; ax3_inner_1 < 16; ++ax3_inner_1) {\n        if (((ax3_outer_1 * 8) + (ax3_inner_1 >> 1)) < 61) {\n          conv2d_nchw[(((ax2_1 * 122) + (ax3_outer_1 * 16)) + ax3_inner_1)] = -3.402823e+38f;\n        }\n        if (((ax3_outer_1 * 8) + (ax3_inner_1 >> 1)) < 61) {\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              conv2d_nchw[(((ax2_1 * 122) + (ax3_outer_1 * 16)) + ax3_inner_1)] = max(conv2d_nchw[(((ax2_1 * 122) + (ax3_outer_1 * 16)) + ax3_inner_1)], pad_temp[(((((ax2_1 * 124) + (rv0 * 124)) + (ax3_outer_1 * 16)) + ax3_inner_1) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_1 = 0; yy_1 < 122; ++yy_1) {\n    for (int32_t xx_outer_1 = 0; xx_outer_1 < 8; ++xx_outer_1) {\n      for (int32_t xx_inner_1 = 0; xx_inner_1 < 16; ++xx_inner_1) {\n        if (((xx_outer_1 * 8) + (xx_inner_1 >> 1)) < 61) {\n          pad_temp[(((yy_1 * 122) + (xx_outer_1 * 16)) + xx_inner_1)] = 0.000000e+00f;\n        }\n        if (((xx_outer_1 * 8) + (xx_inner_1 >> 1)) < 61) {\n          pad_temp[(((yy_1 * 122) + (xx_outer_1 * 16)) + xx_inner_1)] = (pad_temp[(((yy_1 * 122) + (xx_outer_1 * 16)) + xx_inner_1)] + (conv2d_nchw[(((yy_1 * 122) + (xx_outer_1 * 16)) + xx_inner_1)] * ph_24[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_2 = 0; i2_2 < 122; ++i2_2) {\n    for (int32_t i3_outer_2 = 0; i3_outer_2 < 8; ++i3_outer_2) {\n      for (int32_t i3_inner_2 = 0; i3_inner_2 < 16; ++i3_inner_2) {\n        if (((i3_outer_2 * 8) + (i3_inner_2 >> 1)) < 61) {\n          conv2d_nchw[(((i2_2 * 122) + (i3_outer_2 * 16)) + i3_inner_2)] = max(pad_temp[(((i2_2 * 122) + (i3_outer_2 * 16)) + i3_inner_2)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t yy_2 = 0; yy_2 < 122; ++yy_2) {\n    for (int32_t xx_outer_2 = 0; xx_outer_2 < 8; ++xx_outer_2) {\n      for (int32_t xx_inner_2 = 0; xx_inner_2 < 16; ++xx_inner_2) {\n        if (((xx_outer_2 * 8) + (xx_inner_2 >> 1)) < 61) {\n          pad_temp[(((yy_2 * 122) + (xx_outer_2 * 16)) + xx_inner_2)] = 0.000000e+00f;\n        }\n        if (((xx_outer_2 * 8) + (xx_inner_2 >> 1)) < 61) {\n          pad_temp[(((yy_2 * 122) + (xx_outer_2 * 16)) + xx_inner_2)] = (pad_temp[(((yy_2 * 122) + (xx_outer_2 * 16)) + xx_inner_2)] + (conv2d_nchw[(((yy_2 * 122) + (xx_outer_2 * 16)) + xx_inner_2)] * ph_23[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_3 = 0; i2_3 < 122; ++i2_3) {\n    for (int32_t i3_outer_3 = 0; i3_outer_3 < 8; ++i3_outer_3) {\n      for (int32_t i3_inner_3 = 0; i3_inner_3 < 16; ++i3_inner_3) {\n        if (((i3_outer_3 * 8) + (i3_inner_3 >> 1)) < 61) {\n          conv2d_nchw[(((i2_3 * 122) + (i3_outer_3 * 16)) + i3_inner_3)] = max(pad_temp[(((i2_3 * 122) + (i3_outer_3 * 16)) + i3_inner_3)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_4 = 0; i2_4 < 122; ++i2_4) {\n    for (int32_t i3_outer_4 = 0; i3_outer_4 < 8; ++i3_outer_4) {\n      for (int32_t i3_inner_4 = 0; i3_inner_4 < 16; ++i3_inner_4) {\n        if (((i3_outer_4 * 8) + (i3_inner_4 >> 1)) < 61) {\n          pad_temp[(((i2_4 * 122) + (i3_outer_4 * 16)) + i3_inner_4)] = conv2d_nchw[(((i2_4 * 122) + (i3_outer_4 * 16)) + i3_inner_4)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_3 = 0; yy_3 < 120; ++yy_3) {\n    for (int32_t xx_outer_3 = 0; xx_outer_3 < 8; ++xx_outer_3) {\n      for (int32_t xx_inner_3 = 0; xx_inner_3 < 16; ++xx_inner_3) {\n        if (((xx_outer_3 * 2) + (xx_inner_3 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_3 * 120) + (xx_outer_3 * 16)) + xx_inner_3)] = 0.000000e+00f;\n        }\n        if (((xx_outer_3 * 2) + (xx_inner_3 >> 3)) < 15) {\n          for (int32_t ry_1 = 0; ry_1 < 3; ++ry_1) {\n            for (int32_t rx_1 = 0; rx_1 < 3; ++rx_1) {\n              conv2d_nchw_1[(((yy_3 * 120) + (xx_outer_3 * 16)) + xx_inner_3)] = (conv2d_nchw_1[(((yy_3 * 120) + (xx_outer_3 * 16)) + xx_inner_3)] + (pad_temp[(((((yy_3 * 122) + (ry_1 * 122)) + (xx_outer_3 * 16)) + xx_inner_3) + rx_1)] * ph_22[((ry_1 * 3) + rx_1)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_5 = 0; i2_5 < 120; ++i2_5) {\n    for (int32_t i3_outer_5 = 0; i3_outer_5 < 8; ++i3_outer_5) {\n      for (int32_t i3_inner_5 = 0; i3_inner_5 < 16; ++i3_inner_5) {\n        if (((i3_outer_5 * 2) + (i3_inner_5 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_5 * 120) + (i3_outer_5 * 16)) + i3_inner_5)] = max(conv2d_nchw_1[(((i2_5 * 120) + (i3_outer_5 * 16)) + i3_inner_5)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 2; ++ax0_ax1_fused) {\n    for (int32_t ax2_2 = 0; ax2_2 < 122; ++ax2_2) {\n      for (int32_t ax3_outer_2 = 0; ax3_outer_2 < 8; ++ax3_outer_2) {\n        for (int32_t ax3_inner_2 = 0; ax3_inner_2 < 16; ++ax3_inner_2) {\n          if (((ax3_outer_2 * 8) + (ax3_inner_2 >> 1)) < 61) {\n            pad_temp[((((ax0_ax1_fused * 14884) + (ax2_2 * 122)) + (ax3_outer_2 * 16)) + ax3_inner_2)] = ((ax0_ax1_fused == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused + 1) >> 1) * 14400) + (ax2_2 * 120)) + (ax3_outer_2 * 16)) + ax3_inner_2) - 14400)] : conv2d_nchw[(((ax2_2 * 122) + (ax3_outer_2 * 16)) + ax3_inner_2)]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 2; ++i0_i1_fused_1) {\n    for (int32_t i2_6 = 0; i2_6 < 122; ++i2_6) {\n      for (int32_t i3_outer_6 = 0; i3_outer_6 < 8; ++i3_outer_6) {\n        for (int32_t i3_inner_6 = 0; i3_inner_6 < 16; ++i3_inner_6) {\n          if (((i3_outer_6 * 8) + (i3_inner_6 >> 1)) < 61) {\n            conv2d_nchw[((((i0_i1_fused_1 * 14884) + (i2_6 * 122)) + (i3_outer_6 * 16)) + i3_inner_6)] = pad_temp[((((i0_i1_fused_1 * 14884) + (i2_6 * 122)) + (i3_outer_6 * 16)) + i3_inner_6)];\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_4 = 0; yy_4 < 122; ++yy_4) {\n    for (int32_t xx_outer_4 = 0; xx_outer_4 < 8; ++xx_outer_4) {\n      for (int32_t xx_inner_4 = 0; xx_inner_4 < 16; ++xx_inner_4) {\n        if (((xx_outer_4 * 8) + (xx_inner_4 >> 1)) < 61) {\n          pad_temp[(((yy_4 * 122) + (xx_outer_4 * 16)) + xx_inner_4)] = 0.000000e+00f;\n        }\n        if (((xx_outer_4 * 8) + (xx_inner_4 >> 1)) < 61) {\n          for (int32_t rc_1 = 0; rc_1 < 2; ++rc_1) {\n            pad_temp[(((yy_4 * 122) + (xx_outer_4 * 16)) + xx_inner_4)] = (pad_temp[(((yy_4 * 122) + (xx_outer_4 * 16)) + xx_inner_4)] + (conv2d_nchw[((((rc_1 * 14884) + (yy_4 * 122)) + (xx_outer_4 * 16)) + xx_inner_4)] * ph_21[rc_1]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_7 = 0; i2_7 < 122; ++i2_7) {\n    for (int32_t i3_outer_7 = 0; i3_outer_7 < 8; ++i3_outer_7) {\n      for (int32_t i3_inner_7 = 0; i3_inner_7 < 16; ++i3_inner_7) {\n        if (((i3_outer_7 * 8) + (i3_inner_7 >> 1)) < 61) {\n          conv2d_nchw[(((i2_7 * 122) + (i3_outer_7 * 16)) + i3_inner_7)] = max(pad_temp[(((i2_7 * 122) + (i3_outer_7 * 16)) + i3_inner_7)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_8 = 0; i2_8 < 122; ++i2_8) {\n    for (int32_t i3_outer_8 = 0; i3_outer_8 < 8; ++i3_outer_8) {\n      for (int32_t i3_inner_8 = 0; i3_inner_8 < 16; ++i3_inner_8) {\n        if (((i3_outer_8 * 8) + (i3_inner_8 >> 1)) < 61) {\n          pad_temp[(((i2_8 * 122) + (i3_outer_8 * 16)) + i3_inner_8)] = conv2d_nchw[(((i2_8 * 122) + (i3_outer_8 * 16)) + i3_inner_8)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_5 = 0; yy_5 < 122; ++yy_5) {\n    for (int32_t xx_outer_5 = 0; xx_outer_5 < 8; ++xx_outer_5) {\n      for (int32_t xx_inner_5 = 0; xx_inner_5 < 16; ++xx_inner_5) {\n        if (((xx_outer_5 * 8) + (xx_inner_5 >> 1)) < 61) {\n          conv2d_nchw[(((yy_5 * 122) + (xx_outer_5 * 16)) + xx_inner_5)] = 0.000000e+00f;\n        }\n        if (((xx_outer_5 * 8) + (xx_inner_5 >> 1)) < 61) {\n          conv2d_nchw[(((yy_5 * 122) + (xx_outer_5 * 16)) + xx_inner_5)] = (conv2d_nchw[(((yy_5 * 122) + (xx_outer_5 * 16)) + xx_inner_5)] + (pad_temp[(((yy_5 * 122) + (xx_outer_5 * 16)) + xx_inner_5)] * ph_20[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_9 = 0; i2_9 < 122; ++i2_9) {\n    for (int32_t i3_outer_9 = 0; i3_outer_9 < 8; ++i3_outer_9) {\n      for (int32_t i3_inner_9 = 0; i3_inner_9 < 16; ++i3_inner_9) {\n        if (((i3_outer_9 * 8) + (i3_inner_9 >> 1)) < 61) {\n          pad_temp[(((i2_9 * 122) + (i3_outer_9 * 16)) + i3_inner_9)] = max(conv2d_nchw[(((i2_9 * 122) + (i3_outer_9 * 16)) + i3_inner_9)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_10 = 0; i2_10 < 122; ++i2_10) {\n    for (int32_t i3_outer_10 = 0; i3_outer_10 < 8; ++i3_outer_10) {\n      for (int32_t i3_inner_10 = 0; i3_inner_10 < 16; ++i3_inner_10) {\n        if (((i3_outer_10 * 8) + (i3_inner_10 >> 1)) < 61) {\n          conv2d_nchw[(((i2_10 * 122) + (i3_outer_10 * 16)) + i3_inner_10)] = pad_temp[(((i2_10 * 122) + (i3_outer_10 * 16)) + i3_inner_10)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_6 = 0; yy_6 < 120; ++yy_6) {\n    for (int32_t xx_outer_6 = 0; xx_outer_6 < 8; ++xx_outer_6) {\n      for (int32_t xx_inner_6 = 0; xx_inner_6 < 16; ++xx_inner_6) {\n        if (((xx_outer_6 * 2) + (xx_inner_6 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_6 * 120) + (xx_outer_6 * 16)) + xx_inner_6)] = 0.000000e+00f;\n        }\n        if (((xx_outer_6 * 2) + (xx_inner_6 >> 3)) < 15) {\n          for (int32_t ry_2 = 0; ry_2 < 3; ++ry_2) {\n            for (int32_t rx_2 = 0; rx_2 < 3; ++rx_2) {\n              conv2d_nchw_1[(((yy_6 * 120) + (xx_outer_6 * 16)) + xx_inner_6)] = (conv2d_nchw_1[(((yy_6 * 120) + (xx_outer_6 * 16)) + xx_inner_6)] + (conv2d_nchw[(((((yy_6 * 122) + (ry_2 * 122)) + (xx_outer_6 * 16)) + xx_inner_6) + rx_2)] * ph_19[((ry_2 * 3) + rx_2)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_11 = 0; i2_11 < 120; ++i2_11) {\n    for (int32_t i3_outer_11 = 0; i3_outer_11 < 8; ++i3_outer_11) {\n      for (int32_t i3_inner_11 = 0; i3_inner_11 < 16; ++i3_inner_11) {\n        if (((i3_outer_11 * 2) + (i3_inner_11 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_11 * 120) + (i3_outer_11 * 16)) + i3_inner_11)] = max(conv2d_nchw_1[(((i2_11 * 120) + (i3_outer_11 * 16)) + i3_inner_11)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 2; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_3 = 0; ax2_3 < 122; ++ax2_3) {\n      for (int32_t ax3_outer_3 = 0; ax3_outer_3 < 8; ++ax3_outer_3) {\n        for (int32_t ax3_inner_3 = 0; ax3_inner_3 < 16; ++ax3_inner_3) {\n          if (((ax3_outer_3 * 8) + (ax3_inner_3 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_1 * 14884) + (ax2_3 * 122)) + (ax3_outer_3 * 16)) + ax3_inner_3)] = ((ax0_ax1_fused_1 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_1 + 1) >> 1) * 14400) + (ax2_3 * 120)) + (ax3_outer_3 * 16)) + ax3_inner_3) - 14400)] : pad_temp[(((ax2_3 * 122) + (ax3_outer_3 * 16)) + ax3_inner_3)]);\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_7 = 0; yy_7 < 122; ++yy_7) {\n    for (int32_t xx_outer_7 = 0; xx_outer_7 < 8; ++xx_outer_7) {\n      for (int32_t xx_inner_7 = 0; xx_inner_7 < 16; ++xx_inner_7) {\n        if (((xx_outer_7 * 8) + (xx_inner_7 >> 1)) < 61) {\n          pad_temp[(((yy_7 * 122) + (xx_outer_7 * 16)) + xx_inner_7)] = 0.000000e+00f;\n        }\n        if (((xx_outer_7 * 8) + (xx_inner_7 >> 1)) < 61) {\n          for (int32_t rc_2 = 0; rc_2 < 2; ++rc_2) {\n            pad_temp[(((yy_7 * 122) + (xx_outer_7 * 16)) + xx_inner_7)] = (pad_temp[(((yy_7 * 122) + (xx_outer_7 * 16)) + xx_inner_7)] + (conv2d_nchw[((((rc_2 * 14884) + (yy_7 * 122)) + (xx_outer_7 * 16)) + xx_inner_7)] * ph_18[rc_2]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_12 = 0; i2_12 < 122; ++i2_12) {\n    for (int32_t i3_outer_12 = 0; i3_outer_12 < 8; ++i3_outer_12) {\n      for (int32_t i3_inner_12 = 0; i3_inner_12 < 16; ++i3_inner_12) {\n        if (((i3_outer_12 * 8) + (i3_inner_12 >> 1)) < 61) {\n          conv2d_nchw[(((i2_12 * 122) + (i3_outer_12 * 16)) + i3_inner_12)] = max(pad_temp[(((i2_12 * 122) + (i3_outer_12 * 16)) + i3_inner_12)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_13 = 0; i2_13 < 122; ++i2_13) {\n    for (int32_t i3_outer_13 = 0; i3_outer_13 < 8; ++i3_outer_13) {\n      for (int32_t i3_inner_13 = 0; i3_inner_13 < 16; ++i3_inner_13) {\n        if (((i3_outer_13 * 8) + (i3_inner_13 >> 1)) < 61) {\n          pad_temp[(((i2_13 * 122) + (i3_outer_13 * 16)) + i3_inner_13)] = conv2d_nchw[(((i2_13 * 122) + (i3_outer_13 * 16)) + i3_inner_13)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_8 = 0; yy_8 < 122; ++yy_8) {\n    for (int32_t xx_outer_8 = 0; xx_outer_8 < 8; ++xx_outer_8) {\n      for (int32_t xx_inner_8 = 0; xx_inner_8 < 16; ++xx_inner_8) {\n        if (((xx_outer_8 * 8) + (xx_inner_8 >> 1)) < 61) {\n          conv2d_nchw[(((yy_8 * 122) + (xx_outer_8 * 16)) + xx_inner_8)] = 0.000000e+00f;\n        }\n        if (((xx_outer_8 * 8) + (xx_inner_8 >> 1)) < 61) {\n          conv2d_nchw[(((yy_8 * 122) + (xx_outer_8 * 16)) + xx_inner_8)] = (conv2d_nchw[(((yy_8 * 122) + (xx_outer_8 * 16)) + xx_inner_8)] + (pad_temp[(((yy_8 * 122) + (xx_outer_8 * 16)) + xx_inner_8)] * ph_17[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_14 = 0; i2_14 < 122; ++i2_14) {\n    for (int32_t i3_outer_14 = 0; i3_outer_14 < 8; ++i3_outer_14) {\n      for (int32_t i3_inner_14 = 0; i3_inner_14 < 16; ++i3_inner_14) {\n        if (((i3_outer_14 * 8) + (i3_inner_14 >> 1)) < 61) {\n          pad_temp[(((i2_14 * 122) + (i3_outer_14 * 16)) + i3_inner_14)] = max(conv2d_nchw[(((i2_14 * 122) + (i3_outer_14 * 16)) + i3_inner_14)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_15 = 0; i2_15 < 122; ++i2_15) {\n    for (int32_t i3_outer_15 = 0; i3_outer_15 < 8; ++i3_outer_15) {\n      for (int32_t i3_inner_15 = 0; i3_inner_15 < 16; ++i3_inner_15) {\n        if (((i3_outer_15 * 8) + (i3_inner_15 >> 1)) < 61) {\n          conv2d_nchw[(((i2_15 * 122) + (i3_outer_15 * 16)) + i3_inner_15)] = pad_temp[(((i2_15 * 122) + (i3_outer_15 * 16)) + i3_inner_15)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_9 = 0; yy_9 < 120; ++yy_9) {\n    for (int32_t xx_outer_9 = 0; xx_outer_9 < 8; ++xx_outer_9) {\n      for (int32_t xx_inner_9 = 0; xx_inner_9 < 16; ++xx_inner_9) {\n        if (((xx_outer_9 * 2) + (xx_inner_9 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_9 * 120) + (xx_outer_9 * 16)) + xx_inner_9)] = 0.000000e+00f;\n        }\n        if (((xx_outer_9 * 2) + (xx_inner_9 >> 3)) < 15) {\n          for (int32_t ry_3 = 0; ry_3 < 3; ++ry_3) {\n            for (int32_t rx_3 = 0; rx_3 < 3; ++rx_3) {\n              conv2d_nchw_1[(((yy_9 * 120) + (xx_outer_9 * 16)) + xx_inner_9)] = (conv2d_nchw_1[(((yy_9 * 120) + (xx_outer_9 * 16)) + xx_inner_9)] + (conv2d_nchw[(((((yy_9 * 122) + (ry_3 * 122)) + (xx_outer_9 * 16)) + xx_inner_9) + rx_3)] * ph_16[((ry_3 * 3) + rx_3)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_16 = 0; i2_16 < 120; ++i2_16) {\n    for (int32_t i3_outer_16 = 0; i3_outer_16 < 8; ++i3_outer_16) {\n      for (int32_t i3_inner_16 = 0; i3_inner_16 < 16; ++i3_inner_16) {\n        if (((i3_outer_16 * 2) + (i3_inner_16 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_16 * 120) + (i3_outer_16 * 16)) + i3_inner_16)] = max(conv2d_nchw_1[(((i2_16 * 120) + (i3_outer_16 * 16)) + i3_inner_16)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n    for (int32_t ax2_4 = 0; ax2_4 < 122; ++ax2_4) {\n      for (int32_t ax3_outer_4 = 0; ax3_outer_4 < 8; ++ax3_outer_4) {\n        for (int32_t ax3_inner_4 = 0; ax3_inner_4 < 16; ++ax3_inner_4) {\n          if (((ax3_outer_4 * 8) + (ax3_inner_4 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_2 * 14884) + (ax2_4 * 122)) + (ax3_outer_4 * 16)) + ax3_inner_4)] = ((ax0_ax1_fused_2 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_2 + 1) >> 1) * 14400) + (ax2_4 * 120)) + (ax3_outer_4 * 16)) + ax3_inner_4) - 14400)] : pad_temp[(((ax2_4 * 122) + (ax3_outer_4 * 16)) + ax3_inner_4)]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_3 = 0; ax0_ax1_fused_3 < 2; ++ax0_ax1_fused_3) {\n    for (int32_t ax2_5 = 0; ax2_5 < 124; ++ax2_5) {\n      for (int32_t ax3_outer_5 = 0; ax3_outer_5 < 8; ++ax3_outer_5) {\n        for (int32_t ax3_inner_5 = 0; ax3_inner_5 < 16; ++ax3_inner_5) {\n          if (((ax3_outer_5 * 4) + (ax3_inner_5 >> 2)) < 31) {\n            pad_temp[((((ax0_ax1_fused_3 * 15376) + (ax2_5 * 124)) + (ax3_outer_5 * 16)) + ax3_inner_5)] = (((((1 <= ax2_5) && (ax2_5 < 123)) && (1 <= ((ax3_outer_5 * 16) + ax3_inner_5))) && (((ax3_outer_5 * 16) + ax3_inner_5) < 123)) ? conv2d_nchw[(((((ax0_ax1_fused_3 * 14884) + (ax2_5 * 122)) + (ax3_outer_5 * 16)) + ax3_inner_5) - 123)] : -3.402823e+38f);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_4 = 0; ax0_ax1_fused_4 < 2; ++ax0_ax1_fused_4) {\n    for (int32_t ax2_6 = 0; ax2_6 < 122; ++ax2_6) {\n      for (int32_t ax3_outer_6 = 0; ax3_outer_6 < 8; ++ax3_outer_6) {\n        for (int32_t ax3_inner_6 = 0; ax3_inner_6 < 16; ++ax3_inner_6) {\n          if (((ax3_outer_6 * 8) + (ax3_inner_6 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_4 * 14884) + (ax2_6 * 122)) + (ax3_outer_6 * 16)) + ax3_inner_6)] = -3.402823e+38f;\n          }\n          if (((ax3_outer_6 * 8) + (ax3_inner_6 >> 1)) < 61) {\n            for (int32_t rv0_1 = 0; rv0_1 < 3; ++rv0_1) {\n              for (int32_t rv1_1 = 0; rv1_1 < 3; ++rv1_1) {\n                conv2d_nchw[((((ax0_ax1_fused_4 * 14884) + (ax2_6 * 122)) + (ax3_outer_6 * 16)) + ax3_inner_6)] = max(conv2d_nchw[((((ax0_ax1_fused_4 * 14884) + (ax2_6 * 122)) + (ax3_outer_6 * 16)) + ax3_inner_6)], pad_temp[((((((ax0_ax1_fused_4 * 15376) + (ax2_6 * 124)) + (rv0_1 * 124)) + (ax3_outer_6 * 16)) + ax3_inner_6) + rv1_1)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_10 = 0; yy_10 < 122; ++yy_10) {\n    for (int32_t xx_outer_10 = 0; xx_outer_10 < 8; ++xx_outer_10) {\n      for (int32_t xx_inner_10 = 0; xx_inner_10 < 16; ++xx_inner_10) {\n        if (((xx_outer_10 * 8) + (xx_inner_10 >> 1)) < 61) {\n          pad_temp[(((yy_10 * 122) + (xx_outer_10 * 16)) + xx_inner_10)] = 0.000000e+00f;\n        }\n        if (((xx_outer_10 * 8) + (xx_inner_10 >> 1)) < 61) {\n          for (int32_t rc_3 = 0; rc_3 < 2; ++rc_3) {\n            pad_temp[(((yy_10 * 122) + (xx_outer_10 * 16)) + xx_inner_10)] = (pad_temp[(((yy_10 * 122) + (xx_outer_10 * 16)) + xx_inner_10)] + (conv2d_nchw[((((rc_3 * 14884) + (yy_10 * 122)) + (xx_outer_10 * 16)) + xx_inner_10)] * ph_15[rc_3]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_17 = 0; i2_17 < 122; ++i2_17) {\n    for (int32_t i3_outer_17 = 0; i3_outer_17 < 8; ++i3_outer_17) {\n      for (int32_t i3_inner_17 = 0; i3_inner_17 < 16; ++i3_inner_17) {\n        if (((i3_outer_17 * 8) + (i3_inner_17 >> 1)) < 61) {\n          conv2d_nchw[(((i2_17 * 122) + (i3_outer_17 * 16)) + i3_inner_17)] = max(pad_temp[(((i2_17 * 122) + (i3_outer_17 * 16)) + i3_inner_17)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_18 = 0; i2_18 < 122; ++i2_18) {\n    for (int32_t i3_outer_18 = 0; i3_outer_18 < 8; ++i3_outer_18) {\n      for (int32_t i3_inner_18 = 0; i3_inner_18 < 16; ++i3_inner_18) {\n        if (((i3_outer_18 * 8) + (i3_inner_18 >> 1)) < 61) {\n          pad_temp[(((i2_18 * 122) + (i3_outer_18 * 16)) + i3_inner_18)] = conv2d_nchw[(((i2_18 * 122) + (i3_outer_18 * 16)) + i3_inner_18)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_11 = 0; yy_11 < 122; ++yy_11) {\n    for (int32_t xx_outer_11 = 0; xx_outer_11 < 8; ++xx_outer_11) {\n      for (int32_t xx_inner_11 = 0; xx_inner_11 < 16; ++xx_inner_11) {\n        if (((xx_outer_11 * 8) + (xx_inner_11 >> 1)) < 61) {\n          conv2d_nchw[(((yy_11 * 122) + (xx_outer_11 * 16)) + xx_inner_11)] = 0.000000e+00f;\n        }\n        if (((xx_outer_11 * 8) + (xx_inner_11 >> 1)) < 61) {\n          conv2d_nchw[(((yy_11 * 122) + (xx_outer_11 * 16)) + xx_inner_11)] = (conv2d_nchw[(((yy_11 * 122) + (xx_outer_11 * 16)) + xx_inner_11)] + (pad_temp[(((yy_11 * 122) + (xx_outer_11 * 16)) + xx_inner_11)] * ph_14[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_19 = 0; i2_19 < 122; ++i2_19) {\n    for (int32_t i3_outer_19 = 0; i3_outer_19 < 8; ++i3_outer_19) {\n      for (int32_t i3_inner_19 = 0; i3_inner_19 < 16; ++i3_inner_19) {\n        if (((i3_outer_19 * 8) + (i3_inner_19 >> 1)) < 61) {\n          pad_temp[(((i2_19 * 122) + (i3_outer_19 * 16)) + i3_inner_19)] = max(conv2d_nchw[(((i2_19 * 122) + (i3_outer_19 * 16)) + i3_inner_19)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_20 = 0; i2_20 < 122; ++i2_20) {\n    for (int32_t i3_outer_20 = 0; i3_outer_20 < 8; ++i3_outer_20) {\n      for (int32_t i3_inner_20 = 0; i3_inner_20 < 16; ++i3_inner_20) {\n        if (((i3_outer_20 * 8) + (i3_inner_20 >> 1)) < 61) {\n          conv2d_nchw[(((i2_20 * 122) + (i3_outer_20 * 16)) + i3_inner_20)] = pad_temp[(((i2_20 * 122) + (i3_outer_20 * 16)) + i3_inner_20)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_12 = 0; yy_12 < 120; ++yy_12) {\n    for (int32_t xx_outer_12 = 0; xx_outer_12 < 8; ++xx_outer_12) {\n      for (int32_t xx_inner_12 = 0; xx_inner_12 < 16; ++xx_inner_12) {\n        if (((xx_outer_12 * 2) + (xx_inner_12 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_12 * 120) + (xx_outer_12 * 16)) + xx_inner_12)] = 0.000000e+00f;\n        }\n        if (((xx_outer_12 * 2) + (xx_inner_12 >> 3)) < 15) {\n          for (int32_t ry_4 = 0; ry_4 < 3; ++ry_4) {\n            for (int32_t rx_4 = 0; rx_4 < 3; ++rx_4) {\n              conv2d_nchw_1[(((yy_12 * 120) + (xx_outer_12 * 16)) + xx_inner_12)] = (conv2d_nchw_1[(((yy_12 * 120) + (xx_outer_12 * 16)) + xx_inner_12)] + (conv2d_nchw[(((((yy_12 * 122) + (ry_4 * 122)) + (xx_outer_12 * 16)) + xx_inner_12) + rx_4)] * ph[((ry_4 * 3) + rx_4)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_21 = 0; i2_21 < 120; ++i2_21) {\n    for (int32_t i3_outer_21 = 0; i3_outer_21 < 8; ++i3_outer_21) {\n      for (int32_t i3_inner_21 = 0; i3_inner_21 < 16; ++i3_inner_21) {\n        if (((i3_outer_21 * 2) + (i3_inner_21 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_21 * 120) + (i3_outer_21 * 16)) + i3_inner_21)] = max(conv2d_nchw_1[(((i2_21 * 120) + (i3_outer_21 * 16)) + i3_inner_21)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_5 = 0; ax0_ax1_fused_5 < 2; ++ax0_ax1_fused_5) {\n    for (int32_t ax2_7 = 0; ax2_7 < 122; ++ax2_7) {\n      for (int32_t ax3_outer_7 = 0; ax3_outer_7 < 8; ++ax3_outer_7) {\n        for (int32_t ax3_inner_7 = 0; ax3_inner_7 < 16; ++ax3_inner_7) {\n          if (((ax3_outer_7 * 8) + (ax3_inner_7 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_5 * 14884) + (ax2_7 * 122)) + (ax3_outer_7 * 16)) + ax3_inner_7)] = ((ax0_ax1_fused_5 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_5 + 1) >> 1) * 14400) + (ax2_7 * 120)) + (ax3_outer_7 * 16)) + ax3_inner_7) - 14400)] : pad_temp[(((ax2_7 * 122) + (ax3_outer_7 * 16)) + ax3_inner_7)]);\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_13 = 0; yy_13 < 122; ++yy_13) {\n    for (int32_t xx_outer_13 = 0; xx_outer_13 < 8; ++xx_outer_13) {\n      for (int32_t xx_inner_13 = 0; xx_inner_13 < 16; ++xx_inner_13) {\n        if (((xx_outer_13 * 8) + (xx_inner_13 >> 1)) < 61) {\n          pad_temp[(((yy_13 * 122) + (xx_outer_13 * 16)) + xx_inner_13)] = 0.000000e+00f;\n        }\n        if (((xx_outer_13 * 8) + (xx_inner_13 >> 1)) < 61) {\n          for (int32_t rc_4 = 0; rc_4 < 2; ++rc_4) {\n            pad_temp[(((yy_13 * 122) + (xx_outer_13 * 16)) + xx_inner_13)] = (pad_temp[(((yy_13 * 122) + (xx_outer_13 * 16)) + xx_inner_13)] + (conv2d_nchw[((((rc_4 * 14884) + (yy_13 * 122)) + (xx_outer_13 * 16)) + xx_inner_13)] * ph_12[rc_4]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_22 = 0; i2_22 < 122; ++i2_22) {\n    for (int32_t i3_outer_22 = 0; i3_outer_22 < 8; ++i3_outer_22) {\n      for (int32_t i3_inner_22 = 0; i3_inner_22 < 16; ++i3_inner_22) {\n        if (((i3_outer_22 * 8) + (i3_inner_22 >> 1)) < 61) {\n          conv2d_nchw[(((i2_22 * 122) + (i3_outer_22 * 16)) + i3_inner_22)] = max(pad_temp[(((i2_22 * 122) + (i3_outer_22 * 16)) + i3_inner_22)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_23 = 0; i2_23 < 122; ++i2_23) {\n    for (int32_t i3_outer_23 = 0; i3_outer_23 < 8; ++i3_outer_23) {\n      for (int32_t i3_inner_23 = 0; i3_inner_23 < 16; ++i3_inner_23) {\n        if (((i3_outer_23 * 8) + (i3_inner_23 >> 1)) < 61) {\n          pad_temp[(((i2_23 * 122) + (i3_outer_23 * 16)) + i3_inner_23)] = conv2d_nchw[(((i2_23 * 122) + (i3_outer_23 * 16)) + i3_inner_23)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_14 = 0; yy_14 < 122; ++yy_14) {\n    for (int32_t xx_outer_14 = 0; xx_outer_14 < 8; ++xx_outer_14) {\n      for (int32_t xx_inner_14 = 0; xx_inner_14 < 16; ++xx_inner_14) {\n        if (((xx_outer_14 * 8) + (xx_inner_14 >> 1)) < 61) {\n          conv2d_nchw[(((yy_14 * 122) + (xx_outer_14 * 16)) + xx_inner_14)] = 0.000000e+00f;\n        }\n        if (((xx_outer_14 * 8) + (xx_inner_14 >> 1)) < 61) {\n          conv2d_nchw[(((yy_14 * 122) + (xx_outer_14 * 16)) + xx_inner_14)] = (conv2d_nchw[(((yy_14 * 122) + (xx_outer_14 * 16)) + xx_inner_14)] + (pad_temp[(((yy_14 * 122) + (xx_outer_14 * 16)) + xx_inner_14)] * ph_11[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_24 = 0; i2_24 < 122; ++i2_24) {\n    for (int32_t i3_outer_24 = 0; i3_outer_24 < 8; ++i3_outer_24) {\n      for (int32_t i3_inner_24 = 0; i3_inner_24 < 16; ++i3_inner_24) {\n        if (((i3_outer_24 * 8) + (i3_inner_24 >> 1)) < 61) {\n          pad_temp[(((i2_24 * 122) + (i3_outer_24 * 16)) + i3_inner_24)] = max(conv2d_nchw[(((i2_24 * 122) + (i3_outer_24 * 16)) + i3_inner_24)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_25 = 0; i2_25 < 122; ++i2_25) {\n    for (int32_t i3_outer_25 = 0; i3_outer_25 < 8; ++i3_outer_25) {\n      for (int32_t i3_inner_25 = 0; i3_inner_25 < 16; ++i3_inner_25) {\n        if (((i3_outer_25 * 8) + (i3_inner_25 >> 1)) < 61) {\n          conv2d_nchw[(((i2_25 * 122) + (i3_outer_25 * 16)) + i3_inner_25)] = pad_temp[(((i2_25 * 122) + (i3_outer_25 * 16)) + i3_inner_25)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_15 = 0; yy_15 < 120; ++yy_15) {\n    for (int32_t xx_outer_15 = 0; xx_outer_15 < 8; ++xx_outer_15) {\n      for (int32_t xx_inner_15 = 0; xx_inner_15 < 16; ++xx_inner_15) {\n        if (((xx_outer_15 * 2) + (xx_inner_15 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_15 * 120) + (xx_outer_15 * 16)) + xx_inner_15)] = 0.000000e+00f;\n        }\n        if (((xx_outer_15 * 2) + (xx_inner_15 >> 3)) < 15) {\n          for (int32_t ry_5 = 0; ry_5 < 3; ++ry_5) {\n            for (int32_t rx_5 = 0; rx_5 < 3; ++rx_5) {\n              conv2d_nchw_1[(((yy_15 * 120) + (xx_outer_15 * 16)) + xx_inner_15)] = (conv2d_nchw_1[(((yy_15 * 120) + (xx_outer_15 * 16)) + xx_inner_15)] + (conv2d_nchw[(((((yy_15 * 122) + (ry_5 * 122)) + (xx_outer_15 * 16)) + xx_inner_15) + rx_5)] * ph_10[((ry_5 * 3) + rx_5)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_26 = 0; i2_26 < 120; ++i2_26) {\n    for (int32_t i3_outer_26 = 0; i3_outer_26 < 8; ++i3_outer_26) {\n      for (int32_t i3_inner_26 = 0; i3_inner_26 < 16; ++i3_inner_26) {\n        if (((i3_outer_26 * 2) + (i3_inner_26 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_26 * 120) + (i3_outer_26 * 16)) + i3_inner_26)] = max(conv2d_nchw_1[(((i2_26 * 120) + (i3_outer_26 * 16)) + i3_inner_26)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_6 = 0; ax0_ax1_fused_6 < 2; ++ax0_ax1_fused_6) {\n    for (int32_t ax2_8 = 0; ax2_8 < 122; ++ax2_8) {\n      for (int32_t ax3_outer_8 = 0; ax3_outer_8 < 8; ++ax3_outer_8) {\n        for (int32_t ax3_inner_8 = 0; ax3_inner_8 < 16; ++ax3_inner_8) {\n          if (((ax3_outer_8 * 8) + (ax3_inner_8 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_6 * 14884) + (ax2_8 * 122)) + (ax3_outer_8 * 16)) + ax3_inner_8)] = ((ax0_ax1_fused_6 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_6 + 1) >> 1) * 14400) + (ax2_8 * 120)) + (ax3_outer_8 * 16)) + ax3_inner_8) - 14400)] : pad_temp[(((ax2_8 * 122) + (ax3_outer_8 * 16)) + ax3_inner_8)]);\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_16 = 0; yy_16 < 122; ++yy_16) {\n    for (int32_t xx_outer_16 = 0; xx_outer_16 < 8; ++xx_outer_16) {\n      for (int32_t xx_inner_16 = 0; xx_inner_16 < 16; ++xx_inner_16) {\n        if (((xx_outer_16 * 8) + (xx_inner_16 >> 1)) < 61) {\n          pad_temp[(((yy_16 * 122) + (xx_outer_16 * 16)) + xx_inner_16)] = 0.000000e+00f;\n        }\n        if (((xx_outer_16 * 8) + (xx_inner_16 >> 1)) < 61) {\n          for (int32_t rc_5 = 0; rc_5 < 2; ++rc_5) {\n            pad_temp[(((yy_16 * 122) + (xx_outer_16 * 16)) + xx_inner_16)] = (pad_temp[(((yy_16 * 122) + (xx_outer_16 * 16)) + xx_inner_16)] + (conv2d_nchw[((((rc_5 * 14884) + (yy_16 * 122)) + (xx_outer_16 * 16)) + xx_inner_16)] * ph_9[rc_5]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_27 = 0; i2_27 < 122; ++i2_27) {\n    for (int32_t i3_outer_27 = 0; i3_outer_27 < 8; ++i3_outer_27) {\n      for (int32_t i3_inner_27 = 0; i3_inner_27 < 16; ++i3_inner_27) {\n        if (((i3_outer_27 * 8) + (i3_inner_27 >> 1)) < 61) {\n          conv2d_nchw[(((i2_27 * 122) + (i3_outer_27 * 16)) + i3_inner_27)] = max(pad_temp[(((i2_27 * 122) + (i3_outer_27 * 16)) + i3_inner_27)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_28 = 0; i2_28 < 122; ++i2_28) {\n    for (int32_t i3_outer_28 = 0; i3_outer_28 < 8; ++i3_outer_28) {\n      for (int32_t i3_inner_28 = 0; i3_inner_28 < 16; ++i3_inner_28) {\n        if (((i3_outer_28 * 8) + (i3_inner_28 >> 1)) < 61) {\n          pad_temp[(((i2_28 * 122) + (i3_outer_28 * 16)) + i3_inner_28)] = conv2d_nchw[(((i2_28 * 122) + (i3_outer_28 * 16)) + i3_inner_28)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_17 = 0; yy_17 < 122; ++yy_17) {\n    for (int32_t xx_outer_17 = 0; xx_outer_17 < 8; ++xx_outer_17) {\n      for (int32_t xx_inner_17 = 0; xx_inner_17 < 16; ++xx_inner_17) {\n        if (((xx_outer_17 * 8) + (xx_inner_17 >> 1)) < 61) {\n          conv2d_nchw[(((yy_17 * 122) + (xx_outer_17 * 16)) + xx_inner_17)] = 0.000000e+00f;\n        }\n        if (((xx_outer_17 * 8) + (xx_inner_17 >> 1)) < 61) {\n          conv2d_nchw[(((yy_17 * 122) + (xx_outer_17 * 16)) + xx_inner_17)] = (conv2d_nchw[(((yy_17 * 122) + (xx_outer_17 * 16)) + xx_inner_17)] + (pad_temp[(((yy_17 * 122) + (xx_outer_17 * 16)) + xx_inner_17)] * ph_8[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_29 = 0; i2_29 < 122; ++i2_29) {\n    for (int32_t i3_outer_29 = 0; i3_outer_29 < 8; ++i3_outer_29) {\n      for (int32_t i3_inner_29 = 0; i3_inner_29 < 16; ++i3_inner_29) {\n        if (((i3_outer_29 * 8) + (i3_inner_29 >> 1)) < 61) {\n          pad_temp[(((i2_29 * 122) + (i3_outer_29 * 16)) + i3_inner_29)] = max(conv2d_nchw[(((i2_29 * 122) + (i3_outer_29 * 16)) + i3_inner_29)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_30 = 0; i2_30 < 122; ++i2_30) {\n    for (int32_t i3_outer_30 = 0; i3_outer_30 < 8; ++i3_outer_30) {\n      for (int32_t i3_inner_30 = 0; i3_inner_30 < 16; ++i3_inner_30) {\n        if (((i3_outer_30 * 8) + (i3_inner_30 >> 1)) < 61) {\n          conv2d_nchw[(((i2_30 * 122) + (i3_outer_30 * 16)) + i3_inner_30)] = pad_temp[(((i2_30 * 122) + (i3_outer_30 * 16)) + i3_inner_30)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_18 = 0; yy_18 < 120; ++yy_18) {\n    for (int32_t xx_outer_18 = 0; xx_outer_18 < 8; ++xx_outer_18) {\n      for (int32_t xx_inner_18 = 0; xx_inner_18 < 16; ++xx_inner_18) {\n        if (((xx_outer_18 * 2) + (xx_inner_18 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_18 * 120) + (xx_outer_18 * 16)) + xx_inner_18)] = 0.000000e+00f;\n        }\n        if (((xx_outer_18 * 2) + (xx_inner_18 >> 3)) < 15) {\n          for (int32_t ry_6 = 0; ry_6 < 3; ++ry_6) {\n            for (int32_t rx_6 = 0; rx_6 < 3; ++rx_6) {\n              conv2d_nchw_1[(((yy_18 * 120) + (xx_outer_18 * 16)) + xx_inner_18)] = (conv2d_nchw_1[(((yy_18 * 120) + (xx_outer_18 * 16)) + xx_inner_18)] + (conv2d_nchw[(((((yy_18 * 122) + (ry_6 * 122)) + (xx_outer_18 * 16)) + xx_inner_18) + rx_6)] * ph_7[((ry_6 * 3) + rx_6)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_31 = 0; i2_31 < 120; ++i2_31) {\n    for (int32_t i3_outer_31 = 0; i3_outer_31 < 8; ++i3_outer_31) {\n      for (int32_t i3_inner_31 = 0; i3_inner_31 < 16; ++i3_inner_31) {\n        if (((i3_outer_31 * 2) + (i3_inner_31 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_31 * 120) + (i3_outer_31 * 16)) + i3_inner_31)] = max(conv2d_nchw_1[(((i2_31 * 120) + (i3_outer_31 * 16)) + i3_inner_31)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_7 = 0; ax0_ax1_fused_7 < 2; ++ax0_ax1_fused_7) {\n    for (int32_t ax2_9 = 0; ax2_9 < 122; ++ax2_9) {\n      for (int32_t ax3_outer_9 = 0; ax3_outer_9 < 8; ++ax3_outer_9) {\n        for (int32_t ax3_inner_9 = 0; ax3_inner_9 < 16; ++ax3_inner_9) {\n          if (((ax3_outer_9 * 8) + (ax3_inner_9 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_7 * 14884) + (ax2_9 * 122)) + (ax3_outer_9 * 16)) + ax3_inner_9)] = ((ax0_ax1_fused_7 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_7 + 1) >> 1) * 14400) + (ax2_9 * 120)) + (ax3_outer_9 * 16)) + ax3_inner_9) - 14400)] : pad_temp[(((ax2_9 * 122) + (ax3_outer_9 * 16)) + ax3_inner_9)]);\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_19 = 0; yy_19 < 122; ++yy_19) {\n    for (int32_t xx_outer_19 = 0; xx_outer_19 < 8; ++xx_outer_19) {\n      for (int32_t xx_inner_19 = 0; xx_inner_19 < 16; ++xx_inner_19) {\n        if (((xx_outer_19 * 8) + (xx_inner_19 >> 1)) < 61) {\n          pad_temp[(((yy_19 * 122) + (xx_outer_19 * 16)) + xx_inner_19)] = 0.000000e+00f;\n        }\n        if (((xx_outer_19 * 8) + (xx_inner_19 >> 1)) < 61) {\n          for (int32_t rc_6 = 0; rc_6 < 2; ++rc_6) {\n            pad_temp[(((yy_19 * 122) + (xx_outer_19 * 16)) + xx_inner_19)] = (pad_temp[(((yy_19 * 122) + (xx_outer_19 * 16)) + xx_inner_19)] + (conv2d_nchw[((((rc_6 * 14884) + (yy_19 * 122)) + (xx_outer_19 * 16)) + xx_inner_19)] * ph_6[rc_6]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_32 = 0; i2_32 < 122; ++i2_32) {\n    for (int32_t i3_outer_32 = 0; i3_outer_32 < 8; ++i3_outer_32) {\n      for (int32_t i3_inner_32 = 0; i3_inner_32 < 16; ++i3_inner_32) {\n        if (((i3_outer_32 * 8) + (i3_inner_32 >> 1)) < 61) {\n          conv2d_nchw[(((i2_32 * 122) + (i3_outer_32 * 16)) + i3_inner_32)] = max(pad_temp[(((i2_32 * 122) + (i3_outer_32 * 16)) + i3_inner_32)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_33 = 0; i2_33 < 122; ++i2_33) {\n    for (int32_t i3_outer_33 = 0; i3_outer_33 < 8; ++i3_outer_33) {\n      for (int32_t i3_inner_33 = 0; i3_inner_33 < 16; ++i3_inner_33) {\n        if (((i3_outer_33 * 8) + (i3_inner_33 >> 1)) < 61) {\n          pad_temp[(((i2_33 * 122) + (i3_outer_33 * 16)) + i3_inner_33)] = conv2d_nchw[(((i2_33 * 122) + (i3_outer_33 * 16)) + i3_inner_33)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_20 = 0; yy_20 < 122; ++yy_20) {\n    for (int32_t xx_outer_20 = 0; xx_outer_20 < 8; ++xx_outer_20) {\n      for (int32_t xx_inner_20 = 0; xx_inner_20 < 16; ++xx_inner_20) {\n        if (((xx_outer_20 * 8) + (xx_inner_20 >> 1)) < 61) {\n          conv2d_nchw[(((yy_20 * 122) + (xx_outer_20 * 16)) + xx_inner_20)] = 0.000000e+00f;\n        }\n        if (((xx_outer_20 * 8) + (xx_inner_20 >> 1)) < 61) {\n          conv2d_nchw[(((yy_20 * 122) + (xx_outer_20 * 16)) + xx_inner_20)] = (conv2d_nchw[(((yy_20 * 122) + (xx_outer_20 * 16)) + xx_inner_20)] + (pad_temp[(((yy_20 * 122) + (xx_outer_20 * 16)) + xx_inner_20)] * ph_5[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_34 = 0; i2_34 < 122; ++i2_34) {\n    for (int32_t i3_outer_34 = 0; i3_outer_34 < 8; ++i3_outer_34) {\n      for (int32_t i3_inner_34 = 0; i3_inner_34 < 16; ++i3_inner_34) {\n        if (((i3_outer_34 * 8) + (i3_inner_34 >> 1)) < 61) {\n          pad_temp[(((i2_34 * 122) + (i3_outer_34 * 16)) + i3_inner_34)] = max(conv2d_nchw[(((i2_34 * 122) + (i3_outer_34 * 16)) + i3_inner_34)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_35 = 0; i2_35 < 122; ++i2_35) {\n    for (int32_t i3_outer_35 = 0; i3_outer_35 < 8; ++i3_outer_35) {\n      for (int32_t i3_inner_35 = 0; i3_inner_35 < 16; ++i3_inner_35) {\n        if (((i3_outer_35 * 8) + (i3_inner_35 >> 1)) < 61) {\n          conv2d_nchw[(((i2_35 * 122) + (i3_outer_35 * 16)) + i3_inner_35)] = pad_temp[(((i2_35 * 122) + (i3_outer_35 * 16)) + i3_inner_35)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_21 = 0; yy_21 < 120; ++yy_21) {\n    for (int32_t xx_outer_21 = 0; xx_outer_21 < 8; ++xx_outer_21) {\n      for (int32_t xx_inner_21 = 0; xx_inner_21 < 16; ++xx_inner_21) {\n        if (((xx_outer_21 * 2) + (xx_inner_21 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_21 * 120) + (xx_outer_21 * 16)) + xx_inner_21)] = 0.000000e+00f;\n        }\n        if (((xx_outer_21 * 2) + (xx_inner_21 >> 3)) < 15) {\n          for (int32_t ry_7 = 0; ry_7 < 3; ++ry_7) {\n            for (int32_t rx_7 = 0; rx_7 < 3; ++rx_7) {\n              conv2d_nchw_1[(((yy_21 * 120) + (xx_outer_21 * 16)) + xx_inner_21)] = (conv2d_nchw_1[(((yy_21 * 120) + (xx_outer_21 * 16)) + xx_inner_21)] + (conv2d_nchw[(((((yy_21 * 122) + (ry_7 * 122)) + (xx_outer_21 * 16)) + xx_inner_21) + rx_7)] * ph_4[((ry_7 * 3) + rx_7)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_36 = 0; i2_36 < 120; ++i2_36) {\n    for (int32_t i3_outer_36 = 0; i3_outer_36 < 8; ++i3_outer_36) {\n      for (int32_t i3_inner_36 = 0; i3_inner_36 < 16; ++i3_inner_36) {\n        if (((i3_outer_36 * 2) + (i3_inner_36 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_36 * 120) + (i3_outer_36 * 16)) + i3_inner_36)] = max(conv2d_nchw_1[(((i2_36 * 120) + (i3_outer_36 * 16)) + i3_inner_36)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_8 = 0; ax0_ax1_fused_8 < 2; ++ax0_ax1_fused_8) {\n    for (int32_t ax2_10 = 0; ax2_10 < 122; ++ax2_10) {\n      for (int32_t ax3_outer_10 = 0; ax3_outer_10 < 8; ++ax3_outer_10) {\n        for (int32_t ax3_inner_10 = 0; ax3_inner_10 < 16; ++ax3_inner_10) {\n          if (((ax3_outer_10 * 8) + (ax3_inner_10 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_8 * 14884) + (ax2_10 * 122)) + (ax3_outer_10 * 16)) + ax3_inner_10)] = ((ax0_ax1_fused_8 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_8 + 1) >> 1) * 14400) + (ax2_10 * 120)) + (ax3_outer_10 * 16)) + ax3_inner_10) - 14400)] : pad_temp[(((ax2_10 * 122) + (ax3_outer_10 * 16)) + ax3_inner_10)]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_9 = 0; ax0_ax1_fused_9 < 2; ++ax0_ax1_fused_9) {\n    for (int32_t ax2_11 = 0; ax2_11 < 124; ++ax2_11) {\n      for (int32_t ax3_outer_11 = 0; ax3_outer_11 < 8; ++ax3_outer_11) {\n        for (int32_t ax3_inner_11 = 0; ax3_inner_11 < 16; ++ax3_inner_11) {\n          if (((ax3_outer_11 * 4) + (ax3_inner_11 >> 2)) < 31) {\n            pad_temp[((((ax0_ax1_fused_9 * 15376) + (ax2_11 * 124)) + (ax3_outer_11 * 16)) + ax3_inner_11)] = (((((1 <= ax2_11) && (ax2_11 < 123)) && (1 <= ((ax3_outer_11 * 16) + ax3_inner_11))) && (((ax3_outer_11 * 16) + ax3_inner_11) < 123)) ? conv2d_nchw[(((((ax0_ax1_fused_9 * 14884) + (ax2_11 * 122)) + (ax3_outer_11 * 16)) + ax3_inner_11) - 123)] : -3.402823e+38f);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_10 = 0; ax0_ax1_fused_10 < 2; ++ax0_ax1_fused_10) {\n    for (int32_t ax2_12 = 0; ax2_12 < 122; ++ax2_12) {\n      for (int32_t ax3_outer_12 = 0; ax3_outer_12 < 8; ++ax3_outer_12) {\n        for (int32_t ax3_inner_12 = 0; ax3_inner_12 < 16; ++ax3_inner_12) {\n          if (((ax3_outer_12 * 8) + (ax3_inner_12 >> 1)) < 61) {\n            conv2d_nchw[((((ax0_ax1_fused_10 * 14884) + (ax2_12 * 122)) + (ax3_outer_12 * 16)) + ax3_inner_12)] = -3.402823e+38f;\n          }\n          if (((ax3_outer_12 * 8) + (ax3_inner_12 >> 1)) < 61) {\n            for (int32_t rv0_2 = 0; rv0_2 < 3; ++rv0_2) {\n              for (int32_t rv1_2 = 0; rv1_2 < 3; ++rv1_2) {\n                conv2d_nchw[((((ax0_ax1_fused_10 * 14884) + (ax2_12 * 122)) + (ax3_outer_12 * 16)) + ax3_inner_12)] = max(conv2d_nchw[((((ax0_ax1_fused_10 * 14884) + (ax2_12 * 122)) + (ax3_outer_12 * 16)) + ax3_inner_12)], pad_temp[((((((ax0_ax1_fused_10 * 15376) + (ax2_12 * 124)) + (rv0_2 * 124)) + (ax3_outer_12 * 16)) + ax3_inner_12) + rv1_2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t yy_22 = 0; yy_22 < 122; ++yy_22) {\n    for (int32_t xx_outer_22 = 0; xx_outer_22 < 8; ++xx_outer_22) {\n      for (int32_t xx_inner_22 = 0; xx_inner_22 < 16; ++xx_inner_22) {\n        if (((xx_outer_22 * 8) + (xx_inner_22 >> 1)) < 61) {\n          pad_temp[(((yy_22 * 122) + (xx_outer_22 * 16)) + xx_inner_22)] = 0.000000e+00f;\n        }\n        if (((xx_outer_22 * 8) + (xx_inner_22 >> 1)) < 61) {\n          for (int32_t rc_7 = 0; rc_7 < 2; ++rc_7) {\n            pad_temp[(((yy_22 * 122) + (xx_outer_22 * 16)) + xx_inner_22)] = (pad_temp[(((yy_22 * 122) + (xx_outer_22 * 16)) + xx_inner_22)] + (conv2d_nchw[((((rc_7 * 14884) + (yy_22 * 122)) + (xx_outer_22 * 16)) + xx_inner_22)] * ph_3[rc_7]));\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_37 = 0; i2_37 < 122; ++i2_37) {\n    for (int32_t i3_outer_37 = 0; i3_outer_37 < 8; ++i3_outer_37) {\n      for (int32_t i3_inner_37 = 0; i3_inner_37 < 16; ++i3_inner_37) {\n        if (((i3_outer_37 * 8) + (i3_inner_37 >> 1)) < 61) {\n          conv2d_nchw[(((i2_37 * 122) + (i3_outer_37 * 16)) + i3_inner_37)] = max(pad_temp[(((i2_37 * 122) + (i3_outer_37 * 16)) + i3_inner_37)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_38 = 0; i2_38 < 122; ++i2_38) {\n    for (int32_t i3_outer_38 = 0; i3_outer_38 < 8; ++i3_outer_38) {\n      for (int32_t i3_inner_38 = 0; i3_inner_38 < 16; ++i3_inner_38) {\n        if (((i3_outer_38 * 8) + (i3_inner_38 >> 1)) < 61) {\n          pad_temp[(((i2_38 * 122) + (i3_outer_38 * 16)) + i3_inner_38)] = conv2d_nchw[(((i2_38 * 122) + (i3_outer_38 * 16)) + i3_inner_38)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_23 = 0; yy_23 < 122; ++yy_23) {\n    for (int32_t xx_outer_23 = 0; xx_outer_23 < 8; ++xx_outer_23) {\n      for (int32_t xx_inner_23 = 0; xx_inner_23 < 16; ++xx_inner_23) {\n        if (((xx_outer_23 * 8) + (xx_inner_23 >> 1)) < 61) {\n          conv2d_nchw[(((yy_23 * 122) + (xx_outer_23 * 16)) + xx_inner_23)] = 0.000000e+00f;\n        }\n        if (((xx_outer_23 * 8) + (xx_inner_23 >> 1)) < 61) {\n          conv2d_nchw[(((yy_23 * 122) + (xx_outer_23 * 16)) + xx_inner_23)] = (conv2d_nchw[(((yy_23 * 122) + (xx_outer_23 * 16)) + xx_inner_23)] + (pad_temp[(((yy_23 * 122) + (xx_outer_23 * 16)) + xx_inner_23)] * ph_2[0]));\n        }\n      }\n    }\n  }\n  for (int32_t i2_39 = 0; i2_39 < 122; ++i2_39) {\n    for (int32_t i3_outer_39 = 0; i3_outer_39 < 8; ++i3_outer_39) {\n      for (int32_t i3_inner_39 = 0; i3_inner_39 < 16; ++i3_inner_39) {\n        if (((i3_outer_39 * 8) + (i3_inner_39 >> 1)) < 61) {\n          pad_temp[(((i2_39 * 122) + (i3_outer_39 * 16)) + i3_inner_39)] = max(conv2d_nchw[(((i2_39 * 122) + (i3_outer_39 * 16)) + i3_inner_39)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  for (int32_t i2_40 = 0; i2_40 < 122; ++i2_40) {\n    for (int32_t i3_outer_40 = 0; i3_outer_40 < 8; ++i3_outer_40) {\n      for (int32_t i3_inner_40 = 0; i3_inner_40 < 16; ++i3_inner_40) {\n        if (((i3_outer_40 * 8) + (i3_inner_40 >> 1)) < 61) {\n          conv2d_nchw[(((i2_40 * 122) + (i3_outer_40 * 16)) + i3_inner_40)] = pad_temp[(((i2_40 * 122) + (i3_outer_40 * 16)) + i3_inner_40)];\n        }\n      }\n    }\n  }\n  for (int32_t yy_24 = 0; yy_24 < 120; ++yy_24) {\n    for (int32_t xx_outer_24 = 0; xx_outer_24 < 8; ++xx_outer_24) {\n      for (int32_t xx_inner_24 = 0; xx_inner_24 < 16; ++xx_inner_24) {\n        if (((xx_outer_24 * 2) + (xx_inner_24 >> 3)) < 15) {\n          conv2d_nchw_1[(((yy_24 * 120) + (xx_outer_24 * 16)) + xx_inner_24)] = 0.000000e+00f;\n        }\n        if (((xx_outer_24 * 2) + (xx_inner_24 >> 3)) < 15) {\n          for (int32_t ry_8 = 0; ry_8 < 3; ++ry_8) {\n            for (int32_t rx_8 = 0; rx_8 < 3; ++rx_8) {\n              conv2d_nchw_1[(((yy_24 * 120) + (xx_outer_24 * 16)) + xx_inner_24)] = (conv2d_nchw_1[(((yy_24 * 120) + (xx_outer_24 * 16)) + xx_inner_24)] + (conv2d_nchw[(((((yy_24 * 122) + (ry_8 * 122)) + (xx_outer_24 * 16)) + xx_inner_24) + rx_8)] * ph_1[((ry_8 * 3) + rx_8)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t i2_41 = 0; i2_41 < 120; ++i2_41) {\n    for (int32_t i3_outer_41 = 0; i3_outer_41 < 8; ++i3_outer_41) {\n      for (int32_t i3_inner_41 = 0; i3_inner_41 < 16; ++i3_inner_41) {\n        if (((i3_outer_41 * 2) + (i3_inner_41 >> 3)) < 15) {\n          conv2d_nchw_1[(((i2_41 * 120) + (i3_outer_41 * 16)) + i3_inner_41)] = max(conv2d_nchw_1[(((i2_41 * 120) + (i3_outer_41 * 16)) + i3_inner_41)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_11 = 0; ax0_ax1_fused_11 < 2; ++ax0_ax1_fused_11) {\n    for (int32_t ax2_13 = 0; ax2_13 < 122; ++ax2_13) {\n      for (int32_t ax3_outer_13 = 0; ax3_outer_13 < 8; ++ax3_outer_13) {\n        for (int32_t ax3_inner_13 = 0; ax3_inner_13 < 16; ++ax3_inner_13) {\n          if (((ax3_outer_13 * 8) + (ax3_inner_13 >> 1)) < 61) {\n            T_concat[((((ax0_ax1_fused_11 * 14884) + (ax2_13 * 122)) + (ax3_outer_13 * 16)) + ax3_inner_13)] = ((ax0_ax1_fused_11 == 1) ? conv2d_nchw_1[(((((((ax0_ax1_fused_11 + 1) >> 1) * 14400) + (ax2_13 * 120)) + (ax3_outer_13 * 16)) + ax3_inner_13) - 14400)] : pad_temp[(((ax2_13 * 122) + (ax3_outer_13 * 16)) + ax3_inner_13)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_40(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_10(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_39(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_69(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_66(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_32(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 961) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (((((31 <= (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) % 3844)) && ((((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 15376) < 15252)) && (1 <= (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124))) && ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124) < 123)) ? conv2d_nchw[(((((((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) / 961) * 14884) + (((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) % 3844) / 31) * 122)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124)) - 123)] : -3.402823e+38f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_26(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_2(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_45(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_25(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_23(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_34(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_63(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_18(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 3; ++rc) {\n    for (int ry = 0; ry < 7; ++ry) {\n      for (int rx = 0; rx < 7; ++rx) {\n        if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n          conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[(((((rc * 16384) + ((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 61) * 128)) + (ry * 128)) + rx) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122))] * ph[(((rc * 49) + (ry * 7)) + rx)]));\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ph[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_75(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_31(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_36(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_21(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_41(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_78(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_55(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_43(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_70(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) < 961) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (((((31 <= (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) % 3844)) && ((((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 15376) < 15252)) && (1 <= (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124))) && ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124) < 123)) ? conv2d_nchw[(((((((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) / 961) * 14884) + (((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) % 3844) / 31) * 122)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124)) - 123)] : -3.402823e+38f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_24(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_20(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_8(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_52(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_64(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_48(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_59(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_53(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_61(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_72(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_62(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_49(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_27(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_28(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_30(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_22(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_37(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_5(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_68(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_14(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 2; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[(((rc * 14884) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] * ph[rc]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_50(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_77(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_79(float* __restrict__ conv2d_nchw) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_17(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_54(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_42(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 961) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (((((31 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) && (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3813)) && (1 <= (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124))) && ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124) < 123)) ? conv2d_nchw[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 31) * 122) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 124)) - 123)] : -3.402823e+38f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_16(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_56(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_60(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_29(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_76(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_65(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_80(float* __restrict__ T_concat, float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    T_concat[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_46(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_47(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_6(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_71(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], pad_temp[((((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 15376) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 124)) + (rv0 * 124)) + rv1) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122))]);\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_51(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw_1[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_74(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * ph[0]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_33(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], pad_temp[((((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 15376) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 124)) + (rv0 * 124)) + rv1) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122))]);\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_44(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((3721 <= ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2))) ? conv2d_nchw[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3721) * 14400) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7442) / 61) * 120)) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122)) - 14400)] : conv2d_nchw_1[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_58(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_38(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_73(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_57(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_4(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], pad_temp[(((((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 61) * 124) + (rv0 * 124)) + rv1) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 122))]);\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_35(float* __restrict__ conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 3721) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_67(float* __restrict__ conv2d_nchw, float* __restrict__ conv2d_nchw_1, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n    conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) < 225) {\n        conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (conv2d_nchw_1[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 15) * 122) + (ry * 122)) + rx) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 120))] * ph[((ry * 3) + rx)]));\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 3, 128, 128), \"float32\"), ph_1: T.Buffer((1, 96, 7, 7), \"float32\"), ph_2: T.Buffer((1, 16, 1, 1), \"float32\"), ph_3: T.Buffer((1, 64, 1, 1), \"float32\"), ph_4: T.Buffer((1, 64, 3, 3), \"float32\"), ph_5: T.Buffer((1, 16, 1, 1), \"float32\"), ph_6: T.Buffer((1, 64, 1, 1), \"float32\"), ph_7: T.Buffer((1, 64, 3, 3), \"float32\"), ph_8: T.Buffer((1, 32, 1, 1), \"float32\"), ph_9: T.Buffer((1, 128, 1, 1), \"float32\"), ph_10: T.Buffer((1, 128, 3, 3), \"float32\"), ph_11: T.Buffer((1, 32, 1, 1), \"float32\"), ph_12: T.Buffer((1, 128, 1, 1), \"float32\"), ph_13: T.Buffer((1, 128, 3, 3), \"float32\"), ph_14: T.Buffer((1, 48, 1, 1), \"float32\"), ph_15: T.Buffer((1, 192, 1, 1), \"float32\"), ph_16: T.Buffer((1, 192, 3, 3), \"float32\"), ph_17: T.Buffer((1, 48, 1, 1), \"float32\"), ph_18: T.Buffer((1, 192, 1, 1), \"float32\"), ph_19: T.Buffer((1, 192, 3, 3), \"float32\"), ph_20: T.Buffer((1, 64, 1, 1), \"float32\"), ph_21: T.Buffer((1, 256, 1, 1), \"float32\"), ph_22: T.Buffer((1, 256, 3, 3), \"float32\"), ph_23: T.Buffer((1, 64, 1, 1), \"float32\"), ph_24: T.Buffer((1, 256, 1, 1), \"float32\"), ph_25: T.Buffer((1, 256, 3, 3), \"float32\"), T_concat: T.Buffer((1, 2, 122, 122), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([49152], \"float32\", \"global\")\n        conv2d_nchw = T.allocate([29768], \"float32\", \"global\")\n        conv2d_nchw_1 = T.allocate([14400], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((49152,), data=pad_temp)\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                ph_26 = T.Buffer((49152,), data=ph.data)\n                pad_temp_1[cse_var_1] = ph_26[cse_var_1]\n        conv2d_nchw_2 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_2[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc, ry, rx in T.grid(3, 7, 7):\n                    cse_var_3: T.int32 = xx_outer * 16\n                    cse_var_2: T.int32 = yy * 122 + cse_var_3 + xx_inner\n                    ph_26 = T.Buffer((4704,), data=ph_1.data)\n                    conv2d_nchw_2[cse_var_2] = conv2d_nchw_2[cse_var_2] + pad_temp_1[rc * 16384 + yy * 128 + ry * 128 + cse_var_3 + xx_inner + rx] * ph_26[rc * 49 + ry * 7 + rx]\n        conv2d_nchw_3 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_4: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_3[cse_var_4] = T.max(conv2d_nchw_2[cse_var_4], T.float32(0))\n        pad_temp_2 = T.Buffer((15376,), data=pad_temp)\n        for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n            if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                cse_var_6: T.int32 = ax3_outer * 16\n                cse_var_5: T.int32 = cse_var_6 + ax3_inner\n                pad_temp_2[ax2 * 124 + cse_var_6 + ax3_inner] = T.if_then_else(1 <= ax2 and ax2 < 123 and 1 <= cse_var_5 and cse_var_5 < 123, conv2d_nchw_3[ax2 * 122 + cse_var_6 + ax3_inner - 123], T.float32(-3.4028234663852886e+38))\n        for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n            conv2d_nchw_4 = T.Buffer((14884,), data=conv2d_nchw)\n            if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                conv2d_nchw_4[ax2 * 122 + ax3_outer * 16 + ax3_inner] = T.float32(-3.4028234663852886e+38)\n            if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_8: T.int32 = ax3_outer * 16\n                    cse_var_7: T.int32 = ax2 * 122 + cse_var_8 + ax3_inner\n                    conv2d_nchw_4[cse_var_7] = T.max(conv2d_nchw_4[cse_var_7], pad_temp_2[ax2 * 124 + rv0 * 124 + cse_var_8 + ax3_inner + rv1])\n        pad_temp_3 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_3[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_4 = T.Buffer((14884,), data=conv2d_nchw)\n                ph_26 = T.Buffer((16,), data=ph_2.data)\n                cse_var_9: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                pad_temp_3[cse_var_9] = pad_temp_3[cse_var_9] + conv2d_nchw_4[cse_var_9] * ph_26[0]\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                conv2d_nchw_4 = T.Buffer((14884,), data=conv2d_nchw)\n                cse_var_10: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_4[cse_var_10] = T.max(pad_temp_3[cse_var_10], T.float32(0))\n        pad_temp_4 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_4[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_4 = T.Buffer((14884,), data=conv2d_nchw)\n                ph_26 = T.Buffer((64,), data=ph_3.data)\n                cse_var_11: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                pad_temp_4[cse_var_11] = pad_temp_4[cse_var_11] + conv2d_nchw_4[cse_var_11] * ph_26[0]\n        conv2d_nchw_4 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_12: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_4[cse_var_12] = T.max(pad_temp_4[cse_var_12], T.float32(0))\n        pad_temp_5 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_13: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_5[cse_var_13] = conv2d_nchw_4[cse_var_13]\n        conv2d_nchw_5 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_5[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_15: T.int32 = xx_outer * 16\n                    cse_var_14: T.int32 = yy * 120 + cse_var_15 + xx_inner\n                    ph_26 = T.Buffer((576,), data=ph_4.data)\n                    conv2d_nchw_5[cse_var_14] = conv2d_nchw_5[cse_var_14] + pad_temp_5[yy * 122 + ry * 122 + cse_var_15 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_6 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_16: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_6[cse_var_16] = T.max(conv2d_nchw_5[cse_var_16], T.float32(0))\n        pad_temp_6 = T.Buffer((29768,), data=pad_temp)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_18: T.int32 = ax2 * 122\n                    cse_var_17: T.int32 = ax3_outer * 16\n                    pad_temp_6[ax0_ax1_fused * 14884 + cse_var_18 + cse_var_17 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_6[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_17 + ax3_inner - 14400], conv2d_nchw_4[cse_var_18 + cse_var_17 + ax3_inner])\n        conv2d_nchw_7 = T.Buffer((29768,), data=conv2d_nchw)\n        for i0_i1_fused in T.parallel(2):\n            for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                    cse_var_19: T.int32 = i0_i1_fused * 14884 + i2 * 122 + i3_outer * 16 + i3_inner\n                    conv2d_nchw_7[cse_var_19] = pad_temp_6[cse_var_19]\n        pad_temp_7 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_7[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_22: T.int32 = yy * 122\n                    cse_var_21: T.int32 = xx_outer * 16\n                    cse_var_20: T.int32 = cse_var_22 + cse_var_21 + xx_inner\n                    ph_26 = T.Buffer((16,), data=ph_5.data)\n                    pad_temp_7[cse_var_20] = pad_temp_7[cse_var_20] + conv2d_nchw_7[rc * 14884 + cse_var_22 + cse_var_21 + xx_inner] * ph_26[rc]\n        conv2d_nchw_8 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_23: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_8[cse_var_23] = T.max(pad_temp_7[cse_var_23], T.float32(0))\n        pad_temp_8 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_24: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_8[cse_var_24] = conv2d_nchw_8[cse_var_24]\n        conv2d_nchw_9 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_9[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((64,), data=ph_6.data)\n                cse_var_25: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_9[cse_var_25] = conv2d_nchw_9[cse_var_25] + pad_temp_8[cse_var_25] * ph_26[0]\n        pad_temp_9 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_26: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_9[cse_var_26] = T.max(conv2d_nchw_9[cse_var_26], T.float32(0))\n        conv2d_nchw_10 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_27: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_10[cse_var_27] = pad_temp_9[cse_var_27]\n        conv2d_nchw_11 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_11[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_29: T.int32 = xx_outer * 16\n                    cse_var_28: T.int32 = yy * 120 + cse_var_29 + xx_inner\n                    ph_26 = T.Buffer((576,), data=ph_7.data)\n                    conv2d_nchw_11[cse_var_28] = conv2d_nchw_11[cse_var_28] + conv2d_nchw_10[yy * 122 + ry * 122 + cse_var_29 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_12 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_30: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_12[cse_var_30] = T.max(conv2d_nchw_11[cse_var_30], T.float32(0))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_32: T.int32 = ax2 * 122\n                    cse_var_31: T.int32 = ax3_outer * 16\n                    conv2d_nchw_13 = T.Buffer((29768,), data=conv2d_nchw)\n                    conv2d_nchw_13[ax0_ax1_fused * 14884 + cse_var_32 + cse_var_31 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_12[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_31 + ax3_inner - 14400], pad_temp_9[cse_var_32 + cse_var_31 + ax3_inner])\n        pad_temp_10 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_10[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_35: T.int32 = yy * 122\n                    cse_var_34: T.int32 = xx_outer * 16\n                    cse_var_33: T.int32 = cse_var_35 + cse_var_34 + xx_inner\n                    conv2d_nchw_13 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((32,), data=ph_8.data)\n                    pad_temp_10[cse_var_33] = pad_temp_10[cse_var_33] + conv2d_nchw_13[rc * 14884 + cse_var_35 + cse_var_34 + xx_inner] * ph_26[rc]\n        conv2d_nchw_13 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_36: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_13[cse_var_36] = T.max(pad_temp_10[cse_var_36], T.float32(0))\n        pad_temp_11 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_37: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_11[cse_var_37] = conv2d_nchw_13[cse_var_37]\n        conv2d_nchw_14 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_14[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((128,), data=ph_9.data)\n                cse_var_38: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_14[cse_var_38] = conv2d_nchw_14[cse_var_38] + pad_temp_11[cse_var_38] * ph_26[0]\n        pad_temp_12 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_39: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_12[cse_var_39] = T.max(conv2d_nchw_14[cse_var_39], T.float32(0))\n        conv2d_nchw_15 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_40: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_15[cse_var_40] = pad_temp_12[cse_var_40]\n        conv2d_nchw_16 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_16[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_42: T.int32 = xx_outer * 16\n                    cse_var_41: T.int32 = yy * 120 + cse_var_42 + xx_inner\n                    ph_26 = T.Buffer((1152,), data=ph_10.data)\n                    conv2d_nchw_16[cse_var_41] = conv2d_nchw_16[cse_var_41] + conv2d_nchw_15[yy * 122 + ry * 122 + cse_var_42 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_17 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_43: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_17[cse_var_43] = T.max(conv2d_nchw_16[cse_var_43], T.float32(0))\n        conv2d_nchw_18 = T.Buffer((29768,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_45: T.int32 = ax2 * 122\n                    cse_var_44: T.int32 = ax3_outer * 16\n                    conv2d_nchw_18[ax0_ax1_fused * 14884 + cse_var_45 + cse_var_44 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_17[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_44 + ax3_inner - 14400], pad_temp_12[cse_var_45 + cse_var_44 + ax3_inner])\n        pad_temp_13 = T.Buffer((30752,), data=pad_temp)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_47: T.int32 = ax3_outer * 16\n                    cse_var_46: T.int32 = cse_var_47 + ax3_inner\n                    pad_temp_13[ax0_ax1_fused * 15376 + ax2 * 124 + cse_var_47 + ax3_inner] = T.if_then_else(1 <= ax2 and ax2 < 123 and 1 <= cse_var_46 and cse_var_46 < 123, conv2d_nchw_18[ax0_ax1_fused * 14884 + ax2 * 122 + cse_var_47 + ax3_inner - 123], T.float32(-3.4028234663852886e+38))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                conv2d_nchw_19 = T.Buffer((29768,), data=conv2d_nchw)\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    conv2d_nchw_19[ax0_ax1_fused * 14884 + ax2 * 122 + ax3_outer * 16 + ax3_inner] = T.float32(-3.4028234663852886e+38)\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_49: T.int32 = ax3_outer * 16\n                        cse_var_48: T.int32 = ax0_ax1_fused * 14884 + ax2 * 122 + cse_var_49 + ax3_inner\n                        conv2d_nchw_19[cse_var_48] = T.max(conv2d_nchw_19[cse_var_48], pad_temp_13[ax0_ax1_fused * 15376 + ax2 * 124 + rv0 * 124 + cse_var_49 + ax3_inner + rv1])\n        pad_temp_14 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_14[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_52: T.int32 = yy * 122\n                    cse_var_51: T.int32 = xx_outer * 16\n                    cse_var_50: T.int32 = cse_var_52 + cse_var_51 + xx_inner\n                    conv2d_nchw_19 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((32,), data=ph_11.data)\n                    pad_temp_14[cse_var_50] = pad_temp_14[cse_var_50] + conv2d_nchw_19[rc * 14884 + cse_var_52 + cse_var_51 + xx_inner] * ph_26[rc]\n        conv2d_nchw_19 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_53: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_19[cse_var_53] = T.max(pad_temp_14[cse_var_53], T.float32(0))\n        pad_temp_15 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_54: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_15[cse_var_54] = conv2d_nchw_19[cse_var_54]\n        conv2d_nchw_20 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_20[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((128,), data=ph_12.data)\n                cse_var_55: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_20[cse_var_55] = conv2d_nchw_20[cse_var_55] + pad_temp_15[cse_var_55] * ph_26[0]\n        pad_temp_16 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_56: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_16[cse_var_56] = T.max(conv2d_nchw_20[cse_var_56], T.float32(0))\n        conv2d_nchw_21 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_57: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_21[cse_var_57] = pad_temp_16[cse_var_57]\n        conv2d_nchw_22 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_22[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_59: T.int32 = xx_outer * 16\n                    cse_var_58: T.int32 = yy * 120 + cse_var_59 + xx_inner\n                    ph_26 = T.Buffer((1152,), data=ph_13.data)\n                    conv2d_nchw_22[cse_var_58] = conv2d_nchw_22[cse_var_58] + conv2d_nchw_21[yy * 122 + ry * 122 + cse_var_59 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_23 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_60: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_23[cse_var_60] = T.max(conv2d_nchw_22[cse_var_60], T.float32(0))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_62: T.int32 = ax2 * 122\n                    cse_var_61: T.int32 = ax3_outer * 16\n                    conv2d_nchw_24 = T.Buffer((29768,), data=conv2d_nchw)\n                    conv2d_nchw_24[ax0_ax1_fused * 14884 + cse_var_62 + cse_var_61 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_23[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_61 + ax3_inner - 14400], pad_temp_16[cse_var_62 + cse_var_61 + ax3_inner])\n        pad_temp_17 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_17[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_65: T.int32 = yy * 122\n                    cse_var_64: T.int32 = xx_outer * 16\n                    cse_var_63: T.int32 = cse_var_65 + cse_var_64 + xx_inner\n                    conv2d_nchw_24 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((48,), data=ph_14.data)\n                    pad_temp_17[cse_var_63] = pad_temp_17[cse_var_63] + conv2d_nchw_24[rc * 14884 + cse_var_65 + cse_var_64 + xx_inner] * ph_26[rc]\n        conv2d_nchw_24 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_66: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_24[cse_var_66] = T.max(pad_temp_17[cse_var_66], T.float32(0))\n        pad_temp_18 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_67: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_18[cse_var_67] = conv2d_nchw_24[cse_var_67]\n        conv2d_nchw_25 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_25[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((192,), data=ph_15.data)\n                cse_var_68: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_25[cse_var_68] = conv2d_nchw_25[cse_var_68] + pad_temp_18[cse_var_68] * ph_26[0]\n        pad_temp_19 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_69: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_19[cse_var_69] = T.max(conv2d_nchw_25[cse_var_69], T.float32(0))\n        conv2d_nchw_26 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_70: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_26[cse_var_70] = pad_temp_19[cse_var_70]\n        conv2d_nchw_27 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_27[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_72: T.int32 = xx_outer * 16\n                    cse_var_71: T.int32 = yy * 120 + cse_var_72 + xx_inner\n                    ph_26 = T.Buffer((1728,), data=ph_16.data)\n                    conv2d_nchw_27[cse_var_71] = conv2d_nchw_27[cse_var_71] + conv2d_nchw_26[yy * 122 + ry * 122 + cse_var_72 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_28 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_73: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_28[cse_var_73] = T.max(conv2d_nchw_27[cse_var_73], T.float32(0))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_75: T.int32 = ax2 * 122\n                    cse_var_74: T.int32 = ax3_outer * 16\n                    conv2d_nchw_29 = T.Buffer((29768,), data=conv2d_nchw)\n                    conv2d_nchw_29[ax0_ax1_fused * 14884 + cse_var_75 + cse_var_74 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_28[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_74 + ax3_inner - 14400], pad_temp_19[cse_var_75 + cse_var_74 + ax3_inner])\n        pad_temp_20 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_20[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_78: T.int32 = yy * 122\n                    cse_var_77: T.int32 = xx_outer * 16\n                    cse_var_76: T.int32 = cse_var_78 + cse_var_77 + xx_inner\n                    conv2d_nchw_29 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((48,), data=ph_17.data)\n                    pad_temp_20[cse_var_76] = pad_temp_20[cse_var_76] + conv2d_nchw_29[rc * 14884 + cse_var_78 + cse_var_77 + xx_inner] * ph_26[rc]\n        conv2d_nchw_29 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_79: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_29[cse_var_79] = T.max(pad_temp_20[cse_var_79], T.float32(0))\n        pad_temp_21 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_80: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_21[cse_var_80] = conv2d_nchw_29[cse_var_80]\n        conv2d_nchw_30 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_30[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((192,), data=ph_18.data)\n                cse_var_81: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_30[cse_var_81] = conv2d_nchw_30[cse_var_81] + pad_temp_21[cse_var_81] * ph_26[0]\n        pad_temp_22 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_82: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_22[cse_var_82] = T.max(conv2d_nchw_30[cse_var_82], T.float32(0))\n        conv2d_nchw_31 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_83: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_31[cse_var_83] = pad_temp_22[cse_var_83]\n        conv2d_nchw_32 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_32[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_85: T.int32 = xx_outer * 16\n                    cse_var_84: T.int32 = yy * 120 + cse_var_85 + xx_inner\n                    ph_26 = T.Buffer((1728,), data=ph_19.data)\n                    conv2d_nchw_32[cse_var_84] = conv2d_nchw_32[cse_var_84] + conv2d_nchw_31[yy * 122 + ry * 122 + cse_var_85 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_33 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_86: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_33[cse_var_86] = T.max(conv2d_nchw_32[cse_var_86], T.float32(0))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_88: T.int32 = ax2 * 122\n                    cse_var_87: T.int32 = ax3_outer * 16\n                    conv2d_nchw_34 = T.Buffer((29768,), data=conv2d_nchw)\n                    conv2d_nchw_34[ax0_ax1_fused * 14884 + cse_var_88 + cse_var_87 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_33[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_87 + ax3_inner - 14400], pad_temp_22[cse_var_88 + cse_var_87 + ax3_inner])\n        pad_temp_23 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_23[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_91: T.int32 = yy * 122\n                    cse_var_90: T.int32 = xx_outer * 16\n                    cse_var_89: T.int32 = cse_var_91 + cse_var_90 + xx_inner\n                    conv2d_nchw_34 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((64,), data=ph_20.data)\n                    pad_temp_23[cse_var_89] = pad_temp_23[cse_var_89] + conv2d_nchw_34[rc * 14884 + cse_var_91 + cse_var_90 + xx_inner] * ph_26[rc]\n        conv2d_nchw_34 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_92: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_34[cse_var_92] = T.max(pad_temp_23[cse_var_92], T.float32(0))\n        pad_temp_24 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_93: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_24[cse_var_93] = conv2d_nchw_34[cse_var_93]\n        conv2d_nchw_35 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_35[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((256,), data=ph_21.data)\n                cse_var_94: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_35[cse_var_94] = conv2d_nchw_35[cse_var_94] + pad_temp_24[cse_var_94] * ph_26[0]\n        pad_temp_25 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_95: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_25[cse_var_95] = T.max(conv2d_nchw_35[cse_var_95], T.float32(0))\n        conv2d_nchw_36 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_96: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_36[cse_var_96] = pad_temp_25[cse_var_96]\n        conv2d_nchw_37 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_37[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_98: T.int32 = xx_outer * 16\n                    cse_var_97: T.int32 = yy * 120 + cse_var_98 + xx_inner\n                    ph_26 = T.Buffer((2304,), data=ph_22.data)\n                    conv2d_nchw_37[cse_var_97] = conv2d_nchw_37[cse_var_97] + conv2d_nchw_36[yy * 122 + ry * 122 + cse_var_98 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_38 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_99: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_38[cse_var_99] = T.max(conv2d_nchw_37[cse_var_99], T.float32(0))\n        conv2d_nchw_39 = T.Buffer((29768,), data=conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_101: T.int32 = ax2 * 122\n                    cse_var_100: T.int32 = ax3_outer * 16\n                    conv2d_nchw_39[ax0_ax1_fused * 14884 + cse_var_101 + cse_var_100 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_38[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_100 + ax3_inner - 14400], pad_temp_25[cse_var_101 + cse_var_100 + ax3_inner])\n        pad_temp_26 = T.Buffer((30752,), data=pad_temp)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(124, 8, 16):\n                if T.likely(ax3_outer * 4 + ax3_inner // 4 < 31):\n                    cse_var_103: T.int32 = ax3_outer * 16\n                    cse_var_102: T.int32 = cse_var_103 + ax3_inner\n                    pad_temp_26[ax0_ax1_fused * 15376 + ax2 * 124 + cse_var_103 + ax3_inner] = T.if_then_else(1 <= ax2 and ax2 < 123 and 1 <= cse_var_102 and cse_var_102 < 123, conv2d_nchw_39[ax0_ax1_fused * 14884 + ax2 * 122 + cse_var_103 + ax3_inner - 123], T.float32(-3.4028234663852886e+38))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                conv2d_nchw_40 = T.Buffer((29768,), data=conv2d_nchw)\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    conv2d_nchw_40[ax0_ax1_fused * 14884 + ax2 * 122 + ax3_outer * 16 + ax3_inner] = T.float32(-3.4028234663852886e+38)\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_105: T.int32 = ax3_outer * 16\n                        cse_var_104: T.int32 = ax0_ax1_fused * 14884 + ax2 * 122 + cse_var_105 + ax3_inner\n                        conv2d_nchw_40[cse_var_104] = T.max(conv2d_nchw_40[cse_var_104], pad_temp_26[ax0_ax1_fused * 15376 + ax2 * 124 + rv0 * 124 + cse_var_105 + ax3_inner + rv1])\n        pad_temp_27 = T.Buffer((14884,), data=pad_temp)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                pad_temp_27[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                for rc in range(2):\n                    cse_var_108: T.int32 = yy * 122\n                    cse_var_107: T.int32 = xx_outer * 16\n                    cse_var_106: T.int32 = cse_var_108 + cse_var_107 + xx_inner\n                    conv2d_nchw_40 = T.Buffer((29768,), data=conv2d_nchw)\n                    ph_26 = T.Buffer((64,), data=ph_23.data)\n                    pad_temp_27[cse_var_106] = pad_temp_27[cse_var_106] + conv2d_nchw_40[rc * 14884 + cse_var_108 + cse_var_107 + xx_inner] * ph_26[rc]\n        conv2d_nchw_40 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_109: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_40[cse_var_109] = T.max(pad_temp_27[cse_var_109], T.float32(0))\n        pad_temp_28 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_110: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_28[cse_var_110] = conv2d_nchw_40[cse_var_110]\n        conv2d_nchw_41 = T.Buffer((14884,), data=conv2d_nchw)\n        for yy, xx_outer, xx_inner in T.grid(122, 8, 16):\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                conv2d_nchw_41[yy * 122 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 8 + xx_inner // 2 < 61):\n                ph_26 = T.Buffer((256,), data=ph_24.data)\n                cse_var_111: T.int32 = yy * 122 + xx_outer * 16 + xx_inner\n                conv2d_nchw_41[cse_var_111] = conv2d_nchw_41[cse_var_111] + pad_temp_28[cse_var_111] * ph_26[0]\n        pad_temp_29 = T.Buffer((14884,), data=pad_temp)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_112: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                pad_temp_29[cse_var_112] = T.max(conv2d_nchw_41[cse_var_112], T.float32(0))\n        conv2d_nchw_42 = T.Buffer((14884,), data=conv2d_nchw)\n        for i2, i3_outer, i3_inner in T.grid(122, 8, 16):\n            if T.likely(i3_outer * 8 + i3_inner // 2 < 61):\n                cse_var_113: T.int32 = i2 * 122 + i3_outer * 16 + i3_inner\n                conv2d_nchw_42[cse_var_113] = pad_temp_29[cse_var_113]\n        conv2d_nchw_43 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for yy, xx_outer, xx_inner in T.grid(120, 8, 16):\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                conv2d_nchw_43[yy * 120 + xx_outer * 16 + xx_inner] = T.float32(0)\n            if T.likely(xx_outer * 2 + xx_inner // 8 < 15):\n                for ry, rx in T.grid(3, 3):\n                    cse_var_115: T.int32 = xx_outer * 16\n                    cse_var_114: T.int32 = yy * 120 + cse_var_115 + xx_inner\n                    ph_26 = T.Buffer((2304,), data=ph_25.data)\n                    conv2d_nchw_43[cse_var_114] = conv2d_nchw_43[cse_var_114] + conv2d_nchw_42[yy * 122 + ry * 122 + cse_var_115 + xx_inner + rx] * ph_26[ry * 3 + rx]\n        conv2d_nchw_44 = T.Buffer((14400,), data=conv2d_nchw_1)\n        for i2, i3_outer, i3_inner in T.grid(120, 8, 16):\n            if T.likely(i3_outer * 2 + i3_inner // 8 < 15):\n                cse_var_116: T.int32 = i2 * 120 + i3_outer * 16 + i3_inner\n                conv2d_nchw_44[cse_var_116] = T.max(conv2d_nchw_43[cse_var_116], T.float32(0))\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2, ax3_outer, ax3_inner in T.grid(122, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 61):\n                    cse_var_118: T.int32 = ax2 * 122\n                    cse_var_117: T.int32 = ax3_outer * 16\n                    T_concat_1 = T.Buffer((29768,), data=T_concat.data)\n                    T_concat_1[ax0_ax1_fused * 14884 + cse_var_118 + cse_var_117 + ax3_inner] = T.if_then_else(ax0_ax1_fused == 1, conv2d_nchw_44[(ax0_ax1_fused + 1) // 2 * 14400 + ax2 * 120 + cse_var_117 + ax3_inner - 14400], pad_temp_29[cse_var_118 + cse_var_117 + ax3_inner])",
        "op_args": "None",
        "input_shape": [
            [
                1,
                3,
                128,
                128
            ],
            [
                1,
                96,
                7,
                7
            ],
            [
                1,
                16,
                1,
                1
            ],
            [
                1,
                64,
                1,
                1
            ],
            [
                1,
                64,
                3,
                3
            ],
            [
                1,
                16,
                1,
                1
            ],
            [
                1,
                64,
                1,
                1
            ],
            [
                1,
                64,
                3,
                3
            ],
            [
                1,
                32,
                1,
                1
            ],
            [
                1,
                128,
                1,
                1
            ],
            [
                1,
                128,
                3,
                3
            ],
            [
                1,
                32,
                1,
                1
            ],
            [
                1,
                128,
                1,
                1
            ],
            [
                1,
                128,
                3,
                3
            ],
            [
                1,
                48,
                1,
                1
            ],
            [
                1,
                192,
                1,
                1
            ],
            [
                1,
                192,
                3,
                3
            ],
            [
                1,
                48,
                1,
                1
            ],
            [
                1,
                192,
                1,
                1
            ],
            [
                1,
                192,
                3,
                3
            ],
            [
                1,
                64,
                1,
                1
            ],
            [
                1,
                256,
                1,
                1
            ],
            [
                1,
                256,
                3,
                3
            ],
            [
                1,
                64,
                1,
                1
            ],
            [
                1,
                256,
                1,
                1
            ],
            [
                1,
                256,
                3,
                3
            ]
        ],
        "output_shape": [
            [
                1,
                2,
                122,
                122
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "T_concat"
        ]
    },
    {
        "op_name": "depthwise",
        "c_code": "void default_function_kernel(float* compute, float* ph, float* ph_1, float* ph_2, float* ph_3, float* ph_4, float* ph_5, float* ph_6, float* ph_7, float* ph_8, float* ph_9, float* ph_10) {\n  float pad_temp[49152];\n  float group_conv2d_nchw[47628];\n  float T_reshape[3];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 8; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          pad_temp[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)] = ph[((((i0_i1_fused * 16384) + (i2 * 128)) + (i3_outer * 16)) + i3_inner)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused = 0; nn_ff_fused < 3; ++nn_ff_fused) {\n    for (int32_t yy = 0; yy < 126; ++yy) {\n      for (int32_t xx_outer = 0; xx_outer < 8; ++xx_outer) {\n        for (int32_t xx_inner = 0; xx_inner < 16; ++xx_inner) {\n          if (((xx_outer * 8) + (xx_inner >> 1)) < 63) {\n            group_conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] = 0.000000e+00f;\n          }\n          if (((xx_outer * 8) + (xx_inner >> 1)) < 63) {\n            for (int32_t ry = 0; ry < 3; ++ry) {\n              for (int32_t rx = 0; rx < 3; ++rx) {\n                group_conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] = (group_conv2d_nchw[((((nn_ff_fused * 15876) + (yy * 126)) + (xx_outer * 16)) + xx_inner)] + (pad_temp[((((((nn_ff_fused * 16384) + (yy * 128)) + (ry * 128)) + (xx_outer * 16)) + xx_inner) + rx)] * ph_1[(((nn_ff_fused * 27) + (ry * 3)) + rx)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 3; ++ax0_ax1_fused) {\n    T_reshape[ax0_ax1_fused] = ph_2[ax0_ax1_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 3; ++ax0_ax1_fused_1) {\n    for (int32_t ax2 = 0; ax2 < 126; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 8; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          if (((ax3_outer * 8) + (ax3_inner >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_1 * 15876) + (ax2 * 126)) + (ax3_outer * 16)) + ax3_inner)] = (group_conv2d_nchw[((((ax0_ax1_fused_1 * 15876) + (ax2 * 126)) + (ax3_outer * 16)) + ax3_inner)] - T_reshape[ax0_ax1_fused_1]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 3; ++ax0_ax1_fused_2) {\n    T_reshape[ax0_ax1_fused_2] = ph_3[ax0_ax1_fused_2];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_3 = 0; ax0_ax1_fused_3 < 3; ++ax0_ax1_fused_3) {\n    T_reshape[ax0_ax1_fused_3] = (T_reshape[ax0_ax1_fused_3] + 1.000000e-05f);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 3; ++i0_i1_fused_1) {\n    T_reshape[i0_i1_fused_1] = sqrtf(T_reshape[i0_i1_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_4 = 0; ax0_ax1_fused_4 < 3; ++ax0_ax1_fused_4) {\n    for (int32_t ax2_1 = 0; ax2_1 < 126; ++ax2_1) {\n      for (int32_t ax3_outer_1 = 0; ax3_outer_1 < 8; ++ax3_outer_1) {\n        for (int32_t ax3_inner_1 = 0; ax3_inner_1 < 16; ++ax3_inner_1) {\n          if (((ax3_outer_1 * 8) + (ax3_inner_1 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_4 * 15876) + (ax2_1 * 126)) + (ax3_outer_1 * 16)) + ax3_inner_1)] = (group_conv2d_nchw[((((ax0_ax1_fused_4 * 15876) + (ax2_1 * 126)) + (ax3_outer_1 * 16)) + ax3_inner_1)] / T_reshape[ax0_ax1_fused_4]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_5 = 0; ax0_ax1_fused_5 < 3; ++ax0_ax1_fused_5) {\n    T_reshape[ax0_ax1_fused_5] = ph_4[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_6 = 0; ax0_ax1_fused_6 < 3; ++ax0_ax1_fused_6) {\n    for (int32_t ax2_2 = 0; ax2_2 < 126; ++ax2_2) {\n      for (int32_t ax3_outer_2 = 0; ax3_outer_2 < 8; ++ax3_outer_2) {\n        for (int32_t ax3_inner_2 = 0; ax3_inner_2 < 16; ++ax3_inner_2) {\n          if (((ax3_outer_2 * 8) + (ax3_inner_2 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_6 * 15876) + (ax2_2 * 126)) + (ax3_outer_2 * 16)) + ax3_inner_2)] = (group_conv2d_nchw[((((ax0_ax1_fused_6 * 15876) + (ax2_2 * 126)) + (ax3_outer_2 * 16)) + ax3_inner_2)] * T_reshape[ax0_ax1_fused_6]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_7 = 0; ax0_ax1_fused_7 < 3; ++ax0_ax1_fused_7) {\n    T_reshape[ax0_ax1_fused_7] = ph_5[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_8 = 0; ax0_ax1_fused_8 < 3; ++ax0_ax1_fused_8) {\n    for (int32_t ax2_3 = 0; ax2_3 < 126; ++ax2_3) {\n      for (int32_t ax3_outer_3 = 0; ax3_outer_3 < 8; ++ax3_outer_3) {\n        for (int32_t ax3_inner_3 = 0; ax3_inner_3 < 16; ++ax3_inner_3) {\n          if (((ax3_outer_3 * 8) + (ax3_inner_3 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_8 * 15876) + (ax2_3 * 126)) + (ax3_outer_3 * 16)) + ax3_inner_3)] = (group_conv2d_nchw[((((ax0_ax1_fused_8 * 15876) + (ax2_3 * 126)) + (ax3_outer_3 * 16)) + ax3_inner_3)] + T_reshape[ax0_ax1_fused_8]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_2 = 0; i0_i1_fused_2 < 3; ++i0_i1_fused_2) {\n    for (int32_t i2_1 = 0; i2_1 < 126; ++i2_1) {\n      for (int32_t i3_outer_1 = 0; i3_outer_1 < 8; ++i3_outer_1) {\n        for (int32_t i3_inner_1 = 0; i3_inner_1 < 16; ++i3_inner_1) {\n          if (((i3_outer_1 * 8) + (i3_inner_1 >> 1)) < 63) {\n            group_conv2d_nchw[((((i0_i1_fused_2 * 15876) + (i2_1 * 126)) + (i3_outer_1 * 16)) + i3_inner_1)] = max(group_conv2d_nchw[((((i0_i1_fused_2 * 15876) + (i2_1 * 126)) + (i3_outer_1 * 16)) + i3_inner_1)], 0.000000e+00f);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t nn_ff_fused_1 = 0; nn_ff_fused_1 < 3; ++nn_ff_fused_1) {\n    for (int32_t yy_1 = 0; yy_1 < 126; ++yy_1) {\n      for (int32_t xx_outer_1 = 0; xx_outer_1 < 8; ++xx_outer_1) {\n        for (int32_t xx_inner_1 = 0; xx_inner_1 < 16; ++xx_inner_1) {\n          if (((xx_outer_1 * 8) + (xx_inner_1 >> 1)) < 63) {\n            pad_temp[((((nn_ff_fused_1 * 15876) + (yy_1 * 126)) + (xx_outer_1 * 16)) + xx_inner_1)] = 0.000000e+00f;\n          }\n          if (((xx_outer_1 * 8) + (xx_inner_1 >> 1)) < 63) {\n            for (int32_t rc = 0; rc < 3; ++rc) {\n              pad_temp[((((nn_ff_fused_1 * 15876) + (yy_1 * 126)) + (xx_outer_1 * 16)) + xx_inner_1)] = (pad_temp[((((nn_ff_fused_1 * 15876) + (yy_1 * 126)) + (xx_outer_1 * 16)) + xx_inner_1)] + (group_conv2d_nchw[((((rc * 15876) + (yy_1 * 126)) + (xx_outer_1 * 16)) + xx_inner_1)] * ph_6[((nn_ff_fused_1 * 9) + rc)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_9 = 0; ax0_ax1_fused_9 < 3; ++ax0_ax1_fused_9) {\n    T_reshape[ax0_ax1_fused_9] = ph_7[ax0_ax1_fused_9];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_10 = 0; ax0_ax1_fused_10 < 3; ++ax0_ax1_fused_10) {\n    for (int32_t ax2_4 = 0; ax2_4 < 126; ++ax2_4) {\n      for (int32_t ax3_outer_4 = 0; ax3_outer_4 < 8; ++ax3_outer_4) {\n        for (int32_t ax3_inner_4 = 0; ax3_inner_4 < 16; ++ax3_inner_4) {\n          if (((ax3_outer_4 * 8) + (ax3_inner_4 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_10 * 15876) + (ax2_4 * 126)) + (ax3_outer_4 * 16)) + ax3_inner_4)] = (pad_temp[((((ax0_ax1_fused_10 * 15876) + (ax2_4 * 126)) + (ax3_outer_4 * 16)) + ax3_inner_4)] - T_reshape[ax0_ax1_fused_10]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_11 = 0; ax0_ax1_fused_11 < 3; ++ax0_ax1_fused_11) {\n    T_reshape[ax0_ax1_fused_11] = ph_8[ax0_ax1_fused_11];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_12 = 0; ax0_ax1_fused_12 < 3; ++ax0_ax1_fused_12) {\n    T_reshape[ax0_ax1_fused_12] = (T_reshape[ax0_ax1_fused_12] + 1.000000e-05f);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_3 = 0; i0_i1_fused_3 < 3; ++i0_i1_fused_3) {\n    T_reshape[i0_i1_fused_3] = sqrtf(T_reshape[i0_i1_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_13 = 0; ax0_ax1_fused_13 < 3; ++ax0_ax1_fused_13) {\n    for (int32_t ax2_5 = 0; ax2_5 < 126; ++ax2_5) {\n      for (int32_t ax3_outer_5 = 0; ax3_outer_5 < 8; ++ax3_outer_5) {\n        for (int32_t ax3_inner_5 = 0; ax3_inner_5 < 16; ++ax3_inner_5) {\n          if (((ax3_outer_5 * 8) + (ax3_inner_5 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_13 * 15876) + (ax2_5 * 126)) + (ax3_outer_5 * 16)) + ax3_inner_5)] = (group_conv2d_nchw[((((ax0_ax1_fused_13 * 15876) + (ax2_5 * 126)) + (ax3_outer_5 * 16)) + ax3_inner_5)] / T_reshape[ax0_ax1_fused_13]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_14 = 0; ax0_ax1_fused_14 < 3; ++ax0_ax1_fused_14) {\n    T_reshape[ax0_ax1_fused_14] = ph_9[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_15 = 0; ax0_ax1_fused_15 < 3; ++ax0_ax1_fused_15) {\n    for (int32_t ax2_6 = 0; ax2_6 < 126; ++ax2_6) {\n      for (int32_t ax3_outer_6 = 0; ax3_outer_6 < 8; ++ax3_outer_6) {\n        for (int32_t ax3_inner_6 = 0; ax3_inner_6 < 16; ++ax3_inner_6) {\n          if (((ax3_outer_6 * 8) + (ax3_inner_6 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_15 * 15876) + (ax2_6 * 126)) + (ax3_outer_6 * 16)) + ax3_inner_6)] = (group_conv2d_nchw[((((ax0_ax1_fused_15 * 15876) + (ax2_6 * 126)) + (ax3_outer_6 * 16)) + ax3_inner_6)] * T_reshape[ax0_ax1_fused_15]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_16 = 0; ax0_ax1_fused_16 < 3; ++ax0_ax1_fused_16) {\n    T_reshape[ax0_ax1_fused_16] = ph_10[0];\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_17 = 0; ax0_ax1_fused_17 < 3; ++ax0_ax1_fused_17) {\n    for (int32_t ax2_7 = 0; ax2_7 < 126; ++ax2_7) {\n      for (int32_t ax3_outer_7 = 0; ax3_outer_7 < 8; ++ax3_outer_7) {\n        for (int32_t ax3_inner_7 = 0; ax3_inner_7 < 16; ++ax3_inner_7) {\n          if (((ax3_outer_7 * 8) + (ax3_inner_7 >> 1)) < 63) {\n            group_conv2d_nchw[((((ax0_ax1_fused_17 * 15876) + (ax2_7 * 126)) + (ax3_outer_7 * 16)) + ax3_inner_7)] = (group_conv2d_nchw[((((ax0_ax1_fused_17 * 15876) + (ax2_7 * 126)) + (ax3_outer_7 * 16)) + ax3_inner_7)] + T_reshape[ax0_ax1_fused_17]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_4 = 0; i0_i1_fused_4 < 3; ++i0_i1_fused_4) {\n    for (int32_t i2_2 = 0; i2_2 < 126; ++i2_2) {\n      for (int32_t i3_outer_2 = 0; i3_outer_2 < 8; ++i3_outer_2) {\n        for (int32_t i3_inner_2 = 0; i3_inner_2 < 16; ++i3_inner_2) {\n          if (((i3_outer_2 * 8) + (i3_inner_2 >> 1)) < 63) {\n            compute[((((i0_i1_fused_4 * 15876) + (i2_2 * 126)) + (i3_outer_2 * 16)) + i3_inner_2)] = max(group_conv2d_nchw[((((i0_i1_fused_4 * 15876) + (i2_2 * 126)) + (i3_outer_2 * 16)) + i3_inner_2)], 0.000000e+00f);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_4(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_15(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw, float* __restrict__ pad_temp) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] - T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_18(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = sqrtf(T_reshape[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_23(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_20(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_17(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = (T_reshape[((int)threadIdx.x)] + 1.000000e-05f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_6(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = sqrtf(T_reshape[((int)threadIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_21(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_22(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_2(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_8(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_5(float* __restrict__ T_reshape) {\n  T_reshape[((int)threadIdx.x)] = (T_reshape[((int)threadIdx.x)] + 1.000000e-05f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_19(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] / T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_13(float* __restrict__ group_conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rc = 0; rc < 3; ++rc) {\n    if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n      pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (group_conv2d_nchw[((rc * 15876) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 15876))] * ph[(((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969) * 9) + rc)]));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_9(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] / T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_10(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_14(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_12(float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_11(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ pad_temp, float* __restrict__ ph) {\n  pad_temp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ph[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_16(float* __restrict__ T_reshape, float* __restrict__ ph) {\n  T_reshape[((int)threadIdx.x)] = ph[((int)threadIdx.x)];\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_24(float* __restrict__ compute, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ group_conv2d_nchw, float* __restrict__ pad_temp, float* __restrict__ ph) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int ry = 0; ry < 3; ++ry) {\n    for (int rx = 0; rx < 3; ++rx) {\n      if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n        group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + (pad_temp[((((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969) * 16384) + (((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 7938) / 63) * 128)) + (ry * 128)) + rx) + (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 126))] * ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969) * 27) + (ry * 3)) + rx)]));\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ T_reshape, float* __restrict__ group_conv2d_nchw) {\n  if (((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) < 11907) {\n    group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (group_conv2d_nchw[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] - T_reshape[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3969)]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph: T.Buffer((1, 3, 128, 128), \"float32\"), ph_1: T.Buffer((3, 3, 3, 3), \"float32\"), ph_2: T.Buffer((1, 3, 126, 126), \"float32\"), ph_3: T.Buffer((1, 3, 126, 126), \"float32\"), ph_4: T.Buffer((1,), \"float32\"), ph_5: T.Buffer((1,), \"float32\"), ph_6: T.Buffer((3, 9, 1, 1), \"float32\"), ph_7: T.Buffer((1, 3, 126, 126), \"float32\"), ph_8: T.Buffer((1, 3, 126, 126), \"float32\"), ph_9: T.Buffer((1,), \"float32\"), ph_10: T.Buffer((1,), \"float32\"), compute: T.Buffer((1, 3, 126, 126), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([49152], \"float32\", \"global\")\n        group_conv2d_nchw = T.allocate([47628], \"float32\", \"global\")\n        T_reshape = T.allocate([3], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((49152,), data=pad_temp)\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3_outer, i3_inner in T.grid(128, 8, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 16384 + i2 * 128 + i3_outer * 16 + i3_inner\n                ph_11 = T.Buffer((49152,), data=ph.data)\n                pad_temp_1[cse_var_1] = ph_11[cse_var_1]\n        group_conv2d_nchw_1 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for nn_ff_fused in T.parallel(3):\n            for yy, xx_outer, xx_inner in T.grid(126, 8, 16):\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    group_conv2d_nchw_1[nn_ff_fused * 15876 + yy * 126 + xx_outer * 16 + xx_inner] = T.float32(0)\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    for ry, rx in T.grid(3, 3):\n                        cse_var_3: T.int32 = xx_outer * 16\n                        cse_var_2: T.int32 = nn_ff_fused * 15876 + yy * 126 + cse_var_3 + xx_inner\n                        ph_11 = T.Buffer((81,), data=ph_1.data)\n                        group_conv2d_nchw_1[cse_var_2] = group_conv2d_nchw_1[cse_var_2] + pad_temp_1[nn_ff_fused * 16384 + yy * 128 + ry * 128 + cse_var_3 + xx_inner + rx] * ph_11[nn_ff_fused * 27 + ry * 3 + rx]\n        T_reshape_1 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            ph_11 = T.Buffer((47628,), data=ph_2.data)\n            T_reshape_1[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        group_conv2d_nchw_2 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_4: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_2[cse_var_4] = group_conv2d_nchw_1[cse_var_4] - T_reshape_1[ax0_ax1_fused]\n        T_reshape_2 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            ph_11 = T.Buffer((47628,), data=ph_3.data)\n            T_reshape_2[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        T_reshape_3 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_3[ax0_ax1_fused] = T_reshape_2[ax0_ax1_fused] + T.float32(1.0000000000000001e-05)\n        T_reshape_4 = T.Buffer((3,), data=T_reshape, align=8)\n        for i0_i1_fused in T.parallel(3):\n            T_reshape_4[i0_i1_fused] = T.sqrt(T_reshape_3[i0_i1_fused])\n        group_conv2d_nchw_3 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_5: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_3[cse_var_5] = group_conv2d_nchw_2[cse_var_5] / T_reshape_4[ax0_ax1_fused]\n        T_reshape_5 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_5[ax0_ax1_fused] = ph_4[0]\n        group_conv2d_nchw_4 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_6: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_4[cse_var_6] = group_conv2d_nchw_3[cse_var_6] * T_reshape_5[ax0_ax1_fused]\n        T_reshape_6 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_6[ax0_ax1_fused] = ph_5[0]\n        group_conv2d_nchw_5 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_7: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_5[cse_var_7] = group_conv2d_nchw_4[cse_var_7] + T_reshape_6[ax0_ax1_fused]\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3_outer, i3_inner in T.grid(126, 8, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 63):\n                    group_conv2d_nchw_6 = T.Buffer((47628,), data=group_conv2d_nchw)\n                    cse_var_8: T.int32 = i0_i1_fused * 15876 + i2 * 126 + i3_outer * 16 + i3_inner\n                    group_conv2d_nchw_6[cse_var_8] = T.max(group_conv2d_nchw_5[cse_var_8], T.float32(0))\n        pad_temp_2 = T.Buffer((47628,), data=pad_temp)\n        for nn_ff_fused in T.parallel(3):\n            for yy, xx_outer, xx_inner in T.grid(126, 8, 16):\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    pad_temp_2[nn_ff_fused * 15876 + yy * 126 + xx_outer * 16 + xx_inner] = T.float32(0)\n                if T.likely(xx_outer * 8 + xx_inner // 2 < 63):\n                    for rc in range(3):\n                        cse_var_11: T.int32 = yy * 126\n                        cse_var_10: T.int32 = xx_outer * 16\n                        cse_var_9: T.int32 = nn_ff_fused * 15876 + cse_var_11 + cse_var_10 + xx_inner\n                        group_conv2d_nchw_6 = T.Buffer((47628,), data=group_conv2d_nchw)\n                        ph_11 = T.Buffer((27,), data=ph_6.data)\n                        pad_temp_2[cse_var_9] = pad_temp_2[cse_var_9] + group_conv2d_nchw_6[rc * 15876 + cse_var_11 + cse_var_10 + xx_inner] * ph_11[nn_ff_fused * 9 + rc]\n        T_reshape_7 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            ph_11 = T.Buffer((47628,), data=ph_7.data)\n            T_reshape_7[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        group_conv2d_nchw_6 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_12: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_6[cse_var_12] = pad_temp_2[cse_var_12] - T_reshape_7[ax0_ax1_fused]\n        T_reshape_8 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            ph_11 = T.Buffer((47628,), data=ph_8.data)\n            T_reshape_8[ax0_ax1_fused] = ph_11[ax0_ax1_fused]\n        T_reshape_9 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_9[ax0_ax1_fused] = T_reshape_8[ax0_ax1_fused] + T.float32(1.0000000000000001e-05)\n        T_reshape_10 = T.Buffer((3,), data=T_reshape, align=8)\n        for i0_i1_fused in T.parallel(3):\n            T_reshape_10[i0_i1_fused] = T.sqrt(T_reshape_9[i0_i1_fused])\n        group_conv2d_nchw_7 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_13: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_7[cse_var_13] = group_conv2d_nchw_6[cse_var_13] / T_reshape_10[ax0_ax1_fused]\n        T_reshape_11 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_11[ax0_ax1_fused] = ph_9[0]\n        group_conv2d_nchw_8 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_14: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_8[cse_var_14] = group_conv2d_nchw_7[cse_var_14] * T_reshape_11[ax0_ax1_fused]\n        T_reshape_12 = T.Buffer((3,), data=T_reshape, align=8)\n        for ax0_ax1_fused in T.parallel(3):\n            T_reshape_12[ax0_ax1_fused] = ph_10[0]\n        group_conv2d_nchw_9 = T.Buffer((47628,), data=group_conv2d_nchw)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2, ax3_outer, ax3_inner in T.grid(126, 8, 16):\n                if T.likely(ax3_outer * 8 + ax3_inner // 2 < 63):\n                    cse_var_15: T.int32 = ax0_ax1_fused * 15876 + ax2 * 126 + ax3_outer * 16 + ax3_inner\n                    group_conv2d_nchw_9[cse_var_15] = group_conv2d_nchw_8[cse_var_15] + T_reshape_12[ax0_ax1_fused]\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3_outer, i3_inner in T.grid(126, 8, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 63):\n                    compute_1 = T.Buffer((47628,), data=compute.data)\n                    cse_var_16: T.int32 = i0_i1_fused * 15876 + i2 * 126 + i3_outer * 16 + i3_inner\n                    compute_1[cse_var_16] = T.max(group_conv2d_nchw_9[cse_var_16], T.float32(0))",
        "op_args": "None",
        "input_shape": [
            [
                1,
                3,
                128,
                128
            ],
            [
                3,
                3,
                3,
                3
            ],
            [
                1,
                3,
                126,
                126
            ],
            [
                1,
                3,
                126,
                126
            ],
            [
                1
            ],
            [
                1
            ],
            [
                3,
                9,
                1,
                1
            ],
            [
                1,
                3,
                126,
                126
            ],
            [
                1,
                3,
                126,
                126
            ],
            [
                1
            ],
            [
                1
            ]
        ],
        "output_shape": [
            [
                1,
                3,
                126,
                126
            ]
        ],
        "input_name": [
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph",
            "ph"
        ],
        "output_name": [
            "compute"
        ]
    }
]