extern "C" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {
  int64_t first[1];
  int64_t last[1];
  int64_t first_1[1];
  int64_t last_1[1];
  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {
    if (i_0 == (int64_t)0) {
      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));
      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));
      while ((first[0] < last[0])) {
        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {
          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);
        } else {
          last[0] = ((first[0] + last[0]) >> (int64_t)1);
        }
      }
      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);
      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);
      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {
        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {
          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {
            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            first[0] = (first[0] + (int64_t)1);
          } else {
            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            last[0] = (last[0] + (int64_t)1);
          }
        } else {
          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {
            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            first[0] = (first[0] + (int64_t)1);
          } else {
            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            last[0] = (last[0] + (int64_t)1);
          }
        }
      }
    } else {
      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));
      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));
      while ((first_1[0] < last_1[0])) {
        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {
          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);
        } else {
          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);
        }
      }
      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);
      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);
      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {
        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {
          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            first_1[0] = (first_1[0] + (int64_t)1);
          } else {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            last_1[0] = (last_1[0] + (int64_t)1);
          }
        } else {
          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            first_1[0] = (first_1[0] + (int64_t)1);
          } else {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            last_1[0] = (last_1[0] + (int64_t)1);
          }
        }
      }
    }
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {
  if (((int)threadIdx.x) < 512) {
    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];
    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));
  }
}

extern "C" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {
  __shared__ float temp_keys_swap[128];
  __shared__ float temp_values_swap[128];
  float temp_cond1[1];
  float temp_cond2[1];
  float temp_keys[1];
  float temp_values[1];
  for (int i = 0; i < 2; ++i) {
    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];
    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];
  }
  __syncthreads();
  for (int j = 0; j < 128; ++j) {
    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {
      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];
      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];
      if (temp_cond2[0] < temp_cond1[0]) {
        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];
        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];
        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];
        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];
        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];
        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];
      }
    }
    __syncthreads();
  }
  for (int k = 0; k < 2; ++k) {
    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];
    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];
    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];
    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];
  }
}

