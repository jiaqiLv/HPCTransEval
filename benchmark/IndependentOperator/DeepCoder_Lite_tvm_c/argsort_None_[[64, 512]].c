#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <math.h>

void default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {
  int64_t first[1];
  int64_t last[1];
  int64_t first_1[1];
  int64_t last_1[1];
  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {
    if (i_0 == (int64_t)0) {
      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));
      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));
      while ((first[0] < last[0])) {
        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {
          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);
        } else {
          last[0] = ((first[0] + last[0]) >> (int64_t)1);
        }
      }
      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);
      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);
      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {
        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {
          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {
            argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            argsort_gpu_v3[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            first[0] = (first[0] + (int64_t)1);
          } else {
            argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            argsort_gpu_v3[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            last[0] = (last[0] + (int64_t)1);
          }
        } else {
          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {
            argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            argsort_gpu_v3[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];
            first[0] = (first[0] + (int64_t)1);
          } else {
            argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            argsort_gpu_v3[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];
            last[0] = (last[0] + (int64_t)1);
          }
        }
      }
    } else {
      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));
      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));
      while ((first_1[0] < last_1[0])) {
        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {
          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);
        } else {
          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);
        }
      }
      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);
      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);
      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {
        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {
          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            first_1[0] = (first_1[0] + (int64_t)1);
          } else {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            last_1[0] = (last_1[0] + (int64_t)1);
          }
        } else {
          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];
            first_1[0] = (first_1[0] + (int64_t)1);
          } else {
            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];
            last_1[0] = (last_1[0] + (int64_t)1);
          }
        }
      }
    }
  }
}

void default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {
  int64_t cse_var_1 = 10; // Assuming cse_var_1 is a constant value derived from the CUDA kernel
  int64_t i_0 = 0; // Assuming i_0 is a constant value derived from the CUDA kernel
  int64_t num_blocks = 64; // Assuming num_blocks is derived from the CUDA kernel launch configuration
  int64_t num_threads = 512; // Assuming num_threads is derived from the CUDA kernel launch configuration

  #pragma omp parallel for
  for (int64_t i = 0; i < (int64_t)64; ++i) {
    for (int64_t j = 0; j < (int64_t)512; ++j) {
      argsort_gpu_v0[((i * 512) + j)] = data[((i * 512) + j)];
      argsort_gpu[(i * 512) + j] = (float)j;
    }
  }

  for (int64_t i = 0; i < (int64_t)64; ++i) {
    for (int64_t j = 0; j < (int64_t)512; ++j) {
      default_function_kernel(&argsort_gpu[(i * 512)], &argsort_gpu_v0[(i * 512)], cse_var_1, i_0);
    }
  }

  for (int64_t i = 0; i < (int64_t)64; ++i) {
    for (int64_t j = 0; j < (int64_t)512; ++j) {
      float temp_keys[1];
      float temp_values[1];
      for (int64_t k = 0; k < 128; ++k) {
        temp_keys[k] = argsort_gpu_v0[(i * 512) + j + k];
        temp_values[k] = argsort_gpu[(i * 512) + j + k];
      }
      for (int64_t k = 0; k < 128; ++k) {
        for (int64_t l = 0; l < 128 - k - 1; ++l) {
          if (temp_keys[l] > temp_Keys[l + 1]) {
            float temp = temp_keys[l];
            temp_keys[l] = temp_keys[l + 1];
            temp_keys[l + 1] = temp;
            float temp_val = temp_values[l];
            temp_values[l] = temp_values[l + 1];
            temp_values[l + 1] = temp_val;
          }
        }
      }
      for (int64_t k = 0; k < 128; ++k) {
        argsort_gpu_v0[(i * 512) + j + k] = temp_keys[k];
        argsort_gpu[(i * 512) + j + k] = temp_values[k];
      }
    }
  }
}